<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Questions on functional dimension of ReLU networks | Christopher R. Cornwell </title> <meta name="author" content="Christopher R. Cornwell"> <meta name="description" content="understanding functional dimension by precomposing"> <meta name="keywords" content="machine learning, neural networks, geometry, towson university, jekyll"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://cornwell.github.io/blog/2024/precomposing-funcdim/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> Christopher R. Cornwell </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">research </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Questions on functional dimension of ReLU networks</h1> <p class="post-meta"> Created in December 20, 2024 </p> <p class="post-tags"> <a href="/blog/2024"> <i class="fa-solid fa-calendar fa-sm"></i> 2024 </a>   ·   <a href="/blog/tag/research"> <i class="fa-solid fa-hashtag fa-sm"></i> research</a>   <a href="/blog/tag/thoughts"> <i class="fa-solid fa-hashtag fa-sm"></i> thoughts</a>   <a href="/blog/tag/relu-networks"> <i class="fa-solid fa-hashtag fa-sm"></i> ReLU_networks</a>   ·   <a href="/blog/category/math"> <i class="fa-solid fa-tag fa-sm"></i> math</a>   <a href="/blog/category/neural-networks"> <i class="fa-solid fa-tag fa-sm"></i> neural_networks</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h2"><a href="#setup-and-notation">Setup and notation</a></li> <li class="toc-entry toc-h2"> <a href="#functional-dimension-of-a-parameter-for-a-relu-neural-network">Functional Dimension of a Parameter for a ReLU Neural Network</a> <ul> <li class="toc-entry toc-h5"><a href="#lemma-82">Lemma 8.2</a></li> <li class="toc-entry toc-h5"> <a href="#proof-sketch">Proof sketch</a> <ul> <li class="toc-entry toc-h6"><a href="#question">Question</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#technical-lemma-85">Technical Lemma 8.5</a> <ul> <li class="toc-entry toc-h5"><a href="#lemma-85">Lemma 8.5</a></li> <li class="toc-entry toc-h5"><a href="#proof-sketch-of-lemma-85">Proof sketch of Lemma 8.5</a></li> <li class="toc-entry toc-h5"><a href="#lemma-83">Lemma 8.3.</a></li> <li class="toc-entry toc-h5"><a href="#proof-sketch-1">Proof sketch</a></li> <li class="toc-entry toc-h5"><a href="#theorem-87">Theorem 8.7.</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#the-question-of-dim_funtheta-vs-dim_funtheta">The Question of \(\dim_{fun+}(\theta)\) vs. \(\dim_{fun}(\theta)\)</a> <ul> <li class="toc-entry toc-h5"><a href="#claim">Claim</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <p>I first started writing this post as a draft at my old Wordpress website. I’m attempting to port it here. Please be patient if I missed a spot where some syntax or formatting needed to be translated.</p> <hr> <p>This is one of a series of posts about ReLU neural networks. My focus in this post is on the <strong>functional dimension</strong> of (a parameter for) a ReLU neural network. <br>      See <em>Functional dimension of feedforward ReLU neural networks</em> by Grigsby, Lindsey, Meyerhoff, and Wu (<a href="https://arxiv.org/abs/2209.04036" rel="external nofollow noopener" target="_blank">link to the preprint</a>).</p> <p>I will abbreviate references to this paper by writing [GLMW22].  In particular, I want to work on understanding Lemmas 8.2 and 8.5, as well as Theorem 8.7 in that paper – having to do with how the functional dimension behaves under composition of networks.</p> <h2 id="setup-and-notation">Setup and notation</h2> <hr> <p>The notation used will largely match that of [GLMW22]. We’ll be working with feedforward ReLU neural networks and their associated network function. Here is the general setup.</p> <p><strong>Definition 1.</strong> Let \(n_0\in \mathbb N\) and \(d \in \mathbb N\) and let \(\phi:\mathbb R \to \mathbb R\) be a continuous function. A <strong>feedforward neural network</strong> \(\mathcal N\) defined on \(\mathbb R^{n_0}\), with <em>depth</em> \(d\) and <em>activation function</em> \(\phi\), is an ordered sequence of \(d\) affine maps \(A^1, A^2, \ldots, A^d,\) with associated positive integers \(n_1,n_2,\ldots,n_d,\) that are defined such that \(A^{\ell}:\mathbb R^{n_{\ell-1}} \to \mathbb R^{n_{\ell}}\) for every \(1 \le \ell \le d.\) For any \(n\in \mathbb N\), define \(\Phi:\mathbb R^n\to\mathbb R^n\) as the component-wise application of the function \(\phi\) on the input. The <strong>network function</strong> associated to \(\mathcal N\) is the function \(F:\mathbb R^{n_0}\to \mathbb R^{n_d}\) that is defined by</p> \[F(\mathbf{x}) = A^d\circ \Phi\circ A^{d-1} \circ \ldots \circ \Phi\circ A^{2} \circ \Phi\circ A^{1}(\mathbf{x})\] <p>for \(\mathbf{x} \in \mathbb R^{n_0}\). The list \((n_0, n_1, \ldots, n_d)\) is called the <strong>architecture</strong> of the neural network \(\mathcal N\).</p> <p><br></p> <p>In this post the focus is exclusively on <em>ReLU (feedforward) neural networks</em> which are feedforward neural networks that have an activation function given by \(\phi(x) = \max\{0, x\}\), for \(x\in \mathbb R\).</p> <p>For every \(1\le \ell\le d-1\), the <strong>\(\ell\)-th layer map</strong> \(F^{\ell}:\mathbb R^{n_{\ell-1}} \to \mathbb R^{n_{\ell}}\) is defined by \(F^{\ell} = \Phi\circ A^{\ell}\), with the \(d\)-th layer map being simply \(A^d\).  For every \(1\le \ell\le d\) and every \(1\le j\le n_{\ell}\), the pair \((\ell, j)\) is referred to as a <strong>neuron</strong> of \(\mathcal N\); sometimes this <strong>neuron</strong> refers to the function \(z^{\ell}_j : \mathbb R^{n_0} \to \mathbb R\) that takes on the (“pre-activation”) values at \((\ell, j)\), i.e., letting \(\pi_j\) denote projection to the \(j\)-th coordinate, we have that \(z^{\ell}_j = \pi_j \circ A^{\ell}\circ F^{\ell-1} \circ\ldots\circ F^1\).</p> <p>Note that each affine map \(A^{\ell}\) may be expressed in coordinates, so that for \(\mathbf{x}\in\mathbb R^{n_{\ell-1}}\), we have \(A^{\ell}(\mathbf{x}) = W^{\ell}\mathbf{x} + \mathbf{b}^{\ell}\) where \(W^{\ell} \in \mathbb R^{n_{\ell}\times n_{\ell-1}}\) is a “weight” matrix and \(\mathbf{b}^{\ell} \in \mathbb R^{n_\ell}\) is a “bias” vector.</p> <p>Thinking of ReLU neural networks with a given architecture \((n_0, n_1, \ldots, n_d)\) – or, more appropriately, the associated network functions for such networks – as a parameterized class of functions, the entries in the weight matrices and bias vectors for the network’s layers make up the parameters. We organize the parameters as \(\theta = (W^1, \mathbf{b}^1, \ldots, W^d, \mathbf{b}^d)\), which we associate in some chosen manner to a point in \(\mathbb R^D\), with \(D = D(n_0,\ldots,n_d) := \sum_{i=1}^d n_i(n_{i-1}+1).\) We write \(\Omega := \mathbb R^D\) in order to emphasize the space of parameters – and how it corresponds to weights and biases in \(\mathcal N\). At times, in order to denote the neural network with parameters \(\theta\), for an understood architecture, we write \(\mathcal N(\theta)\). Additionally, the associated network function may be written as \(F_\theta: \mathbb R^{n_0} \to \mathbb R^{n_d}.\)</p> <h2 id="functional-dimension-of-a-parameter-for-a-relu-neural-network">Functional Dimension of a Parameter for a ReLU Neural Network</h2> <hr> <p>The lemmas and theorem that we want to understand better are related to a quantity called \(\dim_{fun}(\theta)\), the functional dimension of \(\theta\) (for a ReLU network \(\mathcal N(\theta)\)). Some effort is required to define this quantity. For the definition, there are a few technical assumptions that must be made about \(\theta\), but the assumptions are not very restrictive. To the contrary, they rule out some rare situations for the network. In order to arrive at the definition of \(\dim_{fun}(\theta)\) quickly, however, for now we bypass describing the technical assumptions; we will say simply that \(\theta\) is “nice” (or, “satisfies nice conditions”) and move forward.</p> <p>Consider a fixed architecture \((n_0, n_1, \ldots, n_d)\) and the class of ReLU networks with this architecture. Given some \(\theta\in\Omega\), for \(1\le j\le n_d\), use \(F_{\theta,j}:\mathbb R^{n_0}\to\mathbb R\) to denote the \(j\)-th coordinate function of \(F_\theta\), so for every \(\mathbf{x}\in\mathbb R^{n_0}\) we have \(F_\theta(\mathbf{x}) = (F_{\theta,1}(\mathbf{x}), \ldots, F_{\theta,n_d}(\mathbf{x}))\).<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> Additionally, consider a finite ordered set of points \(Z = \{z_1, z_2, \ldots, z_k\} \subset \mathbb R^{n_0}\).  The evaluation map at \(Z\), \(E_Z: \Omega \to \mathbb R^{k\cdot n_d}\) is given by setting, for every \(\theta\in\Omega\),</p> \[E_Z(\theta) = (F_{\theta,1}(z_1), \ldots, F_{\theta,n_d}(z_1), \ldots, F_{\theta,1}(z_k), \ldots, F_{\theta,n_d}(z_k)).\] <p>To define the functional dimension, we use the Jacobian matrix of the evaluation map, which we denote by \(\mathbf{J}E_Z\).  In other words, for each \(z_i\in Z\), let \(\mathbf{J}E_{z_i}\lvert_{\theta}\) be the \(n_m \times D\) matrix, the \(j\)-th row of which is the vector of partial derivatives of \(F_{\theta, j}(z_i)\) <em>with respect to the</em> \(D\) <em>parameters</em>,<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>  evaluated at \(\theta\). The matrix \(\mathbf{J}E_Z\lvert_{\theta}\) is the \(k\cdot n_m \times D\) matrix obtained by stacking these matrices. There are values of \(\theta\) at which some of the partial derivatives in \(\mathbf{J}E_Z\) are not well-defined – hence, the reason for making “nice” assumptions.</p> <p><strong>Definition 2.</strong> Fix an architecture \((n_0, n_1, \ldots, n_d)\) and let \(\Omega\) be its corresponding parameter space. Suppose that \(\theta\in\Omega\) satisfies nice conditions. <sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> The <strong>functional dimension</strong> at \(\theta\) is defined to be</p> \[\dim_{fun}(\theta) = \underset{Z \text{ is finite and smooth for } \theta}{\sup}\operatorname{rank} \mathbf{J}E_Z\lvert_{\theta}.\] <p>In Definition 2, the condition that \(Z\) be smooth for \(\theta\) (or, <em>parametrically smooth</em> in [GLMW22]) refers to the function \(\mathcal{F}:\Omega\times\mathbb R^{n_0} \to \mathbb R^{n_d}\), given by \(\mathcal{F}(\theta, x) = F_\theta(x)\), being smooth at \((\theta, z_i)\) for every \(z_i\in Z\). One can also define the so-called <strong>batch functional dimension</strong> at \(\theta\) for a batch \(Z\), which is equal simply to the rank of \(\mathbf{J}E_Z\lvert_{\theta}\). Clearly, the batch functional dimension is less than or equal to the functional dimension.</p> <p>In Section 5 of [GLMW22], for a given network \(\mathcal N\) with parameter \(\theta\), the authors of that paper describe a finite set in \(\mathbb R^{n_0}\) for which the batch functional dimension for that set is equal to \(\dim_{fun}(\theta)\). To confirm that a <em>given</em> \(Z \subset \mathbb R^{n_0}\) is such a finite set, called a <strong>decisive set</strong> for \(\mathcal N(\theta)\), requires knowledge of the regions in \(\mathbb R^{n_0}\) on which \(F_{\theta}\) is affine-linear; more precisely, for each top-dimensional cell of the “canonical polyhedral complex” of \(F_\theta\), a decisive set contains the vertices of some \(n_0\)-dimensional simplex contained in that cell. Provided that \(\theta\) is “nice” and \(Z\subset\mathbb R^{n_0}\) is a decisive set, we have \(\dim_{fun}(\theta) = \operatorname{rank}\mathbf{J}E_Z\lvert_{\theta}\).</p> <p>A discussion of the “canonical polyhedral complex” of \(F_\theta\) won’t happen in this post. Perhaps I will describe it in another post. Suffice it to say that it consists, in part, of a set of (<em>top-dimensional</em>) polyhedral cells, no two of which intersect in their interiors, but where the union of their closures is the entire domain \(\mathbb R^{n_0}\); furthermore, \(F_\theta\) is affine-linear when restricted to any one of these cells.</p> <p>Finally, we will be interested in a certain restricted notion of functional dimension, when we consider only sets \(Z\) which are in the interior of the positive orthant \(\mathbb R_{&gt;0}^{n_0} = \{(x_1,\ldots,x_{n_0})\ \lvert\ x_i &gt; 0 \text{ for all } 1\le i\le n_0\}\). Write this as</p> \[\dim_{fun+}(\theta) = \underset{Z\subset\mathbb R_{&gt;0}^{n_0} \text{ is finite and smooth for } \theta}{\sup}\operatorname{rank} \mathbf{J}E_Z\lvert_{\theta}.\] <p>After a small change to the notation, the following is the statement of Lemma 8.2 in [GLMW22].</p> <h5 class="env-title" id="lemma-82">Lemma 8.2</h5> <p>Fix \(n_0\in\mathbb N\) and let \(\Omega\) be the parameter space for architecture \((n_1,n_2,\ldots,n_d)\). Let \(\theta\in\Omega\) be such that there is a smooth point for \(\theta\) in the strictly positive orthant \(\mathbb R_{&gt;0}^{n_1}\). Let \(A:\mathbb R^{n_0}\to\mathbb R^{n_1}\) be affine-linear such that every row of the associated matrix has at least one non-zero entry. Use \((A, \theta)\) to denote the parameter for architecture \((n_0,n_1,\ldots,n_d)\) that precomposes with \(A\), i.e., \(\Phi\circ A\) is the first layer map of the network function \(F_{(A,\theta)}\). <br> If \((A, \theta)\) satisfies nice conditions, then \(\dim_{fun}(A,\theta) \le n_0n_1 + \dim_{fun+}(\theta)\). Furthermore, for this inequality to be equality it is necessary that \(\dim_{fun+}(\theta)\) be realized on a smooth set \(Z^* \subset \operatorname{Im}(\Phi\circ A)\).</p> <blockquote class="block-tip"> <h5 id="proof-sketch">Proof sketch</h5> <p>The proof of this lemma uses the scaling-inverse scaling invariance of \(F_{(A,\theta)}\), in the first hidden layer. The authors use this to identify the parameter space for \((A,\theta)\) with coordinates in the product \(\mathbb R_{&gt;0}^{n_1} \times (\mathbb R^{n_0})^{n_1} \times \Omega\). Then they argue that, for \(Z\subset\mathbb R^{n_0}\), the columns of \(\mathbf{J}E_Z\lvert_{(A,\theta)}\) that correspond to parameters in \(\mathbb R_{&gt;0}^{n_1}\) will be zero, and that the columns corresponding to parameters in \((\mathbb R^{n_0})^{n_1}\) contribute at most \(n_0n_1\) to the rank.</p> <p>Finally, using that \(\Phi\circ A(Z)\) is a subset of  \(\mathbb R_{\ge 0}^{n_1}\), they argue that the rank of columns corresponding to parameters in \(\Omega\) will at most be the \(\sup\) of the rank of \(\mathbf{J}\) of the evaluation map on subsets in \(\mathbb R_{&gt;0}^{n_1}\), evaluated at \(\theta\), which equals \(\dim_{fun+}(\theta).\) \(\blacksquare\)</p> </blockquote> <p>Note that all of the assumptions in Lemma 8.2 that occur before the word “Furthermore” will be true of a full measure subset in the parameter space for \((A, \theta)\). This is proven in [GLMW22] and remarked upon at the beginning of subsection 8.1.  Moreover, if \(n_0 \ge n_1\) then there is a full measure subset of parameters so that \(A\) is surjective, in which case \(\operatorname{Im}(\Phi\circ A) = \mathbb R_{\ge 0}^{n_1}\). By definition, this contains any subset \(Z^*\) that realizes \(\dim_{fun+}(\theta)\). So, if \(n_0 \ge n_1\) then a full measure set of parameters satisfies that necessary condition.</p> <p>However, if \(n_0 &lt; n_1\) then \(\operatorname{Im}(\Phi\circ A)\) cannot contain a set of points in \(\mathbb R_{&gt;0}^{n_1}\) that are vertices of an \(n_1\)-simplex – since \(\operatorname{Im}(\Phi\circ A) \cap \mathbb R_{&gt;0}^{n_1}\) is contained in an \((n_1-1)\)-dimensional affine subspace. This means that any decisive set that realizes \(\dim_{fun+}(\theta)\) is not in the image. However, if \(\dim_{fun+}(\theta)\) can be realized on a set that is <em>not</em> decisive, perhaps it is possible in such a situation to satisfy this condition.</p> <h6 class="env-title" id="question">Question</h6> <p>How do we understand the difference between \(\dim_{fun+}(\theta)\) and \(\dim_{fun}(\theta)\)?</p> <h2 id="technical-lemma-85">Technical Lemma 8.5</h2> <hr> <p>Let’s discuss Lemma 8.5. We’ll need, at least, the notion of the ternary labeling at \(x \in \mathbb R^{n_0}\), determined by \(\theta\). At this juncture, a (minor) reckoning has arrived. As is common for parameterized classes of functions, and in mathematical modeling, we used the notation \(\theta\) for our parameter vector in \(\mathbb R^{D}\), with \(D\) being the total number of weights and biases in network architecture \((n_0,n_1,\ldots,n_d)\). However, the letter \(\theta\) is also used in the literature to denote ternary labelings, which are functions \(\mathbb R^{n_0} \to \{-1, 0, 1\}^{N}\) with \(N = n_1+n_2+\ldots+n_d\). As a solution, we will try using the letter \(\tau.\)</p> <p>So, a definition. For this definition, recall for each neuron \((\ell, j)\) the pre-activation function \(z^{\ell}_j:\mathbb R^{n_0}\to\mathbb R\), when \(1\le \ell\le n_d\) and \(1\le j\le n_\ell.\)</p> <p><strong>Definition 3.</strong> Let \(\theta\in\Omega\) be the parameter for a network with architecture \((n_0,n_1,\ldots,n_d)\). For each neuron \((\ell, j)\) of the network, define \(\tau^\ell_j:\mathbb R^{n_0} \to \{-1, 0, 1\}\) by setting \(\tau^\ell_j(x) = \text{sign}(z^\ell_j(x))\). (Here, the function \(\text{sign}\) returns 0 if the input is zero, 1 if it is positive, and -1 if it is negative.) The (full) <strong>ternary labeling</strong> for the network is the function \(\tau: \mathbb R^{n_0} \to \{-1, 0, 1\}^{N}\) which has a coordinate function for each neuron \((\ell, j)\), namely the function \(\tau^\ell_j.\)</p> <p>Intuitively speaking, the value of \(\tau^\ell_j(x)\) is positive if and only if the neuron \((\ell, j)\) is “on” or “activated” at the point \(x\). It is zero at \(x\) if and only if \(F_{\ell-1}\circ\ldots\circ F_1(x)\) lies inside of the hyperplane \(\{y\ \lvert\ W^\ell y + b^\ell = 0\}.\)</p> <p>For Lemma 8.5, we have a similar setup to Lemma 8.2. There is a parameter \(\theta\in\Omega\), for a network with architecture \((n_1,\ldots, n_d)\), an affine-linear map \(A:\mathbb R^{n_0}\to\mathbb R^{n_1}\) (we may also use \(A\) for the \(n_1\times(n_0+1)\) matrix that determines it, the last column for the translate, or “shift”). We are interested in precomposing with \(A\) to get a neural network with architecture \((n_0,n_1,\ldots,n_d)\) and with parameter \((A, \theta)\).  Note that, before precomposing, there are coordinate ternary labelings \(\tau^\ell_j\) for every  \(1\le \ell\le n_d-1\) and \(1\le j\le n_{\ell+1}.\)<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup></p> <h5 class="env-title" id="lemma-85">Lemma 8.5</h5> <p>Suppose that \(\theta\) and \(A\) are as above and that \(\theta\) satisfies “nice conditions.”  For \(1\le j\le n_1\) and \(x\in \mathbb R^{n_0}\), use \(\tau^A_j(x)\) to denote the ternary label (at \(x\)) with respect to the \(j\)th row of \(A\).  Assume that \(A\) is non-degenerate, in the sense that for \(1\le j\le n_1\) the set where \(\tau^A_j(x) = 0\) is a hyperplane in \(\mathbb R^{n_0}\). We suppose that for every \(1\le k\le n_1\), there exists \(y_k\in\mathbb R^{n_0}\) such that <br>      (i) \(\tau^A_k(y_k) = 0\), <br>      (ii) \(\tau^A_j(y_k) \ne 0\) for all \(j \ne k\), <br>      (iii) for all \((\ell, j)\) with \(1\le \ell\le n_d-1\) and \(1\le j\le n_{\ell+1}\), we have that \(\tau^\ell_j( \Phi\circ A(y_k) ) \ne 0\), and <br>      (iv) the \(k\)th column of \(\mathbf{J}F_{\theta}\lvert_{\Phi\circ A(y_k)}\) is not the zero vector.<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> <br> Then there is a finite set \(Z\) in \(\mathbb R^{n_0}\) such that (v) up to scaling rows of \(A\) by positive numbers, each entry of \(A\) is given by a unique affine-linear combination of the coordinates of the vector \(E_Z(A, \theta)\); and (vi) the ternary labeling for \((A, \theta)\) of every point in \(Z\), at every neuron, is non-zero. <br> Finally, the lemma also states that if \(y_k \in \mathbb R_{&gt;0}^{n_0}\) for every \(k\) then (vii) the set \(Z\) can be chosen to be in \(R_{&gt;0}^{n_0}\).</p> <p><br></p> <p>Now, finding a set \(y_1, \ldots, y_{n_1}\) that satisfy conditions (i) - (iii) is generically possible – the conditions (i) and (ii) can be guaranteed as long as the hyperplane arrangement in \(\mathbb R^{n_0}\) associated to \(A\) is generic; condition (iii) says that each of the \(n_1\) points \(\Phi\circ A(y_k)\) is contained in the interior of a top-dimensional cell of the canonical polyhedral decomposition for \(\mathcal N(\theta)\) and this can be achieved for a choice of \(y_1,\ldots,y_{n_1}\) by a perturbation of \(\theta\).</p> <p>However, there is a positive measure subset of parameters for which condition (iv) will be impossible to satisfy. For example, there is a positive measure subset such that \(F_\theta\) is a constant function on all of \(\mathbb R^{n_1}\). Using \(H_k\) to denote the hyperplane where \(\tau^A_k\) is zero, there is a larger subset on which \(F_\theta\) is constant on the set \(\Phi\circ A(H_k)\) (and in any “nearby” choice of \((A,\theta)\) too).</p> <blockquote class="block-tip"> <h5 id="proof-sketch-of-lemma-85">Proof sketch of Lemma 8.5</h5> <p>To prove Lemma 8.5, the authors of [GLMW22] first note that, as a consequence of the assumptions (i) - (iii), for each \(1\le k\le n_1\), there is an open neighborhood \(U_k\) of \(y_k\) so that the ternary labelings of all neurons in \(\mathcal N(A, \theta)\) are constant on \(U_k\) except for \(\tau^A_k\). Furthermore, letting \(U_k^+\) and \(U_k^-\) denote the connected components of \(U_k \setminus \{x\ \lvert\ \tau^A_k(x) = 0\}\), with sign of \(\tau^A_k\) on each component matching the superscript, we have that the ternary labeling on every neuron of \((A, \theta)\) is constant on \(U_k^+\) and on \(U_k^-\). As a consequence, \(F_{(A,\theta)}\) is affine-linear when restricted to either \(U_k^+\) or \(U_k^-\), and so \(\mathbf{J}F_{(A,\theta)}\) is constant on each of \(U_k^{\pm}\). Furthermore, \(\mathbf{J}F_\theta\) is constant when restricted to \(\Phi\circ A(U_k)\).</p> <p>Next, they show that \(\mathbf{J}F_{(A,\theta)}\lvert_{U_k^+} \ne \mathbf{J}F_{(A,\theta)}\lvert_{U_k^-}\). To do so, they use the chain rule and that \(F_{(A,\theta)} = F_\theta\circ (\Phi\circ A)\). Then, since \(\mathbf{J}(\Phi\circ A)\lvert_{U_k^+}\) contains a non-zero element in the \(k\)th row, and \(\mathbf{J}(\Phi\circ A)\lvert_{U_k^-}\) has a zero \(k\)th row, assumption (iv) guarantees a non-zero difference in one of the entries of \(\mathbf{J}F_{(A,\theta)}\lvert_{U_k^+} - \mathbf{J}F_{(A,\theta)}\lvert_{U_k^-}.\)</p> <p>Having determined that \(\mathbf{J}F_{(A,\theta)}\lvert_{U_k^+} \ne \mathbf{J}F_{(A,\theta)}\lvert_{U_k^-}\), they have a lemma (Lemma 8.3) that gives the conclusion (v). This lemma produces a set \(Z \subset U_k^+ \cup U_k^-\), which means that (vi) holds (by construction of \(U_k^{\pm}\)) and that (vii) must hold – making the neighborhood \(U_k\) smaller if needed. \(\blacksquare\)</p> </blockquote> <h5 class="env-title" id="lemma-83">Lemma 8.3.</h5> <p>Let \(M\) be a polyhedral complex embedded in \(\mathbb R^d\), \(d\ge 1\), and let \(F:\mathbb R^d \to \mathbb R^{n}\) be a continous map that is affine-linear on cells of \(M\). Let \(X, Y\) be two \(d\)-dimensional cells of \(M\) that share a \((d-1)\)-dimensional facet, and denote the hyperplane containing the shared facet by \(H\). Assume that \(\mathbf{J}F\lvert_X \ne \mathbf{J}F\lvert_Y\). Then, for any decisive sets, \(S_X\subset X\) for \(F\lvert_X\) and \(S_Y\subset Y\) for \(F\lvert_Y\), \(H\) is the solution set to an affine-linear equation \(\{x\ \lvert\ c + Ax = \mathbf{0}\}\) where every entry of \(A\) is an affine linear expression in the coordinates of \(E_{S_X\cup S_Y}(F)\). The matrix \(A\) is unique up to rescaling rows by constants.</p> <blockquote class="block-tip"> <h5 id="proof-sketch-1">Proof sketch</h5> <p>To prove Lemma 8.3, write the points \(S_X = \{z_0,z_1,\ldots,z_d\}\) (which are vertices of a \(d\)-dimensional simplex in \(X\), owing to the fact that \(F\lvert_X\) is affine-linear). Now, since the vectors \(u_i := z_i - z_0\), with \(1\le i\le d\), make a basis of \(\mathbb R^d\), each partial derivative \(\partial F/\partial x_i\) is a linear combination of the directional derivatives \(D_{u_i}F(z_0)\). Additionally, \(\lvert z_i-z_0\rvert D_{u_i}F(z_0)\) is the difference between two coordinates of \(E_{S_X}(F)\). And so, for every \(x\in X\), each entry of \(\mathbf{J}F\lvert_x\) is a linear combination of coordinates of \(E_{S_X}(F)\).  This is similarly true for \(\mathbf{J}F\lvert_y\), \(y\in Y\), and \(E_{S_Y}(F)\).</p> <p>Note that the extension to \(\mathbb R^d\) of the affine-linear map \(F\lvert_X\) can be expressed as \(\mathbf{x} \mapsto c_X + \mathbf{J}F\lvert_X\mathbf{x}\), for some constant vector \(c_X\) (and an analogous statement is true for \(Y\)). Since the hyperplane \(H\) that \(X\) and \(Y\) share consists of those \(\mathbf{x}\) where the extension of \(F\lvert_X\) and the extension of \(F\lvert_Y\) agree, we have</p> \[H = \{\mathbf{x} | c_X - c_Y + (\mathbf{J}F\lvert_X - \mathbf{J}F\lvert_Y)\mathbf{x} = \mathbf{0} \},\] <p>proving the statement. \(\blacksquare\)</p> </blockquote> <p>We now discuss Theorem 8.7, which provides sufficient conditions to have equality: \(\dim_{fun}(A, \theta) = n_0n_1 + \dim_{fun}(\theta)\).</p> <h5 class="env-title" id="theorem-87">Theorem 8.7.</h5> <p>Fix a parameter \(\theta\in\Omega\) which is “nice” and suppose that \(Z_1 \subset \mathbb R_{&gt;0}^{n_1}\) is a finite set whose ternary labels with respect to every neuron of \(\mathcal N(\theta)\) are nonzero, and so that \(\dim_{fun}(\theta) = \operatorname{rank} \mathbf{J}E_{Z_1}\lvert_\theta\). Suppose that \(A:\mathbb R^{n_0}\to\mathbb R^{n_1}\) is a surjective affine-linear map that satisfies all the assumptions of Lemma 8.5 (including that every \(y_k\) is in \(\mathbb R_{&gt;0}^{n_0}\)). Then there is a finite set \(Z \subset \mathbb R_{&gt;0}^{n_0}\) such that the ternary labeling, for all \(z\in Z\) and every neuron of \(\mathcal N(A,\theta)\), is nonzero, and</p> \[\dim_{fun}(A, \theta) = \operatorname{rank}\mathbf{J}E_Z\lvert_{(A,\theta)} = n_0n_1 + \dim_{fun}(\theta).\] <p>The proof of Theorem 8.7 is a bit more involved than the proofs of the lemmas above. However, let us remark on the assumptions being made to get the conclusion of this theorem.</p> <p>First, the existence of a set \(Z_1\), as in the theorem statement, requires that \(\dim_{fun+}(\theta) = \dim_{fun}(\theta)\). It would be valuable to understand what causes this to occur.</p> <p>Second, the surjectivity assumption on \(A\) requires that \(n_0 \ge n_1\). It also includes some restrictive assumptions (not full measure) in order to guarantee assumption (<em>iv</em>) of Lemma 8.5, as we discussed above, as well as guaranteeing that \(y_k\) can be chosen from \(\mathbb R_{&gt;0}^{n_0}\) for each \(k\). (For example, this is impossible if one of the hyperplanes/neurons of \(\Phi\circ A\) does not cut through the positive orthant.)</p> <h2 id="the-question-of-dim_funtheta-vs-dim_funtheta">The Question of \(\dim_{fun+}(\theta)\) vs. \(\dim_{fun}(\theta)\)</h2> <hr> <p>As a start for understanding when \(\dim_{fun+}(\theta)\) and \(\dim_{fun}(\theta)\) are the same, let us make a simple observation. Let \(x_0, x_1, \ldots, x_{n_0}\) be affinely independent points in \(\mathbb R^{n_0}\) and let \(y_0,y_1,\ldots, y_{n_0} \in\mathbb R\). There is a unique affine linear function \(F:\mathbb R^{n_0} \to \mathbb R\) with the property that \(F(x_i) = y_i\) for every \(0\le i\le n_0\). This falls out of linear algebra.</p> <p>Indeed, since the points are affinely independent, \(\{x_i - x_0\ \lvert\ 1\le i\le n_0\}\) is a basis of \(\mathbb R^{n_0}\). The function \(F\) is determined by \(n_0+1\) scalars \(a_0,a_1,\ldots, a_{n_0}\), so that, writing \(\mathbf{a}\) for \((a_1,a_2,\ldots,a_{n_0})\), we have \(F(x) = a_0 + \mathbf{a}\cdot x\) for all \(x\in\mathbb R^{n_0}\). Note that \(y_i - y_0 = \mathbf{a}\cdot (x_i - x_0)\). Therefore, since for any \(x\in \mathbb R^{n_0}\), we have scalars \(c_1,\ldots, c_{n_0}\) so that \(x - x_0 = \sum_{i=1}^{n_0} c_i(x_i - x_0),\) we see that</p> \[F(x) - F(x_0) = \mathbf{a}\cdot (x - x_0) = \sum_{i=1}^{n_0} c_i\mathbf{a}\cdot(x_i - x_0) = \sum_{i=1}^{n_0} c_i(y_i - y_0).\] <p>Therefore, \(F(x) = y_0 + \sum_{i=1}^{n_0} c_i(y_i - y_0)\).</p> <p>Let’s consider a scenario.  Given an architecture of a ReLU network and a parameter \(\theta\in\Omega\) that is “nice,” say that we have a point \(p\) in the interior of a top-dimensional cell of the canonical polyhedral complex \(\mathcal C = \mathcal C(\theta)\). Write \(X\) for this cell containing \(p\). Further, suppose that \(Z\subset \mathbb R^{n_0}\) is a finite set so that:</p> <ol> <li>\(Z\) is the union of finite sets of points in the interior of top-dimensional cells of \(\mathcal C\);</li> <li>there is a vertex \(v\) of \(X\) such that for every top-dimensional cell \(A \ne X\) which has \(v\) as one of its vertices, \(int(A) \cap Z \ne\emptyset\); and</li> <li>if \(A\) is a top-dimensional cell and \(int(A)\cap Z\ne \emptyset\), then \(Z\) contains a decisive set (in \(A\)) for \(F_\theta\lvert_A\).<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup> </li> </ol> <p>We <em>hope</em> that this will mean that \(\operatorname{rank}\mathbf{J}E_Z\lvert_\theta = \operatorname{rank}\mathbf{J}E_{Z^*}\lvert_\theta\), where \(Z^* = Z \cup \{p\}\).</p> <p>The intuition for this would be that, first, since \(Z\) has a decisive set on each cell \(A\) that it intersects, \(F_\theta\lvert_A\) is determined by values at points in \(Z\cap A\). This means that \(F_\theta\) is determined on the boundary of \(X\) on an affinely independent set of points.  Thus, it would seem that our set \(Z\) completely determines \(F_\theta\lvert_{X}\). That is, we should be able to “witness” the partial derivatives in \(E_{\{p\}}\lvert_\theta\) through rows of \(E_{Z}\lvert_\theta\).</p> <p>Let \(A\) be a top-dimensional cell of \(\mathcal C(\theta)\). The first thing to observe is that, related to the fact that an affine-linear function is determined by its values on \(n_0+1\) affinely independent points, if \(x_0,x_1,\ldots,x_{n_0}\) in \(int(A)\) is a set of affinely independent points then, for any \(x\in int(A)\), we can get \(\mathbf{J}E_x\lvert_{\theta}\) as a linear combination of \(\mathbf{J}E_{x_i}\lvert_{\theta}, 0\le i\le n_0\). Indeed, there is a unique set of scalars \(c_1,\ldots, c_{n_0}\) so that \(x - x_0 = \sum_{i=1}^{n_0} c_i (x_i - x_0)\).  Note that each element of \(\mathbf{J}E_{x}\lvert_\theta\) is either linear or constant in \(x\) (when restricting to \(int(A)\)). This means that</p> <p>\begin{equation} \label{eq:vertex-loop} \mathbf{J}E_{x}\lvert_\theta - \mathbf{J}E_{x_0}\lvert_\theta = \sum_{i=1}^{n_0}c_i (\mathbf{J}E_{x_i}\lvert_\theta - \mathbf{J}E_{x_0}\lvert_\theta), \end{equation}</p> <p>which expresses \(\mathbf{J}E_{x}\lvert_{\theta}\) in the desired way as a linear combination.</p> <p>Now, the vector \(\mathbf{J}E_{x}\lvert_{\theta}\) is not defined if \(x\) is contained in a facet of \(A\). <sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">7</a></sup> However, suppose that we use \eqref{eq:vertex-loop} to determine such a vector – this would be the limit of \(\mathbf{J}E_{p_i}\lvert_{\theta}\) for a sequence \(\{p_i\} \subset int(A)\), where \(p_i \to x\). Since it depends on “converging from \(int(A)\)”, say that we call this vector \(\mathbf{J}^AE_x\lvert_{\theta}\).  In fact, from here on we will drop the notation that indicates evaluation at \(\theta\), considering that as understood; hence, call this vector simply \(\mathbf{J}^AE_x\).</p> <p>Taking the above construction a step farther, since \(\{x_i - x_0\ \lvert\ 1\le i\le n_0\}\) is a basis of \(\mathbb R^{n_0}\) we could determine \(\mathbf{J}^AE_x\) from \eqref{eq:vertex-loop}, for any \(x \in \mathbb R^{n_0}\).  Another perspective on this would be to consider how the parameters in \(\theta\) express an affine linear function \(\mathbb R^{n_0} \to \mathbb R^{n_d}\) which has a restriction to \(A\) that agrees with the restriction of \(F_\theta\). Then \(\mathbf{J}^AE_x\) is the Jacobian of the evaluation map, at \(x\), corresponding to that affine linear function.</p> <p>Now, if \(A \ne X\) is one of the cells having non-empty intersection with \(Z\) in conditions 1, 2, and 3 above, then for any \(x\in\mathbb R^{n_0}\), \(\mathbf{J}^AE_x\) is a linear combination of rows of \(\mathbf{J}E_Z\). While we are still figuring out how it works in general, let’s consider a special case.</p> <p><strong>How it works when \(n_0 = 2\) and all bent hyperplanes at the vertex come from one layer.</strong> Under our “nice” assumptions when \(n_0=2\), for any vertex \(v\) of \(X\) there are 4 top-dimensional cells of the polyhedral complex that have \(v\) as a vertex, including \(X\). There are two (bent) hyperplanes intersecting at \(v\) – call them \(H_1\) and \(H_2\). The ternary labeling for each of \(H_1\) and \(H_2\) is positive in exactly two of the 4 aforementioned cells. We may associate in one-to-one manner these cells to elements of \(\{+, -\}^4.\) Let \(C\) and \(\bar C\) be the two cells which are associated to \((+,-)\) and \((-,+)\), which can be taken to mean that the ternary labeling for \(H_1\) is positive in \(C\), but negative in \(\bar C\). The opposite occurs for \(H_2\) (i.e., negative in \(C\) and positive in \(\bar C\)). Let \(D\) and \(\bar D\) be the cells which we associate to \((+,+)\) and \((-, -)\) respectively.</p> <h5 class="env-title" id="claim">Claim</h5> <p>\(\mathbf{J}^{C}E_v + \mathbf{J}^{\bar C}E_v - \mathbf{J}^{D}E_v - \mathbf{J}^{\bar D}E_v = \mathbf{0}\).</p> <blockquote class="block-tip"> <p>To prove this, note that for each of \(C, \bar C, D\), and \(\bar D\), the function  in each column of \(\mathbf{J}E_v\) restricts in the interiors to a polynomial, expressible so that every monomial in this polynomial is degree at most 1 in the parameter coordinates, and is degree 1 or less in the coordinates of \(v\), as well.  Suppose that such a monomial is degree 0 in every parameter coordinate corresponding to these two hyperplanes – that is, it is degree 0 in every one of the \(2(n_{\ell-1}+1)\) coordinates for the rows of \((W^{\ell} | b^{\ell})\) that correspond to these neurons, and it is also degree 0 in all \(2n_{\ell+1}\) coordinates appearing in the columns of \(W^{\ell+1}\) that correspond to these two neurons. Then this monomial appears in a column of \(\mathbf{J}^{C}E_v\) if and only if it appears in the same column of \(\mathbf{J}^{\bar C}E_v, \mathbf{J}^{D}E_v\), and \(\mathbf{J}^{\bar D}E_v\). Note, since these hyperplanes come from a “hidden layer”, any monomial of \(E_x\), \(x\) in the interior of one of these cells, that is positive degree in one of the coordinates corresponding to the two hyperplanes must be degree 1 in <em>two</em> such coordinates – one from \((W^{\ell} | b^{\ell})\) and one from \(W^{\ell+1}\). Thus, in the \(2(n_{\ell-1}+1) + 2n_{\ell+1}\) columns for partials with respect to such coordinates every non-zero monomial is degree 1 in one of those coordinates.  Hence, all monomials in every column that have degree 0 in those coordinates will vanish in the summation \(\mathbf{J}^{C}E_v + \mathbf{J}^{\bar C}E_v - \mathbf{J}^{D}E_v - \mathbf{J}^{\bar D}E_v\). Now, suppose that some monomial in \(\mathbf{J}^{A}E_v\), for \(A = C, \bar C, D,\) or \(\bar D\), has degree 1 in one of these coordinates. It cannot be that \(A = \bar D\) since both of the neurons in question are unactivated in \(\bar D\).  Moreover, in each column, such a monomial occuring in \(\mathbf{J}^DE_v\) must be precisely the sum of monomials that separately occur in \(\mathbf{J}^CE_v\) and \(\mathbf{J}^{\bar C}E_v\). Since no monomial in \(\mathbf{J}^{A}E_v\) can be larger than degree 1 in these coordinates, this shows the equation holds.</p> </blockquote> <p>While the discussion of the claim discusses the Jacobian of the evaluation map at \(v\), it would appear that it holds at <em>any</em> point. Using that \(X\) is one of \(C, \bar C, D\), or \(\bar D\), the claim tells us that \(\mathbf{J}^XE_{v}\) (resp. \(\mathbf{J}^XE_{p}\)) is a linear combination of vectors \(\mathbf{J}^AE_{v}\) (resp. \(\mathbf{J}^AE_{p}\)), where \(A\) takes on the other three cells (in each of which we have a subset of \(Z\) that is affinely independent. Since, for each \(A\in \{C, \bar C, D, \bar D\} \setminus \{X\}\), we may write \(\mathbf{J}^AE_v\) and \(\mathbf{J}^AE_p\) as a linear combination of rows of \(\mathbf{J}E_Z\), this tells us that \(\mathbf{J}^XE_p = \mathbf{J}E_p\) can be expressed as a linear combination of rows of \(\mathbf{J}E_Z\).</p> <hr> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>I could have written \(z^{n_d}_j\) instead for the \(j\)-th coordinate function. However, this choice will make for simpler notation below. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:2"> <p>In other words, think of the function \(f_{j,z_i}:\Omega\to\mathbb R\) which is given by \(f_{j,z_i}(\theta) = F_{\theta,j}(z_i)\) and take partial derivatives of \(f_{j,z_i}\). <a href="#fnref:2" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:3"> <p>In terminology of [GLMW22], it is an <em>ordinary point</em>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:4"> <p>Shifting the index here, since the first hidden layer of the network of \(\theta\) has \(n_2\) neurons, and so on. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:5"> <p>The partial derivatives in \(\mathbf{J}F_{\theta}\lvert_{\Phi\circ A(y_k)}\) does not involve partials with respect to parameters, but with respect to spatial coordinates in \(\mathbb R^{n_1}\); i.e., with respect to coordinates \((x_1,x_2,\ldots,x_{n_1})\) in \(\mathbb R^{n_1}\). <a href="#fnref:5" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:6"> <p>We also assume that the polyhedral complex is generic and transversal so, in particular, any subset of the supporting hyperplanes that determine the facets \(X\cap X_i\) (which has cardinality \(\le n_0\)), has a non-empty intersection that is some face of \(X\). <a href="#fnref:6" class="reversefootnote" role="doc-backlink">↩</a></p> </li> <li id="fn:7"> <p>The facet is associated to the bent hyperplane for one neuron, \(\{x\ \lvert\ \tau^{\ell}_j(x) = 0\}\); for any weight or bias “leading to” that neuron, in row \(j\) of \(W^{\ell}\) or \(b^{\ell}\), the partial of \(E_x\) with respect to that weight or bias will be undefined. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">↩</a></p> </li> </ol> </div> </div> </article> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Christopher R. Cornwell. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"dropdown-publications",title:"publications",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-projects",title:"projects",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-blog",title:"blog",description:"",section:"Dropdown",handler:()=>{window.location.href="/blog/"}},{id:"nav-repositories",title:"repositories",description:"Edit the `_data/repositories.yml` and change the `github_users` and `github_repos` lists to include your own GitHub profile and repositories.",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in &#39;_pages/cv.md&#39;. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"nav-teaching",title:"teaching",description:"Materials for courses you taught. Replace this text with your description.",section:"Navigation",handler:()=>{window.location.href="/teaching/"}},{id:"nav-people",title:"people",description:"members of the lab or group",section:"Navigation",handler:()=>{window.location.href="/people/"}},{id:"post-analyzing-cdc-diabetes-health-indicators-data-and-svms",title:"Analyzing CDC Diabetes Health Indicators Data (and SVMs)",description:"a post about this publicly-available data, and using kernel-based SVMs",section:"Posts",handler:()=>{window.location.href="/blog/2025/CDCdiabetes-survey-gaussian-svm/"}},{id:"post-100-day-coding-challenge-log",title:"100-day Coding Challenge Log",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2025/code-challenge/"}},{id:"post-pre-decisive-sets-for-relu-networks",title:"Pre-decisive sets for ReLU networks",description:"on minimal sets that are subsets of decisive sets for ReLU networks",section:"Posts",handler:()=>{window.location.href="/blog/2025/JE-polyhedralrelations/"}},{id:"post-questions-on-functional-dimension-of-relu-networks",title:"Questions on functional dimension of ReLU networks",description:"understanding functional dimension by precomposing",section:"Posts",handler:()=>{window.location.href="/blog/2024/precomposing-funcdim/"}},{id:"post-hello-world",title:"Hello world",description:"a new academic site and blog",section:"Posts",handler:()=>{window.location.href="/blog/2024/hello-world/"}},{id:"post-testing-how-posts-work",title:"Testing how posts work",description:"testing out functionality of blog posts",section:"Posts",handler:()=>{window.location.href="/blog/2024/test-blog-post/"}},{id:"post-a-post-with-image-galleries",title:"a post with image galleries",description:"this is what included image galleries could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/photo-gallery/"}},{id:"post-a-post-with-tabs",title:"a post with tabs",description:"this is what included tabs in a post could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/tabs/"}},{id:"post-a-post-with-typograms",title:"a post with typograms",description:"this is what included typograms code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/typograms/"}},{id:"post-a-post-that-can-be-cited",title:"a post that can be cited",description:"this is what a post that can be cited looks like",section:"Posts",handler:()=>{window.location.href="/blog/2024/post-citation/"}},{id:"post-a-post-with-pseudo-code",title:"a post with pseudo code",description:"this is what included pseudo code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/pseudocode/"}},{id:"post-a-post-with-code-diff",title:"a post with code diff",description:"this is how you can display code diffs",section:"Posts",handler:()=>{window.location.href="/blog/2024/code-diff/"}},{id:"post-a-post-with-advanced-image-components",title:"a post with advanced image components",description:"this is what advanced image components could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/advanced-images/"}},{id:"post-a-post-with-vega-lite",title:"a post with vega lite",description:"this is what included vega lite code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/vega-lite/"}},{id:"post-a-post-with-geojson",title:"a post with geojson",description:"this is what included geojson code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/geojson-map/"}},{id:"post-a-post-with-echarts",title:"a post with echarts",description:"this is what included echarts code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/echarts/"}},{id:"post-a-post-with-chart-js",title:"a post with chart.js",description:"this is what included chart.js code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2024/chartjs/"}},{id:"post-a-post-with-tikzjax",title:"a post with TikZJax",description:"this is what included TikZ code could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/tikzjax/"}},{id:"post-a-post-with-bibliography",title:"a post with bibliography",description:"an example of a blog post with bibliography",section:"Posts",handler:()=>{window.location.href="/blog/2023/post-bibliography/"}},{id:"post-a-post-with-jupyter-notebook",title:"a post with jupyter notebook",description:"an example of a blog post with jupyter notebook",section:"Posts",handler:()=>{window.location.href="/blog/2023/jupyter-notebook/"}},{id:"post-a-post-with-custom-blockquotes",title:"a post with custom blockquotes",description:"an example of a blog post with custom blockquotes",section:"Posts",handler:()=>{window.location.href="/blog/2023/custom-blockquotes/"}},{id:"post-a-post-with-table-of-contents-on-a-sidebar",title:"a post with table of contents on a sidebar",description:"an example of a blog post with table of contents on a sidebar",section:"Posts",handler:()=>{window.location.href="/blog/2023/sidebar-table-of-contents/"}},{id:"post-a-post-with-audios",title:"a post with audios",description:"this is what included audios could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/audios/"}},{id:"post-a-post-with-videos",title:"a post with videos",description:"this is what included videos could look like",section:"Posts",handler:()=>{window.location.href="/blog/2023/videos/"}},{id:"post-displaying-beautiful-tables-with-bootstrap-tables",title:"displaying beautiful tables with Bootstrap Tables",description:"an example of how to use Bootstrap Tables",section:"Posts",handler:()=>{window.location.href="/blog/2023/tables/"}},{id:"post-a-post-with-table-of-contents",title:"a post with table of contents",description:"an example of a blog post with table of contents",section:"Posts",handler:()=>{window.location.href="/blog/2023/table-of-contents/"}},{id:"post-a-post-with-giscus-comments",title:"a post with giscus comments",description:"an example of a blog post with giscus comments",section:"Posts",handler:()=>{window.location.href="/blog/2022/giscus-comments/"}},{id:"post-a-post-with-redirect",title:"a post with redirect",description:"you can also redirect to assets like pdf",section:"Posts",handler:()=>{window.location.href="/assets/pdf/example_pdf.pdf"}},{id:"post-a-post-with-diagrams",title:"a post with diagrams",description:"an example of a blog post with diagrams",section:"Posts",handler:()=>{window.location.href="/blog/2021/diagrams/"}},{id:"post-a-distill-style-blog-post",title:"a distill-style blog post",description:"an example of a distill-style blog post and main elements",section:"Posts",handler:()=>{window.location.href="/blog/2021/distill/"}},{id:"post-a-post-with-twitter",title:"a post with twitter",description:"an example of a blog post with twitter",section:"Posts",handler:()=>{window.location.href="/blog/2020/twitter/"}},{id:"post-a-post-with-disqus-comments",title:"a post with disqus comments",description:"an example of a blog post with disqus comments",section:"Posts",handler:()=>{window.location.href="/blog/2015/disqus-comments/"}},{id:"post-a-post-with-math",title:"a post with math",description:"an example of a blog post with some math",section:"Posts",handler:()=>{window.location.href="/blog/2015/math/"}},{id:"post-a-post-with-code",title:"a post with code",description:"an example of a blog post with some code",section:"Posts",handler:()=>{window.location.href="/blog/2015/code/"}},{id:"post-a-post-with-images",title:"a post with images",description:"this is what included images could look like",section:"Posts",handler:()=>{window.location.href="/blog/2015/images/"}},{id:"post-a-post-with-formatting-and-links",title:"a post with formatting and links",description:"march &amp; april, looking forward to summer",section:"Posts",handler:()=>{window.location.href="/blog/2015/formatting-and-links/"}},{id:"news-a-simple-inline-announcement",title:"[//]: # A simple inline announcement.",description:"",section:"News"},{id:"news-congrats-to-the-2024-reu-participants-on-their-wonderful-final-projects-see-you-in-seattle-sparkles-smile",title:"Congrats to the 2024 REU participants on their wonderful final projects. See you...",description:"",section:"News"},{id:"news-math-371-new-course-in-spring-2025",title:"MATH 371, new course in Spring 2025",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-awesome-job-to-my-2025-reu-students-natalie-ethan-gabriel-and-grace-the-presentation-and-poster-at-jmm-2025-in-seattle-were-great-related-preprint-on-arxiv-https-arxiv-org-abs-2412-06829-we-missed-you-there-grace",title:"Awesome job to my 2025 REU students Natalie, Ethan, Gabriel, and Grace. The...",description:"",section:"News"},{id:"news-getting-summer-going",title:"Getting summer going",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_5/"}},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"social-email",title:"email",section:"Socials",handler:()=>{window.open("mailto:%79%6F%75@%65%78%61%6D%70%6C%65.%63%6F%6D","_blank")}},{id:"social-inspire",title:"Inspire HEP",section:"Socials",handler:()=>{window.open("https://inspirehep.net/authors/1010907","_blank")}},{id:"social-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"social-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=qc6CJjYAAAAJ","_blank")}},{id:"social-custom_social",title:"Custom_social",section:"Socials",handler:()=>{window.open("https://www.alberteinstein.com/","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>