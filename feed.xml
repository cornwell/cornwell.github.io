<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://cornwell.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://cornwell.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-03-03T03:29:17+00:00</updated><id>https://cornwell.github.io/feed.xml</id><title type="html">Christopher R. Cornwell</title><subtitle>A personal blog for academic research at the intersection of mathematics and data science. </subtitle><entry><title type="html">Pre-decisive sets for ReLU networks</title><link href="https://cornwell.github.io/blog/2025/JE-polyhedralrelations/" rel="alternate" type="text/html" title="Pre-decisive sets for ReLU networks"/><published>2025-03-02T14:59:00+00:00</published><updated>2025-03-02T14:59:00+00:00</updated><id>https://cornwell.github.io/blog/2025/JE-polyhedralrelations</id><content type="html" xml:base="https://cornwell.github.io/blog/2025/JE-polyhedralrelations/"><![CDATA[<p>In a <a href="https://cornwell.github.io/blog/2024/precomposing-funcdim/">post last December</a>, I wrote some of my thoughts about Section 8 in the <a href="https://arxiv.org/abs/2209.04036">paper by Grigsby, Lindsey, Meyerhoff, and Wu</a>, and the effect of precomposing with a layer. Something that I continued to think about, with collaborator Na Zhang, was how to leverage what that Section says about \(\dim_{fun+}(\theta)\) versus \(\dim_{fun}(\theta)\) for this purpose.</p> <p>It lead to some basic observations, but we think they’ll be helpful.</p> <hr/> <h2 id="notation">Notation</h2> <p>We are considering feed-forward neural networks with ReLU activation function. Suppose that the input space is \(\mathbb R^{n_0}\) and that the parameter space is \(\Omega\) (which may be identified with \(\mathbb R^D\) as a set, \(D\) being the total number of parameters – weights and biases). We fix a parameter \(\theta\in\Omega\), and so determine a network \(\mathcal N(\theta)\), and we assume \(\theta\) is “nice” in the sense of the source paper (it is <em>generic</em>, <em>transversal</em>, and <em>combinatorially stable</em>). The network has an associated network function \(F_\theta:\mathbb R^{n_0}\to\mathbb R\).<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <p>Associated to \(\mathcal N(\theta)\) is a <em>canonical polyhedral complex</em> \(\mathcal C(\theta)\) that decomposes \(\mathbb R^{n_0}\). A <strong>top cell</strong> of \(\mathcal C(\theta)\) refers to an \(n_0\)-dimensional cell of that polyhedral complex. Note that the restriction of \(F\) to any top cell consists of an affine linear function on that top cell (cf. Lemma 2.8 of [GLMW]).</p> <p>Consider a finite subset \(Z = \{z_1,\ldots,z_m\}\subset\mathbb R^{n_0}\). For a parameter \(v\), the evaluation map \(E_Z\) is defined by \(E_Z(v) = (F_v(z_1),\ldots,F_v(z_m))\). Supposing that points in \(Z\) are parameterically smooth for \(\theta\), there is a neighborhood \(V\subset\Omega\) of \(\theta\) on which \(E_{Z}:V \to \mathbb R^{m}\) is defined and differentiable. I will pay particular attention to the singleton case, \(Z = \{z\}\), where \(z\) is a point in the interior of some top cell for \(\theta\). In this case, we write the Jacobian (gradient) as \(JE_z\vert_{\theta}\) (or, simply \(JE_z\)).</p> <p>Let \(R\) be a top cell of the canonical polyhedral complex for \(\mathcal N(\theta)\) and suppose that \(\{x_0,x_1,\ldots,x_{n_0}\}\) is affinely independent and contained in the interior of \(R\). I observed in my last post, using some basic linear algebra, that if \(x\) is any point in the interior of \(R\) then we have the equation</p> <p>\begin{equation} \label{eqn:JE-lincombo} JE_x - JE_{x_0} = \sum_{i=1}^{n_0} c_i(JE_{x_i} - JE_{x_0}), \end{equation} where the vector \(x-x_0\) satsifies \(x-x_0 = \sum_{i=1}^{n_0} c_i(x_i - x_0)\). In short, the reason for this is that, while \(JE_z\) consists of components that are polynomials in the parameters of the network, if we compute the partial derivatives and then consider it as a function of \(z\) (in \(R\)), rather than parameters, then it is affine linear.</p> <p>Note that (\ref{eqn:JE-lincombo}) fails if \(x\) is contained in another top cell than \(R\). However, if we replace \(F\) by the affine linear function (extended over \(\mathbb R^{n_0}\)) that agrees with \(F\vert_{R}\), and discuss the Jacobian of the evaluation map of that function, then the expression would be valid for all \(x\in\mathbb R^{n_0}\). For that Jacobian, of the evaluation map corresponding to \(F\vert_{R}\), at \(x\), we use the notation \(JE_x^{R}\) and we call this the <strong>Jacobian at</strong> \(R\).</p> <hr/> <h2 id="relations-on-jacobians-at-adjacent-top-cells">Relations on Jacobians at adjacent top cells</h2> <p>Consider two hyperplanes \(H_1\) and \(H_2\) in \(\mathbb R^{n_0}\) which correspond to two of the neurons from layer 1 of \(\mathcal N(\theta)\). As \(\theta\) is generic, \(H_1\cap H_2\) is an \((n_0-2)\)-dimensional affine subspace. Choose a point \(q\) in the intersection that is not contained in any other (bent) hyperplane from another layer (i.e., \(q\) is in the relative interior of an \((n_0-2)\)-dimensional face of \(\mathcal C(\theta)\) contained in both \(H_1\) and \(H_2\)). Suppose that we have top cells \(R, S, A,\) and \(B\) which all contain \(q\) in one of their \((n_0-2)\)-dimensional faces; every neighborhood of \(q\) intersects in a non-empty set with each of \(R, S, A,\) and \(B\), and this is true of no other top cell. Furthermore, choose our naming so that:</p> <ul> <li>\(R\) and \(S\) do not share a facet;</li> <li>\(A\) and \(B\) do not share a facet.</li> </ul> <p>We’ll check a certain relation on Jacobians at these cells. Let \(x\) be a point, and say that \(z^R_0,z^S_0,z^A_0,\) and \(z^B_0\) are points in \(R, S, A,\) and \(B\), respectively.</p> <p>Since each component function of \(JE^R\) is affine linear, there is some matrix \(W^R\) so that \(JE^R_x - JE^R_{z^R_0} = W^R(x - z^R_0)\).</p> <p>Choose one component (entry) of \(JE_x^R\), denote it by \(p^R_x\), and denote the corresponding entry of \(JE_x^A\) by \(p^A_x\). WLOG, we may assume that \(R\) and \(A\) are on the same side of the hyperplane \(H_2\), meaning that they share a facet that is contained in \(H_1\). Denote the weights and bias corresponding to \(H_1\) by \(w^1_1, w^1_2, \ldots, w^1_{n_0}, b^1\). Also, for the set of all weight/bias (variables) for the network except those \(n_0+1\) variables for \(H_1\), use notation \(\Theta\).</p> <p>We may write \(p^R_x\) and \(p^A_x\) as polynomials in \((\mathbb R[\Theta])[w^1_1,\ldots,w^1_{n_0},b^1]\). These polynomials have degree at most 1 (in these variables).</p> <p>Furthermore, since the ternary labelings of \(R\) and \(A\) agree for all neurons except \(H_1\), the only difference between \(JE^R_x - JE^R_{z^R_0}\) and \(JE^A_x - JE^A_{z^R_0}\) is that one of them has degree 1 terms in \((\mathbb R[\Theta])[w^1_1,\ldots,w^1_{n_0},b^1]\) and those are missing in the other. In other words, \(JE^R_x - JE^A_x\) entirely consists of such degree 1 terms, each of which contains a factor of a component in \((x - z^R_0)\).</p> <p>One can do the same thing with \(JE^B_x-JE^S_x\), and must subtract in this order to get the signs on these degree 1 terms to match. There are no additional (or missing) non-zero terms in this, since, even though the ternary label for \(H_2\) has changed in these cells, we have a lemma.</p> <h5 class="env-title" id="lemma">Lemma.</h5> <p>For \(X \in\{R,S,A,B\}\), if a term in \(p^X_x\) is degree 1 in one of the weights and biases from \(H_2\), then it is degree 0 in \(\{w^1_1,\ldots,w^1_{n_0},b^1\}\).</p> <blockquote> <p><em>Proof</em>.    The statement holds because neurons for \(H_1\) and \(H_2\) are in the same layer. Thus, from the compositional structure of the feed-forward neural network, their weights/biases are never jointly in the same monomial of \(E_x\) (as a polynomial in the parameter variables). \(\blacksquare\)</p> </blockquote> <p>We have, therefore, that \(JE^R_x - JE^A_x = JE^B_x - JE^S_x\). By rearranging, \begin{equation} JE^R_x + JE^S_x = JE^A_x + JE^B_x \end{equation}</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Often, I will simply write the network function as \(F:\mathbb R^{n_0}\to\mathbb R\), leaving the parameter of the network as understood.) <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="math"/><category term="neural_networks"/><category term="research"/><category term="thoughts"/><category term="ReLU_networks"/><summary type="html"><![CDATA[on minimal sets that are subsets of decisive sets for ReLU networks]]></summary></entry><entry><title type="html">Questions on functional dimension of ReLU networks</title><link href="https://cornwell.github.io/blog/2024/precomposing-funcdim/" rel="alternate" type="text/html" title="Questions on functional dimension of ReLU networks"/><published>2024-12-20T10:39:00+00:00</published><updated>2024-12-20T10:39:00+00:00</updated><id>https://cornwell.github.io/blog/2024/precomposing-funcdim</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/precomposing-funcdim/"><![CDATA[<p>I first started writing this post as a draft at my old Wordpress website. I’m attempting to port it here. Please be patient if I missed a spot where some syntax or formatting needed to be translated.</p> <hr/> <p>This is one of a series of posts about ReLU neural networks. My focus in this post is on the <strong>functional dimension</strong> of (a parameter for) a ReLU neural network. <br/>      See <em>Functional dimension of feedforward ReLU neural networks</em> by Grigsby, Lindsey, Meyerhoff, and Wu (<a href="https://arxiv.org/abs/2209.04036">link to the preprint</a>).</p> <p>I will abbreviate references to this paper by writing [GLMW22].  In particular, I want to work on understanding Lemmas 8.2 and 8.5, as well as Theorem 8.7 in that paper – having to do with how the functional dimension behaves under composition of networks.</p> <h2 id="setup-and-notation">Setup and notation</h2> <hr/> <p>The notation used will largely match that of [GLMW22]. We’ll be working with feedforward ReLU neural networks and their associated network function. Here is the general setup.</p> <p><strong>Definition 1.</strong> Let \(n_0\in \mathbb N\) and \(d \in \mathbb N\) and let \(\phi:\mathbb R \to \mathbb R\) be a continuous function. A <strong>feedforward neural network</strong> \(\mathcal N\) defined on \(\mathbb R^{n_0}\), with <em>depth</em> \(d\) and <em>activation function</em> \(\phi\), is an ordered sequence of \(d\) affine maps \(A^1, A^2, \ldots, A^d,\) with associated positive integers \(n_1,n_2,\ldots,n_d,\) that are defined such that \(A^{\ell}:\mathbb R^{n_{\ell-1}} \to \mathbb R^{n_{\ell}}\) for every \(1 \le \ell \le d.\) For any \(n\in \mathbb N\), define \(\Phi:\mathbb R^n\to\mathbb R^n\) as the component-wise application of the function \(\phi\) on the input. The <strong>network function</strong> associated to \(\mathcal N\) is the function \(F:\mathbb R^{n_0}\to \mathbb R^{n_d}\) that is defined by</p> \[F(\mathbf{x}) = A^d\circ \Phi\circ A^{d-1} \circ \ldots \circ \Phi\circ A^{2} \circ \Phi\circ A^{1}(\mathbf{x})\] <p>for \(\mathbf{x} \in \mathbb R^{n_0}\). The list \((n_0, n_1, \ldots, n_d)\) is called the <strong>architecture</strong> of the neural network \(\mathcal N\).</p> <p><br/></p> <p>In this post the focus is exclusively on <em>ReLU (feedforward) neural networks</em> which are feedforward neural networks that have an activation function given by \(\phi(x) = \max\{0, x\}\), for \(x\in \mathbb R\).</p> <p>For every \(1\le \ell\le d-1\), the <strong>\(\ell\)-th layer map</strong> \(F^{\ell}:\mathbb R^{n_{\ell-1}} \to \mathbb R^{n_{\ell}}\) is defined by \(F^{\ell} = \Phi\circ A^{\ell}\), with the \(d\)-th layer map being simply \(A^d\).  For every \(1\le \ell\le d\) and every \(1\le j\le n_{\ell}\), the pair \((\ell, j)\) is referred to as a <strong>neuron</strong> of \(\mathcal N\); sometimes this <strong>neuron</strong> refers to the function \(z^{\ell}_j : \mathbb R^{n_0} \to \mathbb R\) that takes on the (“pre-activation”) values at \((\ell, j)\), i.e., letting \(\pi_j\) denote projection to the \(j\)-th coordinate, we have that \(z^{\ell}_j = \pi_j \circ A^{\ell}\circ F^{\ell-1} \circ\ldots\circ F^1\).</p> <p>Note that each affine map \(A^{\ell}\) may be expressed in coordinates, so that for \(\mathbf{x}\in\mathbb R^{n_{\ell-1}}\), we have \(A^{\ell}(\mathbf{x}) = W^{\ell}\mathbf{x} + \mathbf{b}^{\ell}\) where \(W^{\ell} \in \mathbb R^{n_{\ell}\times n_{\ell-1}}\) is a “weight” matrix and \(\mathbf{b}^{\ell} \in \mathbb R^{n_\ell}\) is a “bias” vector.</p> <p>Thinking of ReLU neural networks with a given architecture \((n_0, n_1, \ldots, n_d)\) – or, more appropriately, the associated network functions for such networks – as a parameterized class of functions, the entries in the weight matrices and bias vectors for the network’s layers make up the parameters. We organize the parameters as \(\theta = (W^1, \mathbf{b}^1, \ldots, W^d, \mathbf{b}^d)\), which we associate in some chosen manner to a point in \(\mathbb R^D\), with \(D = D(n_0,\ldots,n_d) := \sum_{i=1}^d n_i(n_{i-1}+1).\) We write \(\Omega := \mathbb R^D\) in order to emphasize the space of parameters – and how it corresponds to weights and biases in \(\mathcal N\). At times, in order to denote the neural network with parameters \(\theta\), for an understood architecture, we write \(\mathcal N(\theta)\). Additionally, the associated network function may be written as \(F_\theta: \mathbb R^{n_0} \to \mathbb R^{n_d}.\)</p> <h2 id="functional-dimension-of-a-parameter-for-a-relu-neural-network">Functional Dimension of a Parameter for a ReLU Neural Network</h2> <hr/> <p>The lemmas and theorem that we want to understand better are related to a quantity called \(\dim_{fun}(\theta)\), the functional dimension of \(\theta\) (for a ReLU network \(\mathcal N(\theta)\)). Some effort is required to define this quantity. For the definition, there are a few technical assumptions that must be made about \(\theta\), but the assumptions are not very restrictive. To the contrary, they rule out some rare situations for the network. In order to arrive at the definition of \(\dim_{fun}(\theta)\) quickly, however, for now we bypass describing the technical assumptions; we will say simply that \(\theta\) is “nice” (or, “satisfies nice conditions”) and move forward.</p> <p>Consider a fixed architecture \((n_0, n_1, \ldots, n_d)\) and the class of ReLU networks with this architecture. Given some \(\theta\in\Omega\), for \(1\le j\le n_d\), use \(F_{\theta,j}:\mathbb R^{n_0}\to\mathbb R\) to denote the \(j\)-th coordinate function of \(F_\theta\), so for every \(\mathbf{x}\in\mathbb R^{n_0}\) we have \(F_\theta(\mathbf{x}) = (F_{\theta,1}(\mathbf{x}), \ldots, F_{\theta,n_d}(\mathbf{x}))\).<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> Additionally, consider a finite ordered set of points \(Z = \{z_1, z_2, \ldots, z_k\} \subset \mathbb R^{n_0}\).  The evaluation map at \(Z\), \(E_Z: \Omega \to \mathbb R^{k\cdot n_d}\) is given by setting, for every \(\theta\in\Omega\),</p> \[E_Z(\theta) = (F_{\theta,1}(z_1), \ldots, F_{\theta,n_d}(z_1), \ldots, F_{\theta,1}(z_k), \ldots, F_{\theta,n_d}(z_k)).\] <p>To define the functional dimension, we use the Jacobian matrix of the evaluation map, which we denote by \(\mathbf{J}E_Z\).  In other words, for each \(z_i\in Z\), let \(\mathbf{J}E_{z_i}\lvert_{\theta}\) be the \(n_m \times D\) matrix, the \(j\)-th row of which is the vector of partial derivatives of \(F_{\theta, j}(z_i)\) <em>with respect to the</em> \(D\) <em>parameters</em>,<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>  evaluated at \(\theta\). The matrix \(\mathbf{J}E_Z\lvert_{\theta}\) is the \(k\cdot n_m \times D\) matrix obtained by stacking these matrices. There are values of \(\theta\) at which some of the partial derivatives in \(\mathbf{J}E_Z\) are not well-defined – hence, the reason for making “nice” assumptions.</p> <p><strong>Definition 2.</strong> Fix an architecture \((n_0, n_1, \ldots, n_d)\) and let \(\Omega\) be its corresponding parameter space. Suppose that \(\theta\in\Omega\) satisfies nice conditions. <sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> The <strong>functional dimension</strong> at \(\theta\) is defined to be</p> \[\dim_{fun}(\theta) = \underset{Z \text{ is finite and smooth for } \theta}{\sup}\operatorname{rank} \mathbf{J}E_Z\lvert_{\theta}.\] <p>In Definition 2, the condition that \(Z\) be smooth for \(\theta\) (or, <em>parametrically smooth</em> in [GLMW22]) refers to the function \(\mathcal{F}:\Omega\times\mathbb R^{n_0} \to \mathbb R^{n_d}\), given by \(\mathcal{F}(\theta, x) = F_\theta(x)\), being smooth at \((\theta, z_i)\) for every \(z_i\in Z\). One can also define the so-called <strong>batch functional dimension</strong> at \(\theta\) for a batch \(Z\), which is equal simply to the rank of \(\mathbf{J}E_Z\lvert_{\theta}\). Clearly, the batch functional dimension is less than or equal to the functional dimension.</p> <p>In Section 5 of [GLMW22], for a given network \(\mathcal N\) with parameter \(\theta\), the authors of that paper describe a finite set in \(\mathbb R^{n_0}\) for which the batch functional dimension for that set is equal to \(\dim_{fun}(\theta)\). To confirm that a <em>given</em> \(Z \subset \mathbb R^{n_0}\) is such a finite set, called a <strong>decisive set</strong> for \(\mathcal N(\theta)\), requires knowledge of the regions in \(\mathbb R^{n_0}\) on which \(F_{\theta}\) is affine-linear; more precisely, for each top-dimensional cell of the “canonical polyhedral complex” of \(F_\theta\), a decisive set contains the vertices of some \(n_0\)-dimensional simplex contained in that cell. Provided that \(\theta\) is “nice” and \(Z\subset\mathbb R^{n_0}\) is a decisive set, we have \(\dim_{fun}(\theta) = \operatorname{rank}\mathbf{J}E_Z\lvert_{\theta}\).</p> <p>A discussion of the “canonical polyhedral complex” of \(F_\theta\) won’t happen in this post. Perhaps I will describe it in another post. Suffice it to say that it consists, in part, of a set of (<em>top-dimensional</em>) polyhedral cells, no two of which intersect in their interiors, but where the union of their closures is the entire domain \(\mathbb R^{n_0}\); furthermore, \(F_\theta\) is affine-linear when restricted to any one of these cells.</p> <p>Finally, we will be interested in a certain restricted notion of functional dimension, when we consider only sets \(Z\) which are in the interior of the positive orthant \(\mathbb R_{&gt;0}^{n_0} = \{(x_1,\ldots,x_{n_0})\ \lvert\ x_i &gt; 0 \text{ for all } 1\le i\le n_0\}\). Write this as</p> \[\dim_{fun+}(\theta) = \underset{Z\subset\mathbb R_{&gt;0}^{n_0} \text{ is finite and smooth for } \theta}{\sup}\operatorname{rank} \mathbf{J}E_Z\lvert_{\theta}.\] <p>After a small change to the notation, the following is the statement of Lemma 8.2 in [GLMW22].</p> <h5 class="env-title" id="lemma-82">Lemma 8.2</h5> <p>Fix \(n_0\in\mathbb N\) and let \(\Omega\) be the parameter space for architecture \((n_1,n_2,\ldots,n_d)\). Let \(\theta\in\Omega\) be such that there is a smooth point for \(\theta\) in the strictly positive orthant \(\mathbb R_{&gt;0}^{n_1}\). Let \(A:\mathbb R^{n_0}\to\mathbb R^{n_1}\) be affine-linear such that every row of the associated matrix has at least one non-zero entry. Use \((A, \theta)\) to denote the parameter for architecture \((n_0,n_1,\ldots,n_d)\) that precomposes with \(A\), i.e., \(\Phi\circ A\) is the first layer map of the network function \(F_{(A,\theta)}\). <br/> If \((A, \theta)\) satisfies nice conditions, then \(\dim_{fun}(A,\theta) \le n_0n_1 + \dim_{fun+}(\theta)\). Furthermore, for this inequality to be equality it is necessary that \(\dim_{fun+}(\theta)\) be realized on a smooth set \(Z^* \subset \operatorname{Im}(\Phi\circ A)\).</p> <blockquote class="block-tip"> <h5 id="proof-sketch">Proof sketch</h5> <p>The proof of this lemma uses the scaling-inverse scaling invariance of \(F_{(A,\theta)}\), in the first hidden layer. The authors use this to identify the parameter space for \((A,\theta)\) with coordinates in the product \(\mathbb R_{&gt;0}^{n_1} \times (\mathbb R^{n_0})^{n_1} \times \Omega\). Then they argue that, for \(Z\subset\mathbb R^{n_0}\), the columns of \(\mathbf{J}E_Z\lvert_{(A,\theta)}\) that correspond to parameters in \(\mathbb R_{&gt;0}^{n_1}\) will be zero, and that the columns corresponding to parameters in \((\mathbb R^{n_0})^{n_1}\) contribute at most \(n_0n_1\) to the rank.</p> <p>Finally, using that \(\Phi\circ A(Z)\) is a subset of  \(\mathbb R_{\ge 0}^{n_1}\), they argue that the rank of columns corresponding to parameters in \(\Omega\) will at most be the \(\sup\) of the rank of \(\mathbf{J}\) of the evaluation map on subsets in \(\mathbb R_{&gt;0}^{n_1}\), evaluated at \(\theta\), which equals \(\dim_{fun+}(\theta).\) \(\blacksquare\)</p> </blockquote> <p>Note that all of the assumptions in Lemma 8.2 that occur before the word “Furthermore” will be true of a full measure subset in the parameter space for \((A, \theta)\). This is proven in [GLMW22] and remarked upon at the beginning of subsection 8.1.  Moreover, if \(n_0 \ge n_1\) then there is a full measure subset of parameters so that \(A\) is surjective, in which case \(\operatorname{Im}(\Phi\circ A) = \mathbb R_{\ge 0}^{n_1}\). By definition, this contains any subset \(Z^*\) that realizes \(\dim_{fun+}(\theta)\). So, if \(n_0 \ge n_1\) then a full measure set of parameters satisfies that necessary condition.</p> <p>However, if \(n_0 &lt; n_1\) then \(\operatorname{Im}(\Phi\circ A)\) cannot contain a set of points in \(\mathbb R_{&gt;0}^{n_1}\) that are vertices of an \(n_1\)-simplex – since \(\operatorname{Im}(\Phi\circ A) \cap \mathbb R_{&gt;0}^{n_1}\) is contained in an \((n_1-1)\)-dimensional affine subspace. This means that any decisive set that realizes \(\dim_{fun+}(\theta)\) is not in the image. However, if \(\dim_{fun+}(\theta)\) can be realized on a set that is <em>not</em> decisive, perhaps it is possible in such a situation to satisfy this condition.</p> <h6 class="env-title" id="question">Question</h6> <p>How do we understand the difference between \(\dim_{fun+}(\theta)\) and \(\dim_{fun}(\theta)\)?</p> <h2 id="technical-lemma-85">Technical Lemma 8.5</h2> <hr/> <p>Let’s discuss Lemma 8.5. We’ll need, at least, the notion of the ternary labeling at \(x \in \mathbb R^{n_0}\), determined by \(\theta\). At this juncture, a (minor) reckoning has arrived. As is common for parameterized classes of functions, and in mathematical modeling, we used the notation \(\theta\) for our parameter vector in \(\mathbb R^{D}\), with \(D\) being the total number of weights and biases in network architecture \((n_0,n_1,\ldots,n_d)\). However, the letter \(\theta\) is also used in the literature to denote ternary labelings, which are functions \(\mathbb R^{n_0} \to \{-1, 0, 1\}^{N}\) with \(N = n_1+n_2+\ldots+n_d\). As a solution, we will try using the letter \(\tau.\)</p> <p>So, a definition. For this definition, recall for each neuron \((\ell, j)\) the pre-activation function \(z^{\ell}_j:\mathbb R^{n_0}\to\mathbb R\), when \(1\le \ell\le n_d\) and \(1\le j\le n_\ell.\)</p> <p><strong>Definition 3.</strong> Let \(\theta\in\Omega\) be the parameter for a network with architecture \((n_0,n_1,\ldots,n_d)\). For each neuron \((\ell, j)\) of the network, define \(\tau^\ell_j:\mathbb R^{n_0} \to \{-1, 0, 1\}\) by setting \(\tau^\ell_j(x) = \text{sign}(z^\ell_j(x))\). (Here, the function \(\text{sign}\) returns 0 if the input is zero, 1 if it is positive, and -1 if it is negative.) The (full) <strong>ternary labeling</strong> for the network is the function \(\tau: \mathbb R^{n_0} \to \{-1, 0, 1\}^{N}\) which has a coordinate function for each neuron \((\ell, j)\), namely the function \(\tau^\ell_j.\)</p> <p>Intuitively speaking, the value of \(\tau^\ell_j(x)\) is positive if and only if the neuron \((\ell, j)\) is “on” or “activated” at the point \(x\). It is zero at \(x\) if and only if \(F_{\ell-1}\circ\ldots\circ F_1(x)\) lies inside of the hyperplane \(\{y\ \lvert\ W^\ell y + b^\ell = 0\}.\)</p> <p>For Lemma 8.5, we have a similar setup to Lemma 8.2. There is a parameter \(\theta\in\Omega\), for a network with architecture \((n_1,\ldots, n_d)\), an affine-linear map \(A:\mathbb R^{n_0}\to\mathbb R^{n_1}\) (we may also use \(A\) for the \(n_1\times(n_0+1)\) matrix that determines it, the last column for the translate, or “shift”). We are interested in precomposing with \(A\) to get a neural network with architecture \((n_0,n_1,\ldots,n_d)\) and with parameter \((A, \theta)\).  Note that, before precomposing, there are coordinate ternary labelings \(\tau^\ell_j\) for every  \(1\le \ell\le n_d-1\) and \(1\le j\le n_{\ell+1}.\)<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup></p> <h5 class="env-title" id="lemma-85">Lemma 8.5</h5> <p>Suppose that \(\theta\) and \(A\) are as above and that \(\theta\) satisfies “nice conditions.”  For \(1\le j\le n_1\) and \(x\in \mathbb R^{n_0}\), use \(\tau^A_j(x)\) to denote the ternary label (at \(x\)) with respect to the \(j\)th row of \(A\).  Assume that \(A\) is non-degenerate, in the sense that for \(1\le j\le n_1\) the set where \(\tau^A_j(x) = 0\) is a hyperplane in \(\mathbb R^{n_0}\). We suppose that for every \(1\le k\le n_1\), there exists \(y_k\in\mathbb R^{n_0}\) such that <br/>      (i) \(\tau^A_k(y_k) = 0\), <br/>      (ii) \(\tau^A_j(y_k) \ne 0\) for all \(j \ne k\), <br/>      (iii) for all \((\ell, j)\) with \(1\le \ell\le n_d-1\) and \(1\le j\le n_{\ell+1}\), we have that \(\tau^\ell_j( \Phi\circ A(y_k) ) \ne 0\), and <br/>      (iv) the \(k\)th column of \(\mathbf{J}F_{\theta}\lvert_{\Phi\circ A(y_k)}\) is not the zero vector.<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> <br/> Then there is a finite set \(Z\) in \(\mathbb R^{n_0}\) such that (v) up to scaling rows of \(A\) by positive numbers, each entry of \(A\) is given by a unique affine-linear combination of the coordinates of the vector \(E_Z(A, \theta)\); and (vi) the ternary labeling for \((A, \theta)\) of every point in \(Z\), at every neuron, is non-zero. <br/> Finally, the lemma also states that if \(y_k \in \mathbb R_{&gt;0}^{n_0}\) for every \(k\) then (vii) the set \(Z\) can be chosen to be in \(R_{&gt;0}^{n_0}\).</p> <p><br/></p> <p>Now, finding a set \(y_1, \ldots, y_{n_1}\) that satisfy conditions (i) - (iii) is generically possible – the conditions (i) and (ii) can be guaranteed as long as the hyperplane arrangement in \(\mathbb R^{n_0}\) associated to \(A\) is generic; condition (iii) says that each of the \(n_1\) points \(\Phi\circ A(y_k)\) is contained in the interior of a top-dimensional cell of the canonical polyhedral decomposition for \(\mathcal N(\theta)\) and this can be achieved for a choice of \(y_1,\ldots,y_{n_1}\) by a perturbation of \(\theta\).</p> <p>However, there is a positive measure subset of parameters for which condition (iv) will be impossible to satisfy. For example, there is a positive measure subset such that \(F_\theta\) is a constant function on all of \(\mathbb R^{n_1}\). Using \(H_k\) to denote the hyperplane where \(\tau^A_k\) is zero, there is a larger subset on which \(F_\theta\) is constant on the set \(\Phi\circ A(H_k)\) (and in any “nearby” choice of \((A,\theta)\) too).</p> <blockquote class="block-tip"> <h5 id="proof-sketch-of-lemma-85">Proof sketch of Lemma 8.5</h5> <p>To prove Lemma 8.5, the authors of [GLMW22] first note that, as a consequence of the assumptions (i) - (iii), for each \(1\le k\le n_1\), there is an open neighborhood \(U_k\) of \(y_k\) so that the ternary labelings of all neurons in \(\mathcal N(A, \theta)\) are constant on \(U_k\) except for \(\tau^A_k\). Furthermore, letting \(U_k^+\) and \(U_k^-\) denote the connected components of \(U_k \setminus \{x\ \lvert\ \tau^A_k(x) = 0\}\), with sign of \(\tau^A_k\) on each component matching the superscript, we have that the ternary labeling on every neuron of \((A, \theta)\) is constant on \(U_k^+\) and on \(U_k^-\). As a consequence, \(F_{(A,\theta)}\) is affine-linear when restricted to either \(U_k^+\) or \(U_k^-\), and so \(\mathbf{J}F_{(A,\theta)}\) is constant on each of \(U_k^{\pm}\). Furthermore, \(\mathbf{J}F_\theta\) is constant when restricted to \(\Phi\circ A(U_k)\).</p> <p>Next, they show that \(\mathbf{J}F_{(A,\theta)}\lvert_{U_k^+} \ne \mathbf{J}F_{(A,\theta)}\lvert_{U_k^-}\). To do so, they use the chain rule and that \(F_{(A,\theta)} = F_\theta\circ (\Phi\circ A)\). Then, since \(\mathbf{J}(\Phi\circ A)\lvert_{U_k^+}\) contains a non-zero element in the \(k\)th row, and \(\mathbf{J}(\Phi\circ A)\lvert_{U_k^-}\) has a zero \(k\)th row, assumption (iv) guarantees a non-zero difference in one of the entries of \(\mathbf{J}F_{(A,\theta)}\lvert_{U_k^+} - \mathbf{J}F_{(A,\theta)}\lvert_{U_k^-}.\)</p> <p>Having determined that \(\mathbf{J}F_{(A,\theta)}\lvert_{U_k^+} \ne \mathbf{J}F_{(A,\theta)}\lvert_{U_k^-}\), they have a lemma (Lemma 8.3) that gives the conclusion (v). This lemma produces a set \(Z \subset U_k^+ \cup U_k^-\), which means that (vi) holds (by construction of \(U_k^{\pm}\)) and that (vii) must hold – making the neighborhood \(U_k\) smaller if needed. \(\blacksquare\)</p> </blockquote> <h5 class="env-title" id="lemma-83">Lemma 8.3.</h5> <p>Let \(M\) be a polyhedral complex embedded in \(\mathbb R^d\), \(d\ge 1\), and let \(F:\mathbb R^d \to \mathbb R^{n}\) be a continous map that is affine-linear on cells of \(M\). Let \(X, Y\) be two \(d\)-dimensional cells of \(M\) that share a \((d-1)\)-dimensional facet, and denote the hyperplane containing the shared facet by \(H\). Assume that \(\mathbf{J}F\lvert_X \ne \mathbf{J}F\lvert_Y\). Then, for any decisive sets, \(S_X\subset X\) for \(F\lvert_X\) and \(S_Y\subset Y\) for \(F\lvert_Y\), \(H\) is the solution set to an affine-linear equation \(\{x\ \lvert\ c + Ax = \mathbf{0}\}\) where every entry of \(A\) is an affine linear expression in the coordinates of \(E_{S_X\cup S_Y}(F)\). The matrix \(A\) is unique up to rescaling rows by constants.</p> <blockquote class="block-tip"> <h5 id="proof-sketch-1">Proof sketch</h5> <p>To prove Lemma 8.3, write the points \(S_X = \{z_0,z_1,\ldots,z_d\}\) (which are vertices of a \(d\)-dimensional simplex in \(X\), owing to the fact that \(F\lvert_X\) is affine-linear). Now, since the vectors \(u_i := z_i - z_0\), with \(1\le i\le d\), make a basis of \(\mathbb R^d\), each partial derivative \(\partial F/\partial x_i\) is a linear combination of the directional derivatives \(D_{u_i}F(z_0)\). Additionally, \(\lvert z_i-z_0\rvert D_{u_i}F(z_0)\) is the difference between two coordinates of \(E_{S_X}(F)\). And so, for every \(x\in X\), each entry of \(\mathbf{J}F\lvert_x\) is a linear combination of coordinates of \(E_{S_X}(F)\).  This is similarly true for \(\mathbf{J}F\lvert_y\), \(y\in Y\), and \(E_{S_Y}(F)\).</p> <p>Note that the extension to \(\mathbb R^d\) of the affine-linear map \(F\lvert_X\) can be expressed as \(\mathbf{x} \mapsto c_X + \mathbf{J}F\lvert_X\mathbf{x}\), for some constant vector \(c_X\) (and an analogous statement is true for \(Y\)). Since the hyperplane \(H\) that \(X\) and \(Y\) share consists of those \(\mathbf{x}\) where the extension of \(F\lvert_X\) and the extension of \(F\lvert_Y\) agree, we have</p> \[H = \{\mathbf{x} | c_X - c_Y + (\mathbf{J}F\lvert_X - \mathbf{J}F\lvert_Y)\mathbf{x} = \mathbf{0} \},\] <p>proving the statement. \(\blacksquare\)</p> </blockquote> <p>We now discuss Theorem 8.7, which provides sufficient conditions to have equality: \(\dim_{fun}(A, \theta) = n_0n_1 + \dim_{fun}(\theta)\).</p> <h5 class="env-title" id="theorem-87">Theorem 8.7.</h5> <p>Fix a parameter \(\theta\in\Omega\) which is “nice” and suppose that \(Z_1 \subset \mathbb R_{&gt;0}^{n_1}\) is a finite set whose ternary labels with respect to every neuron of \(\mathcal N(\theta)\) are nonzero, and so that \(\dim_{fun}(\theta) = \operatorname{rank} \mathbf{J}E_{Z_1}\lvert_\theta\). Suppose that \(A:\mathbb R^{n_0}\to\mathbb R^{n_1}\) is a surjective affine-linear map that satisfies all the assumptions of Lemma 8.5 (including that every \(y_k\) is in \(\mathbb R_{&gt;0}^{n_0}\)). Then there is a finite set \(Z \subset \mathbb R_{&gt;0}^{n_0}\) such that the ternary labeling, for all \(z\in Z\) and every neuron of \(\mathcal N(A,\theta)\), is nonzero, and</p> \[\dim_{fun}(A, \theta) = \operatorname{rank}\mathbf{J}E_Z\lvert_{(A,\theta)} = n_0n_1 + \dim_{fun}(\theta).\] <p>The proof of Theorem 8.7 is a bit more involved than the proofs of the lemmas above. However, let us remark on the assumptions being made to get the conclusion of this theorem.</p> <p>First, the existence of a set \(Z_1\), as in the theorem statement, requires that \(\dim_{fun+}(\theta) = \dim_{fun}(\theta)\). It would be valuable to understand what causes this to occur.</p> <p>Second, the surjectivity assumption on \(A\) requires that \(n_0 \ge n_1\). It also includes some restrictive assumptions (not full measure) in order to guarantee assumption (<em>iv</em>) of Lemma 8.5, as we discussed above, as well as guaranteeing that \(y_k\) can be chosen from \(\mathbb R_{&gt;0}^{n_0}\) for each \(k\). (For example, this is impossible if one of the hyperplanes/neurons of \(\Phi\circ A\) does not cut through the positive orthant.)</p> <h2 id="the-question-of-dim_funtheta-vs-dim_funtheta">The Question of \(\dim_{fun+}(\theta)\) vs. \(\dim_{fun}(\theta)\)</h2> <hr/> <p>As a start for understanding when \(\dim_{fun+}(\theta)\) and \(\dim_{fun}(\theta)\) are the same, let us make a simple observation. Let \(x_0, x_1, \ldots, x_{n_0}\) be affinely independent points in \(\mathbb R^{n_0}\) and let \(y_0,y_1,\ldots, y_{n_0} \in\mathbb R\). There is a unique affine linear function \(F:\mathbb R^{n_0} \to \mathbb R\) with the property that \(F(x_i) = y_i\) for every \(0\le i\le n_0\). This falls out of linear algebra.</p> <p>Indeed, since the points are affinely independent, \(\{x_i - x_0\ \lvert\ 1\le i\le n_0\}\) is a basis of \(\mathbb R^{n_0}\). The function \(F\) is determined by \(n_0+1\) scalars \(a_0,a_1,\ldots, a_{n_0}\), so that, writing \(\mathbf{a}\) for \((a_1,a_2,\ldots,a_{n_0})\), we have \(F(x) = a_0 + \mathbf{a}\cdot x\) for all \(x\in\mathbb R^{n_0}\). Note that \(y_i - y_0 = \mathbf{a}\cdot (x_i - x_0)\). Therefore, since for any \(x\in \mathbb R^{n_0}\), we have scalars \(c_1,\ldots, c_{n_0}\) so that \(x - x_0 = \sum_{i=1}^{n_0} c_i(x_i - x_0),\) we see that</p> \[F(x) - F(x_0) = \mathbf{a}\cdot (x - x_0) = \sum_{i=1}^{n_0} c_i\mathbf{a}\cdot(x_i - x_0) = \sum_{i=1}^{n_0} c_i(y_i - y_0).\] <p>Therefore, \(F(x) = y_0 + \sum_{i=1}^{n_0} c_i(y_i - y_0)\).</p> <p>Let’s consider a scenario.  Given an architecture of a ReLU network and a parameter \(\theta\in\Omega\) that is “nice,” say that we have a point \(p\) in the interior of a top-dimensional cell of the canonical polyhedral complex \(\mathcal C = \mathcal C(\theta)\). Write \(X\) for this cell containing \(p\). Further, suppose that \(Z\subset \mathbb R^{n_0}\) is a finite set so that:</p> <ol> <li>\(Z\) is the union of finite sets of points in the interior of top-dimensional cells of \(\mathcal C\);</li> <li>there is a vertex \(v\) of \(X\) such that for every top-dimensional cell \(A \ne X\) which has \(v\) as one of its vertices, \(int(A) \cap Z \ne\emptyset\); and</li> <li>if \(A\) is a top-dimensional cell and \(int(A)\cap Z\ne \emptyset\), then \(Z\) contains a decisive set (in \(A\)) for \(F_\theta\lvert_A\).<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup></li> </ol> <p>We <em>hope</em> that this will mean that \(\operatorname{rank}\mathbf{J}E_Z\lvert_\theta = \operatorname{rank}\mathbf{J}E_{Z^*}\lvert_\theta\), where \(Z^* = Z \cup \{p\}\).</p> <p>The intuition for this would be that, first, since \(Z\) has a decisive set on each cell \(A\) that it intersects, \(F_\theta\lvert_A\) is determined by values at points in \(Z\cap A\). This means that \(F_\theta\) is determined on the boundary of \(X\) on an affinely independent set of points.  Thus, it would seem that our set \(Z\) completely determines \(F_\theta\lvert_{X}\). That is, we should be able to “witness” the partial derivatives in \(E_{\{p\}}\lvert_\theta\) through rows of \(E_{Z}\lvert_\theta\).</p> <p>Let \(A\) be a top-dimensional cell of \(\mathcal C(\theta)\). The first thing to observe is that, related to the fact that an affine-linear function is determined by its values on \(n_0+1\) affinely independent points, if \(x_0,x_1,\ldots,x_{n_0}\) in \(int(A)\) is a set of affinely independent points then, for any \(x\in int(A)\), we can get \(\mathbf{J}E_x\lvert_{\theta}\) as a linear combination of \(\mathbf{J}E_{x_i}\lvert_{\theta}, 0\le i\le n_0\). Indeed, there is a unique set of scalars \(c_1,\ldots, c_{n_0}\) so that \(x - x_0 = \sum_{i=1}^{n_0} c_i (x_i - x_0)\).  Note that each element of \(\mathbf{J}E_{x}\lvert_\theta\) is either linear or constant in \(x\) (when restricting to \(int(A)\)). This means that</p> <p>\begin{equation} \label{eq:vertex-loop} \mathbf{J}E_{x}\lvert_\theta - \mathbf{J}E_{x_0}\lvert_\theta = \sum_{i=1}^{n_0}c_i (\mathbf{J}E_{x_i}\lvert_\theta - \mathbf{J}E_{x_0}\lvert_\theta), \end{equation}</p> <p>which expresses \(\mathbf{J}E_{x}\lvert_{\theta}\) in the desired way as a linear combination.</p> <p>Now, the vector \(\mathbf{J}E_{x}\lvert_{\theta}\) is not defined if \(x\) is contained in a facet of \(A\). <sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">7</a></sup> However, suppose that we use \eqref{eq:vertex-loop} to determine such a vector – this would be the limit of \(\mathbf{J}E_{p_i}\lvert_{\theta}\) for a sequence \(\{p_i\} \subset int(A)\), where \(p_i \to x\). Since it depends on “converging from \(int(A)\)”, say that we call this vector \(\mathbf{J}^AE_x\lvert_{\theta}\).  In fact, from here on we will drop the notation that indicates evaluation at \(\theta\), considering that as understood; hence, call this vector simply \(\mathbf{J}^AE_x\).</p> <p>Taking the above construction a step farther, since \(\{x_i - x_0\ \lvert\ 1\le i\le n_0\}\) is a basis of \(\mathbb R^{n_0}\) we could determine \(\mathbf{J}^AE_x\) from \eqref{eq:vertex-loop}, for any \(x \in \mathbb R^{n_0}\).  Another perspective on this would be to consider how the parameters in \(\theta\) express an affine linear function \(\mathbb R^{n_0} \to \mathbb R^{n_d}\) which has a restriction to \(A\) that agrees with the restriction of \(F_\theta\). Then \(\mathbf{J}^AE_x\) is the Jacobian of the evaluation map, at \(x\), corresponding to that affine linear function.</p> <p>Now, if \(A \ne X\) is one of the cells having non-empty intersection with \(Z\) in conditions 1, 2, and 3 above, then for any \(x\in\mathbb R^{n_0}\), \(\mathbf{J}^AE_x\) is a linear combination of rows of \(\mathbf{J}E_Z\). While we are still figuring out how it works in general, let’s consider a special case.</p> <p><strong>How it works when \(n_0 = 2\) and all bent hyperplanes at the vertex come from one layer.</strong> Under our “nice” assumptions when \(n_0=2\), for any vertex \(v\) of \(X\) there are 4 top-dimensional cells of the polyhedral complex that have \(v\) as a vertex, including \(X\). There are two (bent) hyperplanes intersecting at \(v\) – call them \(H_1\) and \(H_2\). The ternary labeling for each of \(H_1\) and \(H_2\) is positive in exactly two of the 4 aforementioned cells. We may associate in one-to-one manner these cells to elements of \(\{+, -\}^4.\) Let \(C\) and \(\bar C\) be the two cells which are associated to \((+,-)\) and \((-,+)\), which can be taken to mean that the ternary labeling for \(H_1\) is positive in \(C\), but negative in \(\bar C\). The opposite occurs for \(H_2\) (i.e., negative in \(C\) and positive in \(\bar C\)). Let \(D\) and \(\bar D\) be the cells which we associate to \((+,+)\) and \((-, -)\) respectively.</p> <h5 class="env-title" id="claim">Claim</h5> <p>\(\mathbf{J}^{C}E_v + \mathbf{J}^{\bar C}E_v - \mathbf{J}^{D}E_v - \mathbf{J}^{\bar D}E_v = \mathbf{0}\).</p> <blockquote class="block-tip"> <p>To prove this, note that for each of \(C, \bar C, D\), and \(\bar D\), the function  in each column of \(\mathbf{J}E_v\) restricts in the interiors to a polynomial, expressible so that every monomial in this polynomial is degree at most 1 in the parameter coordinates, and is degree 1 or less in the coordinates of \(v\), as well.  Suppose that such a monomial is degree 0 in every parameter coordinate corresponding to these two hyperplanes – that is, it is degree 0 in every one of the \(2(n_{\ell-1}+1)\) coordinates for the rows of \((W^{\ell} | b^{\ell})\) that correspond to these neurons, and it is also degree 0 in all \(2n_{\ell+1}\) coordinates appearing in the columns of \(W^{\ell+1}\) that correspond to these two neurons. Then this monomial appears in a column of \(\mathbf{J}^{C}E_v\) if and only if it appears in the same column of \(\mathbf{J}^{\bar C}E_v, \mathbf{J}^{D}E_v\), and \(\mathbf{J}^{\bar D}E_v\). Note, since these hyperplanes come from a “hidden layer”, any monomial of \(E_x\), \(x\) in the interior of one of these cells, that is positive degree in one of the coordinates corresponding to the two hyperplanes must be degree 1 in <em>two</em> such coordinates – one from \((W^{\ell} | b^{\ell})\) and one from \(W^{\ell+1}\). Thus, in the \(2(n_{\ell-1}+1) + 2n_{\ell+1}\) columns for partials with respect to such coordinates every non-zero monomial is degree 1 in one of those coordinates.  Hence, all monomials in every column that have degree 0 in those coordinates will vanish in the summation \(\mathbf{J}^{C}E_v + \mathbf{J}^{\bar C}E_v - \mathbf{J}^{D}E_v - \mathbf{J}^{\bar D}E_v\). Now, suppose that some monomial in \(\mathbf{J}^{A}E_v\), for \(A = C, \bar C, D,\) or \(\bar D\), has degree 1 in one of these coordinates. It cannot be that \(A = \bar D\) since both of the neurons in question are unactivated in \(\bar D\).  Moreover, in each column, such a monomial occuring in \(\mathbf{J}^DE_v\) must be precisely the sum of monomials that separately occur in \(\mathbf{J}^CE_v\) and \(\mathbf{J}^{\bar C}E_v\). Since no monomial in \(\mathbf{J}^{A}E_v\) can be larger than degree 1 in these coordinates, this shows the equation holds.</p> </blockquote> <p>While the discussion of the claim discusses the Jacobian of the evaluation map at \(v\), it would appear that it holds at <em>any</em> point. Using that \(X\) is one of \(C, \bar C, D\), or \(\bar D\), the claim tells us that \(\mathbf{J}^XE_{v}\) (resp. \(\mathbf{J}^XE_{p}\)) is a linear combination of vectors \(\mathbf{J}^AE_{v}\) (resp. \(\mathbf{J}^AE_{p}\)), where \(A\) takes on the other three cells (in each of which we have a subset of \(Z\) that is affinely independent. Since, for each \(A\in \{C, \bar C, D, \bar D\} \setminus \{X\}\), we may write \(\mathbf{J}^AE_v\) and \(\mathbf{J}^AE_p\) as a linear combination of rows of \(\mathbf{J}E_Z\), this tells us that \(\mathbf{J}^XE_p = \mathbf{J}E_p\) can be expressed as a linear combination of rows of \(\mathbf{J}E_Z\).</p> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>I could have written \(z^{n_d}_j\) instead for the \(j\)-th coordinate function. However, this choice will make for simpler notation below. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>In other words, think of the function \(f_{j,z_i}:\Omega\to\mathbb R\) which is given by \(f_{j,z_i}(\theta) = F_{\theta,j}(z_i)\) and take partial derivatives of \(f_{j,z_i}\). <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>In terminology of [GLMW22], it is an <em>ordinary point</em>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4"> <p>Shifting the index here, since the first hidden layer of the network of \(\theta\) has \(n_2\) neurons, and so on. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5"> <p>The partial derivatives in \(\mathbf{J}F_{\theta}\lvert_{\Phi\circ A(y_k)}\) does not involve partials with respect to parameters, but with respect to spatial coordinates in \(\mathbb R^{n_1}\); i.e., with respect to coordinates \((x_1,x_2,\ldots,x_{n_1})\) in \(\mathbb R^{n_1}\). <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:6"> <p>We also assume that the polyhedral complex is generic and transversal so, in particular, any subset of the supporting hyperplanes that determine the facets \(X\cap X_i\) (which has cardinality \(\le n_0\)), has a non-empty intersection that is some face of \(X\). <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:7"> <p>The facet is associated to the bent hyperplane for one neuron, \(\{x\ \lvert\ \tau^{\ell}_j(x) = 0\}\); for any weight or bias “leading to” that neuron, in row \(j\) of \(W^{\ell}\) or \(b^{\ell}\), the partial of \(E_x\) with respect to that weight or bias will be undefined. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="math"/><category term="neural_networks"/><category term="research"/><category term="thoughts"/><category term="ReLU_networks"/><summary type="html"><![CDATA[understanding functional dimension by precomposing]]></summary></entry><entry><title type="html">Hello world</title><link href="https://cornwell.github.io/blog/2024/hello-world/" rel="alternate" type="text/html" title="Hello world"/><published>2024-12-20T09:06:00+00:00</published><updated>2024-12-20T09:06:00+00:00</updated><id>https://cornwell.github.io/blog/2024/hello-world</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/hello-world/"><![CDATA[<p>It’s almost a new year and the time has come for a revamp of my website! For a while, I have wanted the versatility of a <a href="https://jekyllrb.com/">Jekyll</a>-based GitHub pages website, but I hadn’t taken the time to get it started. Well, I found this <a href="https://github.com/alshedivat/al-folio">al-folio theme</a> in a list of popular Jekyll themes. I like the look of it. Hopefully it will make it easy to integrate elements of research and teaching.</p> <p><mark>Posts from a date previous to this one are sample posts that were created by the al-folio theme contributors.</mark></p> <hr/> <h3 id="previous-webpage">Previous webpage</h3> <p>For a few years, I maintained a <a href="https://wp.towson.edu/ccornwell/">Wordpress webpage</a> and blog where I posted about both research and teaching. I haven’t posted to that in a while, though (1 post in the last 3 years?). I’m hoping to stay motivated in this new venue, especially since it should be easier to put together <em>thoughts and discussion</em> with <em>code and experiment</em>. Let’s say that keeping up the site is a New Year’s resolution! :oncoming_bus: :eyes:</p> <p><strong>Added bonus:</strong> I plan for the new machine learning courses I’ll be teaching (Math 371 and 471) to have some GitHub presence; this site should help with that.<br/></p> <ul> <li style="list-style-type:square"> There are some teaching-focused blog posts on the Wordpress site that I would rather not rewrite. We'll see how it goes, linking to them from here. </li> </ul> <hr/> <h3 id="research-and-teaching">Research and Teaching</h3> <p><strong>Research.</strong> I imagine that most of my updates to the site on the research end will be blogging (just thoughts, as well as research progress) and posting support scripts, or custom packages, that I made to help me with research. I’ll also post about publications and preprints, etc.</p> <p><strong>Teaching.</strong> Let’s start with small goals here, so it doesn’t soak up too much of my time. I plan to make a repo on GitHub for Math 371 – at some point that will appear under the repositories tab – as well as some info for students under the teaching tab. Other classes might follow, particularly Math 471.</p>]]></content><author><name></name></author><category term="personal"/><category term="personal"/><category term="remarks"/><summary type="html"><![CDATA[a new academic site and blog]]></summary></entry><entry><title type="html">Testing how posts work</title><link href="https://cornwell.github.io/blog/2024/test-blog-post/" rel="alternate" type="text/html" title="Testing how posts work"/><published>2024-12-18T19:36:00+00:00</published><updated>2024-12-18T19:36:00+00:00</updated><id>https://cornwell.github.io/blog/2024/test-blog-post</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/test-blog-post/"><![CDATA[<p>First, this theme uses MathJax 3. So we can write in math mode with <code class="language-plaintext highlighter-rouge">$$ $$</code> as expected, for example: \(e^{i\theta} = \cos(\theta) + i\sin(\theta)\), but not using single dollar-signs <code class="language-plaintext highlighter-rouge">$</code>.</p> <p>I am wondering if the MathJax used in Wordpress will cross-over. It would be nice if I could transfer some posts and not rewrite them. This uses <code class="language-plaintext highlighter-rouge">\( \)</code>. Tested <code class="language-plaintext highlighter-rouge">\(e^{i\theta} = \cos(\theta) + i\sin(\theta)\)</code>, but it does not work.</p> <p>Apparently you can also use tikz in this theme, as long as you set the <code class="language-plaintext highlighter-rouge">tikzjax</code> property to true:</p> <div align="center"> <script type="text/tikz">
    \begin{tikzpicture}[>=stealth]
        \draw[red,fill=black!60!red] (0,0) circle [radius=1.5];
        \draw[green,fill=black!60!green] (0,0) circle [x radius=1.5cm, y radius=10mm];
        \draw[blue,fill=black!60!blue] (0,0) circle [x radius=1cm, y radius=5mm, rotate=30];
        \draw[->, thick] (2.5,1.8) --node[at start, right]{$f(\alpha)$} (1.2,1);
        % a commented line? next: a Bezier curve
        \draw[->, thick] (3,0) ..controls (2,0) and (2.2,-1.75) ..(1.2, -1);
        \draw (3,0) node[right]{$\dag$};
    \end{tikzpicture}
    </script> </div> <hr/> <p>You can also embed lines of code. Like this?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_primes_lessthan</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">primes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">is_composite</span> <span class="o">=</span> <span class="nf">test_relcomposite</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">primes</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_composite</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">primes</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">primes</span>
<span class="k">def</span> <span class="nf">test_relcomposite</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">primelist</span><span class="p">):</span>
    <span class="c1"># catch the exceptional cases
</span>    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">num</span> <span class="o">&gt;</span> <span class="mi">1</span>
    <span class="k">except</span> <span class="nb">AssertionError</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">True</span>
    <span class="c1"># now test for divisors in primelist (assume doesn't contain 1)
</span>    <span class="n">found_divisor</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">primelist</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">found_divsor</span> <span class="o">=</span> <span class="ow">not</span> <span class="p">(</span><span class="n">num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">primelist</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">num</span><span class="o">%</span><span class="n">p</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">found_divisor</span> <span class="o">=</span> <span class="bp">True</span>
                <span class="k">break</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">continue</span>
    <span class="k">return</span> <span class="n">found_divisor</span>
</code></pre></div></div> <p>That works. I want to tweak the bg color though, at least in dark mode.</p> <p><mark>Reader beware! The default tab behavior for this theme is 4 spaces; some want only 2.</mark></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="testing"/><category term="formatting"/><summary type="html"><![CDATA[testing out functionality of blog posts]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://cornwell.github.io/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://cornwell.github.io/blog/2024/photo-gallery</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://cornwell.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://cornwell.github.io/blog/2024/tabs</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="3bedcbe7-042c-4dbf-92d6-1b20636d65d1" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="3bedcbe7-042c-4dbf-92d6-1b20636d65d1" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="2cb8b5f5-e922-4a09-8b20-c02791a33fca" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="2cb8b5f5-e922-4a09-8b20-c02791a33fca" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="2f7325ed-ecce-4836-b489-0f6ca3cb9358" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="2f7325ed-ecce-4836-b489-0f6ca3cb9358" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://cornwell.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://cornwell.github.io/blog/2024/typograms</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://cornwell.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://cornwell.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry><entry><title type="html">a post with pseudo code</title><link href="https://cornwell.github.io/blog/2024/pseudocode/" rel="alternate" type="text/html" title="a post with pseudo code"/><published>2024-04-15T00:01:00+00:00</published><updated>2024-04-15T00:01:00+00:00</updated><id>https://cornwell.github.io/blog/2024/pseudocode</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/pseudocode/"><![CDATA[<p>This is an example post with some pseudo code rendered by <a href="https://github.com/SaswatPadhi/pseudocode.js">pseudocode</a>. The example presented here is the same as the one in the <a href="https://saswat.padhi.me/pseudocode.js/">pseudocode.js</a> documentation, with only one simple but important change: everytime you would use <code class="language-plaintext highlighter-rouge">$</code>, you should use <code class="language-plaintext highlighter-rouge">$$</code> instead. Also, note that the <code class="language-plaintext highlighter-rouge">pseudocode</code> key in the front matter is set to <code class="language-plaintext highlighter-rouge">true</code> to enable the rendering of pseudo code. As an example, using this code:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">pseudocode
</span><span class="sb">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Generates:</p> <pre><code class="language-pseudocode">% This quicksort algorithm is extracted from Chapter 7, Introduction to Algorithms (3rd edition)
\begin{algorithm}
\caption{Quicksort}
\begin{algorithmic}
\PROCEDURE{Quicksort}{$$A, p, r$$}
    \IF{$$p &lt; r$$}
        \STATE $$q = $$ \CALL{Partition}{$$A, p, r$$}
        \STATE \CALL{Quicksort}{$$A, p, q - 1$$}
        \STATE \CALL{Quicksort}{$$A, q + 1, r$$}
    \ENDIF
\ENDPROCEDURE
\PROCEDURE{Partition}{$$A, p, r$$}
    \STATE $$x = A[r]$$
    \STATE $$i = p - 1$$
    \FOR{$$j = p$$ \TO $$r - 1$$}
        \IF{$$A[j] &lt; x$$}
            \STATE $$i = i + 1$$
            \STATE exchange
            $$A[i]$$ with $$A[j]$$
        \ENDIF
        \STATE exchange $$A[i]$$ with $$A[r]$$
    \ENDFOR
\ENDPROCEDURE
\end{algorithmic}
\end{algorithm}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included pseudo code could look like]]></summary></entry><entry><title type="html">a post with code diff</title><link href="https://cornwell.github.io/blog/2024/code-diff/" rel="alternate" type="text/html" title="a post with code diff"/><published>2024-01-27T19:22:00+00:00</published><updated>2024-01-27T19:22:00+00:00</updated><id>https://cornwell.github.io/blog/2024/code-diff</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/code-diff/"><![CDATA[<p>You can display diff code by using the regular markdown syntax:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff
</span><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <div class="language-diff highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="gh">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
</span><span class="gd">--- a/sample.js
</span><span class="gi">+++ b/sample.js
</span><span class="p">@@ -1 +1 @@</span>
<span class="gd">-console.log("Hello World!")
</span><span class="gi">+console.log("Hello from Diff2Html!")
</span></code></pre></div></div> <p>But this is difficult to read, specially if you have a large diff. You can use <a href="https://diff2html.xyz/">diff2html</a> to display a more readable version of the diff. For this, just use <code class="language-plaintext highlighter-rouge">diff2html</code> instead of <code class="language-plaintext highlighter-rouge">diff</code> for the code block language:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">diff2html
</span><span class="sb">diff --git a/sample.js b/sample.js
index 0000001..0ddf2ba
--- a/sample.js
+++ b/sample.js
@@ -1 +1 @@
-console.log("Hello World!")
+console.log("Hello from Diff2Html!")</span>
<span class="p">```</span>
</code></pre></div></div> <p>If we use a longer example, for example <a href="https://github.com/rtfpessoa/diff2html/commit/c2c253d3e3f8b8b267f551e659f72b44ca2ac927">this commit from diff2html</a>, it will generate the following output:</p> <pre><code class="language-diff2html">From 2aaae31cc2a37bfff83430c2c914b140bee59b6a Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sun, 9 Oct 2016 16:41:54 +0100
Subject: [PATCH 1/2] Initial template override support

---
 scripts/hulk.js                    |  4 ++--
 src/diff2html.js                   |  3 +--
 src/file-list-printer.js           | 11 ++++++++---
 src/hoganjs-utils.js               | 29 +++++++++++++++++------------
 src/html-printer.js                |  6 ++++++
 src/line-by-line-printer.js        |  6 +++++-
 src/side-by-side-printer.js        |  6 +++++-
 test/file-list-printer-tests.js    |  2 +-
 test/hogan-cache-tests.js          | 18 +++++++++++++++---
 test/line-by-line-tests.js         |  3 +--
 test/side-by-side-printer-tests.js |  3 +--
 11 files changed, 62 insertions(+), 29 deletions(-)

diff --git a/scripts/hulk.js b/scripts/hulk.js
index 5a793c18..a4b1a4d5 100755
--- a/scripts/hulk.js
+++ b/scripts/hulk.js
@@ -173,11 +173,11 @@ function namespace(name) {
 // write a template foreach file that matches template extension
 templates = extractFiles(options.argv.remain)
   .map(function(file) {
-    var openedFile = fs.readFileSync(file, 'utf-8');
+    var openedFile = fs.readFileSync(file, 'utf-8').trim();
     var name;
     if (!openedFile) return;
     name = namespace(path.basename(file).replace(/\..*$/, ''));
-    openedFile = removeByteOrderMark(openedFile.trim());
+    openedFile = removeByteOrderMark(openedFile);
     openedFile = wrap(file, name, openedFile);
     if (!options.outputdir) return openedFile;
     fs.writeFileSync(path.join(options.outputdir, name + '.js')
diff --git a/src/diff2html.js b/src/diff2html.js
index 21b0119e..64e138f5 100644
--- a/src/diff2html.js
+++ b/src/diff2html.js
@@ -7,7 +7,6 @@

 (function() {
   var diffParser = require('./diff-parser.js').DiffParser;
-  var fileLister = require('./file-list-printer.js').FileListPrinter;
   var htmlPrinter = require('./html-printer.js').HtmlPrinter;

   function Diff2Html() {
@@ -43,7 +42,7 @@

     var fileList = '';
     if (configOrEmpty.showFiles === true) {
-      fileList = fileLister.generateFileList(diffJson, configOrEmpty);
+      fileList = htmlPrinter.generateFileListSummary(diffJson, configOrEmpty);
     }

     var diffOutput = '';
diff --git a/src/file-list-printer.js b/src/file-list-printer.js
index e408d9b2..1e0a2c61 100644
--- a/src/file-list-printer.js
+++ b/src/file-list-printer.js
@@ -8,11 +8,16 @@
 (function() {
   var printerUtils = require('./printer-utils.js').PrinterUtils;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var baseTemplatesPath = 'file-summary';
   var iconsBaseTemplatesPath = 'icon';

-  function FileListPrinter() {
+  function FileListPrinter(config) {
+    this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   FileListPrinter.prototype.generateFileList = function(diffFiles) {
@@ -38,5 +43,5 @@
     });
   };

-  module.exports.FileListPrinter = new FileListPrinter();
+  module.exports.FileListPrinter = FileListPrinter;
 })();
diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 9949e5fa..0dda08d7 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -8,18 +8,19 @@
 (function() {
   var fs = require('fs');
   var path = require('path');
-
   var hogan = require('hogan.js');

   var hoganTemplates = require('./templates/diff2html-templates.js');

-  var templatesPath = path.resolve(__dirname, 'templates');
+  var extraTemplates;

-  function HoganJsUtils() {
+  function HoganJsUtils(configuration) {
+    this.config = configuration || {};
+    extraTemplates = this.config.templates || {};
   }

-  HoganJsUtils.prototype.render = function(namespace, view, params, configuration) {
-    var template = this.template(namespace, view, configuration);
+  HoganJsUtils.prototype.render = function(namespace, view, params) {
+    var template = this.template(namespace, view);
     if (template) {
       return template.render(params);
     }
@@ -27,17 +28,16 @@
     return null;
   };

-  HoganJsUtils.prototype.template = function(namespace, view, configuration) {
-    var config = configuration || {};
+  HoganJsUtils.prototype.template = function(namespace, view) {
     var templateKey = this._templateKey(namespace, view);

-    return this._getTemplate(templateKey, config);
+    return this._getTemplate(templateKey);
   };

-  HoganJsUtils.prototype._getTemplate = function(templateKey, config) {
+  HoganJsUtils.prototype._getTemplate = function(templateKey) {
     var template;

-    if (!config.noCache) {
+    if (!this.config.noCache) {
       template = this._readFromCache(templateKey);
     }

@@ -53,6 +53,7 @@

     try {
       if (fs.readFileSync) {
+        var templatesPath = path.resolve(__dirname, 'templates');
         var templatePath = path.join(templatesPath, templateKey);
         var templateContent = fs.readFileSync(templatePath + '.mustache', 'utf8');
         template = hogan.compile(templateContent);
@@ -66,12 +67,16 @@
   };

   HoganJsUtils.prototype._readFromCache = function(templateKey) {
-    return hoganTemplates[templateKey];
+    return extraTemplates[templateKey] || hoganTemplates[templateKey];
   };

   HoganJsUtils.prototype._templateKey = function(namespace, view) {
     return namespace + '-' + view;
   };

-  module.exports.HoganJsUtils = new HoganJsUtils();
+  HoganJsUtils.prototype.compile = function(templateStr) {
+    return hogan.compile(templateStr);
+  };
+
+  module.exports.HoganJsUtils = HoganJsUtils;
 })();
diff --git a/src/html-printer.js b/src/html-printer.js
index 585d5b66..13f83047 100644
--- a/src/html-printer.js
+++ b/src/html-printer.js
@@ -8,6 +8,7 @@
 (function() {
   var LineByLinePrinter = require('./line-by-line-printer.js').LineByLinePrinter;
   var SideBySidePrinter = require('./side-by-side-printer.js').SideBySidePrinter;
+  var FileListPrinter = require('./file-list-printer.js').FileListPrinter;

   function HtmlPrinter() {
   }
@@ -22,5 +23,10 @@
     return sideBySidePrinter.generateSideBySideJsonHtml(diffFiles);
   };

+  HtmlPrinter.prototype.generateFileListSummary = function(diffJson, config) {
+    var fileListPrinter = new FileListPrinter(config);
+    return fileListPrinter.generateFileList(diffJson);
+  };
+
   module.exports.HtmlPrinter = new HtmlPrinter();
 })();
diff --git a/src/line-by-line-printer.js b/src/line-by-line-printer.js
index b07eb53c..d230bedd 100644
--- a/src/line-by-line-printer.js
+++ b/src/line-by-line-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'line-by-line';
   var iconsBaseTemplatesPath = 'icon';
@@ -19,6 +20,9 @@

   function LineByLinePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   LineByLinePrinter.prototype.makeFileDiffHtml = function(file, diffs) {
diff --git a/src/side-by-side-printer.js b/src/side-by-side-printer.js
index bbf1dc8d..5e3033b3 100644
--- a/src/side-by-side-printer.js
+++ b/src/side-by-side-printer.js
@@ -11,7 +11,8 @@
   var utils = require('./utils.js').Utils;
   var Rematch = require('./rematch.js').Rematch;

-  var hoganUtils = require('./hoganjs-utils.js').HoganJsUtils;
+  var hoganUtils;
+
   var genericTemplatesPath = 'generic';
   var baseTemplatesPath = 'side-by-side';
   var iconsBaseTemplatesPath = 'icon';
@@ -26,6 +27,9 @@

   function SideBySidePrinter(config) {
     this.config = config;
+
+    var HoganJsUtils = require('./hoganjs-utils.js').HoganJsUtils;
+    hoganUtils = new HoganJsUtils(config);
   }

   SideBySidePrinter.prototype.makeDiffHtml = function(file, diffs) {
diff --git a/test/file-list-printer-tests.js b/test/file-list-printer-tests.js
index a502a46f..60ea3208 100644
--- a/test/file-list-printer-tests.js
+++ b/test/file-list-printer-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var fileListPrinter = require('../src/file-list-printer.js').FileListPrinter;
+var fileListPrinter = new (require('../src/file-list-printer.js').FileListPrinter)();

 describe('FileListPrinter', function() {
   describe('generateFileList', function() {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 190bf6f8..3bb754ac 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -1,6 +1,6 @@
 var assert = require('assert');

-var HoganJsUtils = require('../src/hoganjs-utils.js').HoganJsUtils;
+var HoganJsUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)();
 var diffParser = require('../src/diff-parser.js').DiffParser;

 describe('HoganJsUtils', function() {
@@ -21,16 +21,28 @@ describe('HoganJsUtils', function() {
       });
       assert.equal(emptyDiffHtml, result);
     });
+
     it('should render view without cache', function() {
       var result = HoganJsUtils.render('generic', 'empty-diff', {
         contentClass: 'd2h-code-line',
         diffParser: diffParser
       }, {noCache: true});
-      assert.equal(emptyDiffHtml + '\n', result);
+      assert.equal(emptyDiffHtml, result);
     });
+
     it('should return null if template is missing', function() {
-      var result = HoganJsUtils.render('generic', 'missing-template', {}, {noCache: true});
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)({noCache: true});
+      var result = hoganUtils.render('generic', 'missing-template', {});
       assert.equal(null, result);
     });
+
+    it('should allow templates to be overridden', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+
+      var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
diff --git a/test/line-by-line-tests.js b/test/line-by-line-tests.js
index 1cd92073..8869b3df 100644
--- a/test/line-by-line-tests.js
+++ b/test/line-by-line-tests.js
@@ -14,7 +14,7 @@ describe('LineByLinePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expected, fileHtml);
     });
@@ -422,7 +422,6 @@ describe('LineByLinePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                &lt;/tbody&gt;\n' +
         '            &lt;/table&gt;\n' +
         '        &lt;/div&gt;\n' +
diff --git a/test/side-by-side-printer-tests.js b/test/side-by-side-printer-tests.js
index 76625f8e..771daaa5 100644
--- a/test/side-by-side-printer-tests.js
+++ b/test/side-by-side-printer-tests.js
@@ -14,7 +14,7 @@ describe('SideBySidePrinter', function() {
         '            File without changes\n' +
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
-        '&lt;/tr&gt;\n';
+        '&lt;/tr&gt;';

       assert.equal(expectedRight, fileHtml.right);
       assert.equal(expectedLeft, fileHtml.left);
@@ -324,7 +324,6 @@ describe('SideBySidePrinter', function() {
         '        &lt;/div&gt;\n' +
         '    &lt;/td&gt;\n' +
         '&lt;/tr&gt;\n' +
-        '\n' +
         '                    &lt;/tbody&gt;\n' +
         '                &lt;/table&gt;\n' +
         '            &lt;/div&gt;\n' +

From f3cadb96677d0eb82fc2752dc3ffbf35ca9b5bdb Mon Sep 17 00:00:00 2001
From: Rodrigo Fernandes &lt;rtfrodrigo@gmail.com&gt;
Date: Sat, 15 Oct 2016 13:21:22 +0100
Subject: [PATCH 2/2] Allow uncompiled templates

---
 README.md                 |  3 +++
 src/hoganjs-utils.js      |  7 +++++++
 test/hogan-cache-tests.js | 24 +++++++++++++++++++++++-
 3 files changed, 33 insertions(+), 1 deletion(-)

diff --git a/README.md b/README.md
index 132c8a28..46909f25 100644
--- a/README.md
+++ b/README.md
@@ -98,6 +98,9 @@ The HTML output accepts a Javascript object with configuration. Possible options
   - `synchronisedScroll`: scroll both panes in side-by-side mode: `true` or `false`, default is `false`
   - `matchWordsThreshold`: similarity threshold for word matching, default is 0.25
   - `matchingMaxComparisons`: perform at most this much comparisons for line matching a block of changes, default is `2500`
+  - `templates`: object with previously compiled templates to replace parts of the html
+  - `rawTemplates`: object with raw not compiled templates to replace parts of the html
+  &gt; For more information regarding the possible templates look into [src/templates](https://github.com/rtfpessoa/diff2html/tree/master/src/templates)

 ## Diff2HtmlUI Helper

diff --git a/src/hoganjs-utils.js b/src/hoganjs-utils.js
index 0dda08d7..b2e9c275 100644
--- a/src/hoganjs-utils.js
+++ b/src/hoganjs-utils.js
@@ -17,6 +17,13 @@
   function HoganJsUtils(configuration) {
     this.config = configuration || {};
     extraTemplates = this.config.templates || {};
+
+    var rawTemplates = this.config.rawTemplates || {};
+    for (var templateName in rawTemplates) {
+      if (rawTemplates.hasOwnProperty(templateName)) {
+        if (!extraTemplates[templateName]) extraTemplates[templateName] = this.compile(rawTemplates[templateName]);
+      }
+    }
   }

   HoganJsUtils.prototype.render = function(namespace, view, params) {
diff --git a/test/hogan-cache-tests.js b/test/hogan-cache-tests.js
index 3bb754ac..a34839c0 100644
--- a/test/hogan-cache-tests.js
+++ b/test/hogan-cache-tests.js
@@ -36,7 +36,7 @@ describe('HoganJsUtils', function() {
       assert.equal(null, result);
     });

-    it('should allow templates to be overridden', function() {
+    it('should allow templates to be overridden with compiled templates', function() {
       var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');

       var config = {templates: {'generic-empty-diff': emptyDiffTemplate}};
@@ -44,5 +44,27 @@ describe('HoganJsUtils', function() {
       var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
       assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
     });
+
+    it('should allow templates to be overridden with uncompiled templates', function() {
+      var emptyDiffTemplate = '&lt;p&gt;&lt;/p&gt;';
+
+      var config = {rawTemplates: {'generic-empty-diff': emptyDiffTemplate}};
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
+
+    it('should allow templates to be overridden giving priority to compiled templates', function() {
+      var emptyDiffTemplate = HoganJsUtils.compile('&lt;p&gt;&lt;/p&gt;');
+      var emptyDiffTemplateUncompiled = '&lt;p&gt;Not used!&lt;/p&gt;';
+
+      var config = {
+        templates: {'generic-empty-diff': emptyDiffTemplate},
+        rawTemplates: {'generic-empty-diff': emptyDiffTemplateUncompiled}
+      };
+      var hoganUtils = new (require('../src/hoganjs-utils.js').HoganJsUtils)(config);
+      var result = hoganUtils.render('generic', 'empty-diff', {myName: 'Rodrigo Fernandes'});
+      assert.equal('&lt;p&gt;Rodrigo Fernandes&lt;/p&gt;', result);
+    });
   });
 });
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is how you can display code diffs]]></summary></entry></feed>