<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://cornwell.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://cornwell.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-02T12:20:23+00:00</updated><id>https://cornwell.github.io/feed.xml</id><title type="html">Christopher R. Cornwell</title><subtitle>A personal blog for academic research at the intersection of mathematics and data science. </subtitle><entry><title type="html">Analyzing CDC Diabetes Health Indicators Data (and SVMs)</title><link href="https://cornwell.github.io/blog/2025/CDCdiabetes-survey-gaussian-svm/" rel="alternate" type="text/html" title="Analyzing CDC Diabetes Health Indicators Data (and SVMs)"/><published>2025-05-30T00:00:00+00:00</published><updated>2025-05-30T00:00:00+00:00</updated><id>https://cornwell.github.io/blog/2025/CDCdiabetes-survey-gaussian-svm</id><content type="html" xml:base="https://cornwell.github.io/blog/2025/CDCdiabetes-survey-gaussian-svm/"><![CDATA[<h2 id="the-data">the data</h2> <p>As part of their class project, my students looked at the CDC Diabetes Health Indicators survey data – a data set taken from the 2015 BRFSS Survey, made <a href="https://www.cdc.gov/brfss/annual_data/annual_2014.html">available on the CDC website</a>. The data set contains 253,680 instances (each the survey responses of an individual), which is a <em>clean</em> subset of the original 441,455 responses to the CDC survey. The original survey data has 330 features; however, the data that the students used has just 21 indicators (features) tracked. Also, there is a target variable with three classes: diabetic, pre-diabetic, and no diabetes. My students used a data set that was extracted from this one: first, the diabetic and pre-diabetic classes were put into one class, making it a binary classification problem; second, a subset (of 70,692 instances) was taken so that the two classes are balanced, having 50% of the instances have no diabetes for target value. The extracted data set (and a notebook with how preprocessing was done) is available through the <a href="https://archive.ics.uci.edu/dataset/891/cdc+diabetes+health+indicators">UCI Machine Learning repository</a> and on <a href="https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset">kaggle.com</a>, from the contributor Alex Teboul.</p> <p>As mentioned, many of the features are responses to a survey; however, some are calculated variables based on participant responses. The data includes answers from respondents in the United States (from all 50 states and Washington, DC), as well as from Guam and Puerto Rico.</p> <p><strong>Features.</strong> Two thirds of the features in the data are binary variables that correspond to a response of <em>yes</em> or <em>no</em> to a survey question. These binary variables are listed in the table below.</p> <table> <thead> <tr> <th style="text-align: center">feature</th> <th style="text-align: left">description</th> </tr> </thead> <tbody> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">HighBP</code></td> <td style="text-align: left"><em>ever been told [by health professional] that you have high blood pressure?</em></td> </tr> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">HighChol</code></td> <td style="text-align: left"><em>ever been told [by health professional] that your blood cholesterol is high?</em></td> </tr> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">CholCheck</code></td> <td style="text-align: left"><em>have you had a cholesterol check within past 5 years?</em></td> </tr> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">Smoker</code></td> <td style="text-align: left"><em>…smoked at least 100 cigarettes in your entire life?</em></td> </tr> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">Stroke</code></td> <td style="text-align: left"><em>have you ever had a stroke?</em></td> </tr> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">HeartDiseaseorAttack</code></td> <td style="text-align: left"><em>ever told you have coronary heart disease or had a heart attack?</em></td> </tr> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">PhysActivity</code></td> <td style="text-align: left"><em>…done physical activity or exercise (other than your job) in past 30 days?</em></td> </tr> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">Fruits</code></td> <td style="text-align: left"><em>do you consume fruit 1 or more times per day?</em></td> </tr> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">Veggies</code></td> <td style="text-align: left"><em>do you consume vegetables 1 or more times per day?</em></td> </tr> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">HvyAlcoholConsump</code></td> <td style="text-align: left"><em>more than 14 drinks per week (men)? more than 7 drinks per week (women)?</em></td> </tr> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">AnyHealthcare</code></td> <td style="text-align: left"><em>have any kind of health care coverage?</em></td> </tr> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">NoDocbcCost</code></td> <td style="text-align: left"><em>in past 12 months, have you needed to see doctor but could not because of cost?</em></td> </tr> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">DiffWalk</code></td> <td style="text-align: left"><em>do you have serious difficulty walking or climbing stairs?</em></td> </tr> <tr> <td style="text-align: center"><code class="language-plaintext highlighter-rouge">Sex</code></td> <td style="text-align: left">the sex of the respondent</td> </tr> </tbody> </table> <p></p> <p>Other features are integers, with various meanings.</p> <table> <tbody> <tr> <td><code class="language-plaintext highlighter-rouge">BMI</code></td> <td><code class="language-plaintext highlighter-rouge">GenHlth</code></td> <td><code class="language-plaintext highlighter-rouge">MentHlth</code></td> <td><code class="language-plaintext highlighter-rouge">PhysHlth</code></td> <td><code class="language-plaintext highlighter-rouge">Age</code></td> <td><code class="language-plaintext highlighter-rouge">Education</code></td> <td><code class="language-plaintext highlighter-rouge">Income</code></td> </tr> </tbody> </table> <ul> <li><code class="language-plaintext highlighter-rouge">GenHlth</code> is a Likert-scale response in range 1-5.</li> <li>Two of them – <code class="language-plaintext highlighter-rouge">MentHlth</code> and <code class="language-plaintext highlighter-rouge">PhysHlth</code> – are in range 1-30, corresponding to number of days within the past 30 days that mental, resp. physical, health were <em>not good</em>.</li> <li>Respondents reported their age, education level, and income which were binned into categories, giving the features <code class="language-plaintext highlighter-rouge">Age</code>, <code class="language-plaintext highlighter-rouge">Education</code>, and <code class="language-plaintext highlighter-rouge">Income</code>.</li> <li>The feature <code class="language-plaintext highlighter-rouge">BMI</code> is the body mass index, calculated from other responses.</li> </ul> <h3 id="previous-work">Previous work</h3> <p>On Kaggle, Teboul provided a link to a relevant study, <a href="https://www.cdc.gov/pcd/issues/2019/19_0109.htm">posted on the CDC website</a>, by Xie et al., which aimed to use predictive models for identifying type 2 diabetes. This study selected 26 of the original variables from the CDC survey.</p> <p>The target variable fetched by Teboul (for the 3-class data set) is the same as the linked study. However, most of the independent variables in the study are <strong>not</strong> those used by Teboul. A few variables align – e.g., <code class="language-plaintext highlighter-rouge">GenHlth</code>, <code class="language-plaintext highlighter-rouge">MentHlth</code>, <code class="language-plaintext highlighter-rouge">AnyHealthcare</code> (<code class="language-plaintext highlighter-rouge">HLTHPLN1</code> originally); some contain similar or reformatted information – e.g., the study uses a categorized version of <code class="language-plaintext highlighter-rouge">BMI</code>; but, many are completely distinct. (See Appendix A in the study.)</p> <p>In the study by Xie et al., the most accurate model was a neural network model, with 82.41% accuracy. However, this model only had a recall of 0.3781. The researchers fit a decision tree model which had accuracy 74.26% and recall of 0.5161. They also used a support vector machine with Gaussian kernel, achieving accuracy of 81.78% and recall of 0.4014.</p> <p>My students used a support vector machine with Gaussian kernel to fit a predictive model to the balanced data posted on Kaggle by Teboul. They obtained between 74% and 75% accuracy on test data, but did not report on the recall for the model.</p> <h3 id="reconsidering-the-data">Reconsidering the data</h3> <p>Here, I’ll work with the balanced data that my students used. Before jumping into training a model, let’s think about properties of this data. Of the 21 independent variables, 14 are binary. Consider just these 14 columns in the data; that is, for the moment, remove the other columns so that we have just over 70,000 instances, each a vector of fourteen 0’s and 1’s. There are \(2^{14} \approx 16,000\) possible vectors, so we are guaranteed that there are some distinct rows (instances) that are identical in these binary columns.</p> <p>I come from a background that is on the geometry side of mathematics, so I think of each of these binary vectors as representing a vertex on a hypercube \([0,1]^{14} \subset \mathbb R^{14}\). While some of the vertices will represent multiple instances in the data, how <em>mixed</em> are the vertices in terms of the target label (on average)? In other words, are most instances at a vertex where the proportion of the other instances at the same vertex, that have the same label, is something close to 0.5? Or are most of the vertices “<em>mostly just one label</em>”?</p> <p>First off, since 70,692 is a bit shy of \(4.5*16000\), if the points were evenly spread about the \(2^{14}\) vertices then each vertex would correspond to either 4 or 5 instances in the data (i.e., it would have close to \(6\times10^{-3}\) percent of the data).</p> <p>Below, I find instances which correspond to the same vertex as row \(0\) (see second Jupyter notebook cell). This is about \(0.6\ \%\) of the data. I then compute the average of $y$-labels of these instances, getting about \(49.3\ \%\). In other words, the labels of the data at that same vertex are nearly evenly split. Luckily, this is not the case for all vertices. In the later notebook cells, we see that for the instances that match row \(20\) (which is over \(4.5\ \%\) of the data), only \(11.6\ \%\) of them have $y$-label equal to 1; and, for the instances that match row \(2725\) (which is about \(0.51\ \%\) of the data), over \(84\ \%\) of them have $y$-label equal to 1.</p> <div class="jupyter-notebook" style="position: relative; width: 100%; margin: 0 auto;"> <div class="jupyter-notebook-iframe-container"> <iframe src="/assets/jupyter/vertex-label-mix.ipynb.html" style="position: absolute; top: 0; left: 0; border-style: none;" width="100%" height="100%" onload="this.parentElement.style.paddingBottom = (this.contentWindow.document.documentElement.scrollHeight + 10) + 'px'"></iframe> </div> </div> <p>What do we learn from this? While these 14 binary variables are not “just noise,” <d-footnote>They would be if most of the possible hypercube vertices either had around a 50% mix of labels or did not correspond to any instance.</d-footnote> the inability to separate some 0-labeled points from the 1-labeled points (that are at the same vertex) will present difficulty for classification – unless the remaining 7 variables provide a clear separation of such points. Considering those remaining variables, the <code class="language-plaintext highlighter-rouge">BMI</code> and <code class="language-plaintext highlighter-rouge">Age</code> variables seem to have the best chance of separating the data.</p> <h2 id="gaussian-kernel-svms">gaussian kernel SVMs</h2> <h2 id="results-on-cdc-data">results on CDC data</h2> <h2 id="takeaway">takeaway</h2>]]></content><author><name>Christopher Cornwell</name></author><category term="svm"/><category term="data-quality"/><category term="courses"/><category term="coding-challenge"/><summary type="html"><![CDATA[a post about this publicly-available data, and using kernel-based SVMs]]></summary></entry><entry><title type="html">100-day Coding Challenge Log</title><link href="https://cornwell.github.io/blog/2025/code-challenge/" rel="alternate" type="text/html" title="100-day Coding Challenge Log"/><published>2025-05-29T00:00:00+00:00</published><updated>2025-05-29T00:00:00+00:00</updated><id>https://cornwell.github.io/blog/2025/code-challenge</id><content type="html" xml:base="https://cornwell.github.io/blog/2025/code-challenge/"><![CDATA[<style>table th:first-of-type{width:15%}table th:nth-of-type(2){width:15%}table th:nth-of-type(3){width:70%}</style> <p>From May 27 to Sept 03, 2025.</p> <table> <thead> <tr> <th style="text-align: center">date</th> <th style="text-align: left">topic</th> <th style="text-align: left">activity</th> </tr> </thead> <tbody> <tr> <td style="text-align: center">may 27</td> <td style="text-align: left">svm classifier, course projects</td> <td style="text-align: left">initial read-in of 2015 cdc survey data (from balanced kaggle dataset); checked results from students’ fit to data using polynomial kernel svm, rbf kernel svm; decided rbf kernels were best type, and student models were underfitting; performed a search for best hyperparameters - student used data: \(\gamma\in [0.1,0.2,0.3,\ldots,1]\), \(C\in [0.5, 0.75, 1,\ldots, 2.5]\)); after that, tuned precision - result: \(\gamma = 0.28, C = 0.87\). Test acc.: \(\approx\) 74%.</td> </tr> <tr> <td style="text-align: center">may 28</td> <td style="text-align: left">svm classifier, course projects</td> <td style="text-align: left">considered properties of 2015 survey data - binary variable types; used unrestricted decision tree to check label-purity of vertices (hypercube vx’s corresp. to binary variables); large proportion of training were support vectors (would increase in-sync with increasing amount of data used); almost all dual coefficients equal to -1.5 or 1.5; computed distance to marginal curves - want to understand more.</td> </tr> <tr> <td style="text-align: center">may 29</td> <td style="text-align: left">svm classifier, course projects</td> <td style="text-align: left">included <code class="language-plaintext highlighter-rouge">Income</code>, <code class="language-plaintext highlighter-rouge">Education</code>, and <code class="language-plaintext highlighter-rouge">NoDocbcCost</code> columns (diff.from students); searched for best with this data: \(\gamma\in [0.25,0.3,0.35,\ldots,0.7]\), \(C\in [1, 1.1, 1.2,1.3, 1.4, 1.5]\); best result: \(\gamma=0.45, C=1.4\), test accuracy = 74.9% and recall = 0.80; also tried wildly different values for \(\gamma, C\) (other orders of magnitude), without improvement in accuracy.</td> </tr> <tr> <td style="text-align: center">may 30</td> <td style="text-align: left">boosting, course projects</td> <td style="text-align: left">also attempted to fit data with an ensemble method – boosted decision trees (using AdaBoost); after a search for the value of hyperparameter <code class="language-plaintext highlighter-rouge">max_depth</code> that would not overfit, used decision tree with depth 5 as base estimator; test accuracy of 74.6% and recall of 0.786.</td> </tr> <tr> <td style="text-align: center">may 31</td> <td style="text-align: left">blog post, course projects</td> <td style="text-align: left">made visuals: on toy data for discussing support vectors of kernel-based svms; confusion matrices.</td> </tr> <tr> <td style="text-align: center">june 1</td> <td style="text-align: left">blog post, course projects</td> <td style="text-align: left">began writing blog post on this post-course project exploration; figured some ways to adjust style of <code>layout: post</code>; looked up how to include Jupyter notebook output into distill-style blog.</td> </tr> </tbody> </table>]]></content><author><name>Christopher Cornwell</name></author><category term="coding-challenge"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Pre-decisive sets for ReLU networks</title><link href="https://cornwell.github.io/blog/2025/JE-polyhedralrelations/" rel="alternate" type="text/html" title="Pre-decisive sets for ReLU networks"/><published>2025-03-02T14:59:00+00:00</published><updated>2025-03-02T14:59:00+00:00</updated><id>https://cornwell.github.io/blog/2025/JE-polyhedralrelations</id><content type="html" xml:base="https://cornwell.github.io/blog/2025/JE-polyhedralrelations/"><![CDATA[<p>In a <a href="https://cornwell.github.io/blog/2024/precomposing-funcdim/">post last December</a>, I wrote some of my thoughts about Section 8 in the <a href="https://arxiv.org/abs/2209.04036">paper by Grigsby, Lindsey, Meyerhoff, and Wu</a>, and the effect of precomposing with a layer. Something that I continued to think about, with collaborator Na Zhang, was how to leverage what that Section says about \(\dim_{fun+}(\theta)\) versus \(\dim_{fun}(\theta)\) for this purpose.</p> <p>It lead to some basic observations, but we think they’ll be helpful.</p> <hr/> <h2 id="notation">Notation</h2> <p>We are considering feed-forward neural networks with ReLU activation function. Suppose that the input space is \(\mathbb R^{n_0}\) and that the parameter space is \(\Omega\) (which may be identified with \(\mathbb R^D\) as a set, \(D\) being the total number of parameters – weights and biases). We fix a parameter \(\theta\in\Omega\), and so determine a network \(\mathcal N(\theta)\), and we assume \(\theta\) is “nice” in the sense of the source paper (it is <em>generic</em>, <em>transversal</em>, and <em>combinatorially stable</em>). The network has an associated network function \(F_\theta:\mathbb R^{n_0}\to\mathbb R\).<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup></p> <p>Associated to \(\mathcal N(\theta)\) is a <em>canonical polyhedral complex</em> \(\mathcal C(\theta)\) that decomposes \(\mathbb R^{n_0}\). A <strong>top cell</strong> of \(\mathcal C(\theta)\) refers to an \(n_0\)-dimensional cell of that polyhedral complex. Note that the restriction of \(F\) to any top cell consists of an affine linear function on that top cell (cf. Lemma 2.8 of [GLMW]).</p> <p>Consider a finite subset \(Z = \{z_1,\ldots,z_m\}\subset\mathbb R^{n_0}\). For a parameter \(v\), the evaluation map \(E_Z\) is defined by \(E_Z(v) = (F_v(z_1),\ldots,F_v(z_m))\). Supposing that points in \(Z\) are parameterically smooth for \(\theta\), there is a neighborhood \(V\subset\Omega\) of \(\theta\) on which \(E_{Z}:V \to \mathbb R^{m}\) is defined and differentiable. I will pay particular attention to the singleton case, \(Z = \{z\}\), where \(z\) is a point in the interior of some top cell for \(\theta\). In this case, we write the Jacobian (gradient) as \(JE_z\vert_{\theta}\) (or, simply \(JE_z\)).</p> <p>Let \(R\) be a top cell of the canonical polyhedral complex for \(\mathcal N(\theta)\) and suppose that \(\{x_0,x_1,\ldots,x_{n_0}\}\) is affinely independent and contained in the interior of \(R\). I observed in my last post, using some basic linear algebra, that if \(x\) is any point in the interior of \(R\) then we have the equation</p> <p>\begin{equation} \label{eqn:JE-lincombo} JE_x - JE_{x_0} = \sum_{i=1}^{n_0} c_i(JE_{x_i} - JE_{x_0}), \end{equation} where the vector \(x-x_0\) satsifies \(x-x_0 = \sum_{i=1}^{n_0} c_i(x_i - x_0)\). In short, the reason for this is that, while \(JE_z\) consists of components that are polynomials in the parameters of the network, if we compute the partial derivatives and then consider it as a function of \(z\) (in \(R\)), rather than parameters, then it is affine linear.</p> <p>Note that (\ref{eqn:JE-lincombo}) fails if \(x\) is contained in another top cell than \(R\). However, if we replace \(F\) by the affine linear function (extended over \(\mathbb R^{n_0}\)) that agrees with \(F\vert_{R}\), and discuss the Jacobian of the evaluation map of that function, then the expression would be valid for all \(x\in\mathbb R^{n_0}\). For that Jacobian, of the evaluation map corresponding to \(F\vert_{R}\), at \(x\), we use the notation \(JE_x^{R}\) and we call this the <strong>Jacobian at</strong> \(R\).</p> <hr/> <h2 id="relations-on-jacobians-at-adjacent-top-cells">Relations on Jacobians at adjacent top cells</h2> <p>Consider two hyperplanes \(H_1\) and \(H_2\) in \(\mathbb R^{n_0}\) which correspond to two of the neurons from layer 1 of \(\mathcal N(\theta)\). As \(\theta\) is generic, \(H_1\cap H_2\) is an \((n_0-2)\)-dimensional affine subspace. Choose a point \(q\) in the intersection that is not contained in any other (bent) hyperplane from another layer (i.e., \(q\) is in the relative interior of an \((n_0-2)\)-dimensional face of \(\mathcal C(\theta)\) contained in both \(H_1\) and \(H_2\)). Suppose that we have top cells \(R, S, A,\) and \(B\) which all contain \(q\) in one of their \((n_0-2)\)-dimensional faces; every neighborhood of \(q\) intersects in a non-empty set with each of \(R, S, A,\) and \(B\), and this is true of no other top cell. Furthermore, choose our naming so that:</p> <ul> <li>\(R\) and \(S\) do not share a facet;</li> <li>\(A\) and \(B\) do not share a facet.</li> </ul> <p>We’ll check a certain relation on Jacobians at these cells. Let \(x\) be a point, and say that \(z^R_0,z^S_0,z^A_0,\) and \(z^B_0\) are points in \(R, S, A,\) and \(B\), respectively.</p> <p>Since each component function of \(JE^R\) is affine linear, there is some matrix \(W^R\) so that \(JE^R_x - JE^R_{z^R_0} = W^R(x - z^R_0)\).</p> <p>Choose one component (entry) of \(JE_x^R\), denote it by \(p^R_x\), and denote the corresponding entry of \(JE_x^A\) by \(p^A_x\). WLOG, we may assume that \(R\) and \(A\) are on the same side of the hyperplane \(H_2\), meaning that they share a facet that is contained in \(H_1\). Denote the weights and bias corresponding to \(H_1\) by \(w^1_1, w^1_2, \ldots, w^1_{n_0}, b^1\). Also, for the set of all weight/bias (variables) for the network except those \(n_0+1\) variables for \(H_1\), use notation \(\Theta\).</p> <p>We may write \(p^R_x\) and \(p^A_x\) as polynomials in \((\mathbb R[\Theta])[w^1_1,\ldots,w^1_{n_0},b^1]\). These polynomials have degree at most 1 (in these variables).</p> <p>Furthermore, since the ternary labelings of \(R\) and \(A\) agree for all neurons except \(H_1\), the only difference between \(JE^R_x - JE^R_{z^R_0}\) and \(JE^A_x - JE^A_{z^R_0}\) is that one of them has degree 1 terms in \((\mathbb R[\Theta])[w^1_1,\ldots,w^1_{n_0},b^1]\) and those are missing in the other. In other words, \(JE^R_x - JE^A_x\) entirely consists of such degree 1 terms, each of which contains a factor of a component in \((x - z^R_0)\).</p> <p>One can do the same thing with \(JE^B_x-JE^S_x\), and must subtract in this order to get the signs on these degree 1 terms to match. There are no additional (or missing) non-zero terms in this, since, even though the ternary label for \(H_2\) has changed in these cells, we have a lemma.</p> <h5 class="env-title" id="lemma">Lemma.</h5> <p>For \(X \in\{R,S,A,B\}\), if a term in \(p^X_x\) is degree 1 in one of the weights and biases from \(H_2\), then it is degree 0 in \(\{w^1_1,\ldots,w^1_{n_0},b^1\}\).</p> <blockquote> <p><em>Proof</em>.    The statement holds because neurons for \(H_1\) and \(H_2\) are in the same layer. Thus, from the compositional structure of the feed-forward neural network, their weights/biases are never jointly in the same monomial of \(E_x\) (as a polynomial in the parameter variables). \(\blacksquare\)</p> </blockquote> <p>We have, therefore, that \(JE^R_x - JE^A_x = JE^B_x - JE^S_x\). By rearranging, \begin{equation} JE^R_x + JE^S_x = JE^A_x + JE^B_x \end{equation}</p> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>Often, I will simply write the network function as \(F:\mathbb R^{n_0}\to\mathbb R\), leaving the parameter of the network as understood.) <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="math"/><category term="neural_networks"/><category term="research"/><category term="thoughts"/><category term="ReLU_networks"/><summary type="html"><![CDATA[on minimal sets that are subsets of decisive sets for ReLU networks]]></summary></entry><entry><title type="html">Questions on functional dimension of ReLU networks</title><link href="https://cornwell.github.io/blog/2024/precomposing-funcdim/" rel="alternate" type="text/html" title="Questions on functional dimension of ReLU networks"/><published>2024-12-20T10:39:00+00:00</published><updated>2024-12-20T10:39:00+00:00</updated><id>https://cornwell.github.io/blog/2024/precomposing-funcdim</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/precomposing-funcdim/"><![CDATA[<p>I first started writing this post as a draft at my old Wordpress website. I’m attempting to port it here. Please be patient if I missed a spot where some syntax or formatting needed to be translated.</p> <hr/> <p>This is one of a series of posts about ReLU neural networks. My focus in this post is on the <strong>functional dimension</strong> of (a parameter for) a ReLU neural network. <br/>      See <em>Functional dimension of feedforward ReLU neural networks</em> by Grigsby, Lindsey, Meyerhoff, and Wu (<a href="https://arxiv.org/abs/2209.04036">link to the preprint</a>).</p> <p>I will abbreviate references to this paper by writing [GLMW22].  In particular, I want to work on understanding Lemmas 8.2 and 8.5, as well as Theorem 8.7 in that paper – having to do with how the functional dimension behaves under composition of networks.</p> <h2 id="setup-and-notation">Setup and notation</h2> <hr/> <p>The notation used will largely match that of [GLMW22]. We’ll be working with feedforward ReLU neural networks and their associated network function. Here is the general setup.</p> <p><strong>Definition 1.</strong> Let \(n_0\in \mathbb N\) and \(d \in \mathbb N\) and let \(\phi:\mathbb R \to \mathbb R\) be a continuous function. A <strong>feedforward neural network</strong> \(\mathcal N\) defined on \(\mathbb R^{n_0}\), with <em>depth</em> \(d\) and <em>activation function</em> \(\phi\), is an ordered sequence of \(d\) affine maps \(A^1, A^2, \ldots, A^d,\) with associated positive integers \(n_1,n_2,\ldots,n_d,\) that are defined such that \(A^{\ell}:\mathbb R^{n_{\ell-1}} \to \mathbb R^{n_{\ell}}\) for every \(1 \le \ell \le d.\) For any \(n\in \mathbb N\), define \(\Phi:\mathbb R^n\to\mathbb R^n\) as the component-wise application of the function \(\phi\) on the input. The <strong>network function</strong> associated to \(\mathcal N\) is the function \(F:\mathbb R^{n_0}\to \mathbb R^{n_d}\) that is defined by</p> \[F(\mathbf{x}) = A^d\circ \Phi\circ A^{d-1} \circ \ldots \circ \Phi\circ A^{2} \circ \Phi\circ A^{1}(\mathbf{x})\] <p>for \(\mathbf{x} \in \mathbb R^{n_0}\). The list \((n_0, n_1, \ldots, n_d)\) is called the <strong>architecture</strong> of the neural network \(\mathcal N\).</p> <p><br/></p> <p>In this post the focus is exclusively on <em>ReLU (feedforward) neural networks</em> which are feedforward neural networks that have an activation function given by \(\phi(x) = \max\{0, x\}\), for \(x\in \mathbb R\).</p> <p>For every \(1\le \ell\le d-1\), the <strong>\(\ell\)-th layer map</strong> \(F^{\ell}:\mathbb R^{n_{\ell-1}} \to \mathbb R^{n_{\ell}}\) is defined by \(F^{\ell} = \Phi\circ A^{\ell}\), with the \(d\)-th layer map being simply \(A^d\).  For every \(1\le \ell\le d\) and every \(1\le j\le n_{\ell}\), the pair \((\ell, j)\) is referred to as a <strong>neuron</strong> of \(\mathcal N\); sometimes this <strong>neuron</strong> refers to the function \(z^{\ell}_j : \mathbb R^{n_0} \to \mathbb R\) that takes on the (“pre-activation”) values at \((\ell, j)\), i.e., letting \(\pi_j\) denote projection to the \(j\)-th coordinate, we have that \(z^{\ell}_j = \pi_j \circ A^{\ell}\circ F^{\ell-1} \circ\ldots\circ F^1\).</p> <p>Note that each affine map \(A^{\ell}\) may be expressed in coordinates, so that for \(\mathbf{x}\in\mathbb R^{n_{\ell-1}}\), we have \(A^{\ell}(\mathbf{x}) = W^{\ell}\mathbf{x} + \mathbf{b}^{\ell}\) where \(W^{\ell} \in \mathbb R^{n_{\ell}\times n_{\ell-1}}\) is a “weight” matrix and \(\mathbf{b}^{\ell} \in \mathbb R^{n_\ell}\) is a “bias” vector.</p> <p>Thinking of ReLU neural networks with a given architecture \((n_0, n_1, \ldots, n_d)\) – or, more appropriately, the associated network functions for such networks – as a parameterized class of functions, the entries in the weight matrices and bias vectors for the network’s layers make up the parameters. We organize the parameters as \(\theta = (W^1, \mathbf{b}^1, \ldots, W^d, \mathbf{b}^d)\), which we associate in some chosen manner to a point in \(\mathbb R^D\), with \(D = D(n_0,\ldots,n_d) := \sum_{i=1}^d n_i(n_{i-1}+1).\) We write \(\Omega := \mathbb R^D\) in order to emphasize the space of parameters – and how it corresponds to weights and biases in \(\mathcal N\). At times, in order to denote the neural network with parameters \(\theta\), for an understood architecture, we write \(\mathcal N(\theta)\). Additionally, the associated network function may be written as \(F_\theta: \mathbb R^{n_0} \to \mathbb R^{n_d}.\)</p> <h2 id="functional-dimension-of-a-parameter-for-a-relu-neural-network">Functional Dimension of a Parameter for a ReLU Neural Network</h2> <hr/> <p>The lemmas and theorem that we want to understand better are related to a quantity called \(\dim_{fun}(\theta)\), the functional dimension of \(\theta\) (for a ReLU network \(\mathcal N(\theta)\)). Some effort is required to define this quantity. For the definition, there are a few technical assumptions that must be made about \(\theta\), but the assumptions are not very restrictive. To the contrary, they rule out some rare situations for the network. In order to arrive at the definition of \(\dim_{fun}(\theta)\) quickly, however, for now we bypass describing the technical assumptions; we will say simply that \(\theta\) is “nice” (or, “satisfies nice conditions”) and move forward.</p> <p>Consider a fixed architecture \((n_0, n_1, \ldots, n_d)\) and the class of ReLU networks with this architecture. Given some \(\theta\in\Omega\), for \(1\le j\le n_d\), use \(F_{\theta,j}:\mathbb R^{n_0}\to\mathbb R\) to denote the \(j\)-th coordinate function of \(F_\theta\), so for every \(\mathbf{x}\in\mathbb R^{n_0}\) we have \(F_\theta(\mathbf{x}) = (F_{\theta,1}(\mathbf{x}), \ldots, F_{\theta,n_d}(\mathbf{x}))\).<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">1</a></sup> Additionally, consider a finite ordered set of points \(Z = \{z_1, z_2, \ldots, z_k\} \subset \mathbb R^{n_0}\).  The evaluation map at \(Z\), \(E_Z: \Omega \to \mathbb R^{k\cdot n_d}\) is given by setting, for every \(\theta\in\Omega\),</p> \[E_Z(\theta) = (F_{\theta,1}(z_1), \ldots, F_{\theta,n_d}(z_1), \ldots, F_{\theta,1}(z_k), \ldots, F_{\theta,n_d}(z_k)).\] <p>To define the functional dimension, we use the Jacobian matrix of the evaluation map, which we denote by \(\mathbf{J}E_Z\).  In other words, for each \(z_i\in Z\), let \(\mathbf{J}E_{z_i}\lvert_{\theta}\) be the \(n_m \times D\) matrix, the \(j\)-th row of which is the vector of partial derivatives of \(F_{\theta, j}(z_i)\) <em>with respect to the</em> \(D\) <em>parameters</em>,<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">2</a></sup>  evaluated at \(\theta\). The matrix \(\mathbf{J}E_Z\lvert_{\theta}\) is the \(k\cdot n_m \times D\) matrix obtained by stacking these matrices. There are values of \(\theta\) at which some of the partial derivatives in \(\mathbf{J}E_Z\) are not well-defined – hence, the reason for making “nice” assumptions.</p> <p><strong>Definition 2.</strong> Fix an architecture \((n_0, n_1, \ldots, n_d)\) and let \(\Omega\) be its corresponding parameter space. Suppose that \(\theta\in\Omega\) satisfies nice conditions. <sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">3</a></sup> The <strong>functional dimension</strong> at \(\theta\) is defined to be</p> \[\dim_{fun}(\theta) = \underset{Z \text{ is finite and smooth for } \theta}{\sup}\operatorname{rank} \mathbf{J}E_Z\lvert_{\theta}.\] <p>In Definition 2, the condition that \(Z\) be smooth for \(\theta\) (or, <em>parametrically smooth</em> in [GLMW22]) refers to the function \(\mathcal{F}:\Omega\times\mathbb R^{n_0} \to \mathbb R^{n_d}\), given by \(\mathcal{F}(\theta, x) = F_\theta(x)\), being smooth at \((\theta, z_i)\) for every \(z_i\in Z\). One can also define the so-called <strong>batch functional dimension</strong> at \(\theta\) for a batch \(Z\), which is equal simply to the rank of \(\mathbf{J}E_Z\lvert_{\theta}\). Clearly, the batch functional dimension is less than or equal to the functional dimension.</p> <p>In Section 5 of [GLMW22], for a given network \(\mathcal N\) with parameter \(\theta\), the authors of that paper describe a finite set in \(\mathbb R^{n_0}\) for which the batch functional dimension for that set is equal to \(\dim_{fun}(\theta)\). To confirm that a <em>given</em> \(Z \subset \mathbb R^{n_0}\) is such a finite set, called a <strong>decisive set</strong> for \(\mathcal N(\theta)\), requires knowledge of the regions in \(\mathbb R^{n_0}\) on which \(F_{\theta}\) is affine-linear; more precisely, for each top-dimensional cell of the “canonical polyhedral complex” of \(F_\theta\), a decisive set contains the vertices of some \(n_0\)-dimensional simplex contained in that cell. Provided that \(\theta\) is “nice” and \(Z\subset\mathbb R^{n_0}\) is a decisive set, we have \(\dim_{fun}(\theta) = \operatorname{rank}\mathbf{J}E_Z\lvert_{\theta}\).</p> <p>A discussion of the “canonical polyhedral complex” of \(F_\theta\) won’t happen in this post. Perhaps I will describe it in another post. Suffice it to say that it consists, in part, of a set of (<em>top-dimensional</em>) polyhedral cells, no two of which intersect in their interiors, but where the union of their closures is the entire domain \(\mathbb R^{n_0}\); furthermore, \(F_\theta\) is affine-linear when restricted to any one of these cells.</p> <p>Finally, we will be interested in a certain restricted notion of functional dimension, when we consider only sets \(Z\) which are in the interior of the positive orthant \(\mathbb R_{&gt;0}^{n_0} = \{(x_1,\ldots,x_{n_0})\ \lvert\ x_i &gt; 0 \text{ for all } 1\le i\le n_0\}\). Write this as</p> \[\dim_{fun+}(\theta) = \underset{Z\subset\mathbb R_{&gt;0}^{n_0} \text{ is finite and smooth for } \theta}{\sup}\operatorname{rank} \mathbf{J}E_Z\lvert_{\theta}.\] <p>After a small change to the notation, the following is the statement of Lemma 8.2 in [GLMW22].</p> <h5 class="env-title" id="lemma-82">Lemma 8.2</h5> <p>Fix \(n_0\in\mathbb N\) and let \(\Omega\) be the parameter space for architecture \((n_1,n_2,\ldots,n_d)\). Let \(\theta\in\Omega\) be such that there is a smooth point for \(\theta\) in the strictly positive orthant \(\mathbb R_{&gt;0}^{n_1}\). Let \(A:\mathbb R^{n_0}\to\mathbb R^{n_1}\) be affine-linear such that every row of the associated matrix has at least one non-zero entry. Use \((A, \theta)\) to denote the parameter for architecture \((n_0,n_1,\ldots,n_d)\) that precomposes with \(A\), i.e., \(\Phi\circ A\) is the first layer map of the network function \(F_{(A,\theta)}\). <br/> If \((A, \theta)\) satisfies nice conditions, then \(\dim_{fun}(A,\theta) \le n_0n_1 + \dim_{fun+}(\theta)\). Furthermore, for this inequality to be equality it is necessary that \(\dim_{fun+}(\theta)\) be realized on a smooth set \(Z^* \subset \operatorname{Im}(\Phi\circ A)\).</p> <blockquote class="block-tip"> <h5 id="proof-sketch">Proof sketch</h5> <p>The proof of this lemma uses the scaling-inverse scaling invariance of \(F_{(A,\theta)}\), in the first hidden layer. The authors use this to identify the parameter space for \((A,\theta)\) with coordinates in the product \(\mathbb R_{&gt;0}^{n_1} \times (\mathbb R^{n_0})^{n_1} \times \Omega\). Then they argue that, for \(Z\subset\mathbb R^{n_0}\), the columns of \(\mathbf{J}E_Z\lvert_{(A,\theta)}\) that correspond to parameters in \(\mathbb R_{&gt;0}^{n_1}\) will be zero, and that the columns corresponding to parameters in \((\mathbb R^{n_0})^{n_1}\) contribute at most \(n_0n_1\) to the rank.</p> <p>Finally, using that \(\Phi\circ A(Z)\) is a subset of  \(\mathbb R_{\ge 0}^{n_1}\), they argue that the rank of columns corresponding to parameters in \(\Omega\) will at most be the \(\sup\) of the rank of \(\mathbf{J}\) of the evaluation map on subsets in \(\mathbb R_{&gt;0}^{n_1}\), evaluated at \(\theta\), which equals \(\dim_{fun+}(\theta).\) \(\blacksquare\)</p> </blockquote> <p>Note that all of the assumptions in Lemma 8.2 that occur before the word “Furthermore” will be true of a full measure subset in the parameter space for \((A, \theta)\). This is proven in [GLMW22] and remarked upon at the beginning of subsection 8.1.  Moreover, if \(n_0 \ge n_1\) then there is a full measure subset of parameters so that \(A\) is surjective, in which case \(\operatorname{Im}(\Phi\circ A) = \mathbb R_{\ge 0}^{n_1}\). By definition, this contains any subset \(Z^*\) that realizes \(\dim_{fun+}(\theta)\). So, if \(n_0 \ge n_1\) then a full measure set of parameters satisfies that necessary condition.</p> <p>However, if \(n_0 &lt; n_1\) then \(\operatorname{Im}(\Phi\circ A)\) cannot contain a set of points in \(\mathbb R_{&gt;0}^{n_1}\) that are vertices of an \(n_1\)-simplex – since \(\operatorname{Im}(\Phi\circ A) \cap \mathbb R_{&gt;0}^{n_1}\) is contained in an \((n_1-1)\)-dimensional affine subspace. This means that any decisive set that realizes \(\dim_{fun+}(\theta)\) is not in the image. However, if \(\dim_{fun+}(\theta)\) can be realized on a set that is <em>not</em> decisive, perhaps it is possible in such a situation to satisfy this condition.</p> <h6 class="env-title" id="question">Question</h6> <p>How do we understand the difference between \(\dim_{fun+}(\theta)\) and \(\dim_{fun}(\theta)\)?</p> <h2 id="technical-lemma-85">Technical Lemma 8.5</h2> <hr/> <p>Let’s discuss Lemma 8.5. We’ll need, at least, the notion of the ternary labeling at \(x \in \mathbb R^{n_0}\), determined by \(\theta\). At this juncture, a (minor) reckoning has arrived. As is common for parameterized classes of functions, and in mathematical modeling, we used the notation \(\theta\) for our parameter vector in \(\mathbb R^{D}\), with \(D\) being the total number of weights and biases in network architecture \((n_0,n_1,\ldots,n_d)\). However, the letter \(\theta\) is also used in the literature to denote ternary labelings, which are functions \(\mathbb R^{n_0} \to \{-1, 0, 1\}^{N}\) with \(N = n_1+n_2+\ldots+n_d\). As a solution, we will try using the letter \(\tau.\)</p> <p>So, a definition. For this definition, recall for each neuron \((\ell, j)\) the pre-activation function \(z^{\ell}_j:\mathbb R^{n_0}\to\mathbb R\), when \(1\le \ell\le n_d\) and \(1\le j\le n_\ell.\)</p> <p><strong>Definition 3.</strong> Let \(\theta\in\Omega\) be the parameter for a network with architecture \((n_0,n_1,\ldots,n_d)\). For each neuron \((\ell, j)\) of the network, define \(\tau^\ell_j:\mathbb R^{n_0} \to \{-1, 0, 1\}\) by setting \(\tau^\ell_j(x) = \text{sign}(z^\ell_j(x))\). (Here, the function \(\text{sign}\) returns 0 if the input is zero, 1 if it is positive, and -1 if it is negative.) The (full) <strong>ternary labeling</strong> for the network is the function \(\tau: \mathbb R^{n_0} \to \{-1, 0, 1\}^{N}\) which has a coordinate function for each neuron \((\ell, j)\), namely the function \(\tau^\ell_j.\)</p> <p>Intuitively speaking, the value of \(\tau^\ell_j(x)\) is positive if and only if the neuron \((\ell, j)\) is “on” or “activated” at the point \(x\). It is zero at \(x\) if and only if \(F_{\ell-1}\circ\ldots\circ F_1(x)\) lies inside of the hyperplane \(\{y\ \lvert\ W^\ell y + b^\ell = 0\}.\)</p> <p>For Lemma 8.5, we have a similar setup to Lemma 8.2. There is a parameter \(\theta\in\Omega\), for a network with architecture \((n_1,\ldots, n_d)\), an affine-linear map \(A:\mathbb R^{n_0}\to\mathbb R^{n_1}\) (we may also use \(A\) for the \(n_1\times(n_0+1)\) matrix that determines it, the last column for the translate, or “shift”). We are interested in precomposing with \(A\) to get a neural network with architecture \((n_0,n_1,\ldots,n_d)\) and with parameter \((A, \theta)\).  Note that, before precomposing, there are coordinate ternary labelings \(\tau^\ell_j\) for every  \(1\le \ell\le n_d-1\) and \(1\le j\le n_{\ell+1}.\)<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">4</a></sup></p> <h5 class="env-title" id="lemma-85">Lemma 8.5</h5> <p>Suppose that \(\theta\) and \(A\) are as above and that \(\theta\) satisfies “nice conditions.”  For \(1\le j\le n_1\) and \(x\in \mathbb R^{n_0}\), use \(\tau^A_j(x)\) to denote the ternary label (at \(x\)) with respect to the \(j\)th row of \(A\).  Assume that \(A\) is non-degenerate, in the sense that for \(1\le j\le n_1\) the set where \(\tau^A_j(x) = 0\) is a hyperplane in \(\mathbb R^{n_0}\). We suppose that for every \(1\le k\le n_1\), there exists \(y_k\in\mathbb R^{n_0}\) such that <br/>      (i) \(\tau^A_k(y_k) = 0\), <br/>      (ii) \(\tau^A_j(y_k) \ne 0\) for all \(j \ne k\), <br/>      (iii) for all \((\ell, j)\) with \(1\le \ell\le n_d-1\) and \(1\le j\le n_{\ell+1}\), we have that \(\tau^\ell_j( \Phi\circ A(y_k) ) \ne 0\), and <br/>      (iv) the \(k\)th column of \(\mathbf{J}F_{\theta}\lvert_{\Phi\circ A(y_k)}\) is not the zero vector.<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">5</a></sup> <br/> Then there is a finite set \(Z\) in \(\mathbb R^{n_0}\) such that (v) up to scaling rows of \(A\) by positive numbers, each entry of \(A\) is given by a unique affine-linear combination of the coordinates of the vector \(E_Z(A, \theta)\); and (vi) the ternary labeling for \((A, \theta)\) of every point in \(Z\), at every neuron, is non-zero. <br/> Finally, the lemma also states that if \(y_k \in \mathbb R_{&gt;0}^{n_0}\) for every \(k\) then (vii) the set \(Z\) can be chosen to be in \(R_{&gt;0}^{n_0}\).</p> <p><br/></p> <p>Now, finding a set \(y_1, \ldots, y_{n_1}\) that satisfy conditions (i) - (iii) is generically possible – the conditions (i) and (ii) can be guaranteed as long as the hyperplane arrangement in \(\mathbb R^{n_0}\) associated to \(A\) is generic; condition (iii) says that each of the \(n_1\) points \(\Phi\circ A(y_k)\) is contained in the interior of a top-dimensional cell of the canonical polyhedral decomposition for \(\mathcal N(\theta)\) and this can be achieved for a choice of \(y_1,\ldots,y_{n_1}\) by a perturbation of \(\theta\).</p> <p>However, there is a positive measure subset of parameters for which condition (iv) will be impossible to satisfy. For example, there is a positive measure subset such that \(F_\theta\) is a constant function on all of \(\mathbb R^{n_1}\). Using \(H_k\) to denote the hyperplane where \(\tau^A_k\) is zero, there is a larger subset on which \(F_\theta\) is constant on the set \(\Phi\circ A(H_k)\) (and in any “nearby” choice of \((A,\theta)\) too).</p> <blockquote class="block-tip"> <h5 id="proof-sketch-of-lemma-85">Proof sketch of Lemma 8.5</h5> <p>To prove Lemma 8.5, the authors of [GLMW22] first note that, as a consequence of the assumptions (i) - (iii), for each \(1\le k\le n_1\), there is an open neighborhood \(U_k\) of \(y_k\) so that the ternary labelings of all neurons in \(\mathcal N(A, \theta)\) are constant on \(U_k\) except for \(\tau^A_k\). Furthermore, letting \(U_k^+\) and \(U_k^-\) denote the connected components of \(U_k \setminus \{x\ \lvert\ \tau^A_k(x) = 0\}\), with sign of \(\tau^A_k\) on each component matching the superscript, we have that the ternary labeling on every neuron of \((A, \theta)\) is constant on \(U_k^+\) and on \(U_k^-\). As a consequence, \(F_{(A,\theta)}\) is affine-linear when restricted to either \(U_k^+\) or \(U_k^-\), and so \(\mathbf{J}F_{(A,\theta)}\) is constant on each of \(U_k^{\pm}\). Furthermore, \(\mathbf{J}F_\theta\) is constant when restricted to \(\Phi\circ A(U_k)\).</p> <p>Next, they show that \(\mathbf{J}F_{(A,\theta)}\lvert_{U_k^+} \ne \mathbf{J}F_{(A,\theta)}\lvert_{U_k^-}\). To do so, they use the chain rule and that \(F_{(A,\theta)} = F_\theta\circ (\Phi\circ A)\). Then, since \(\mathbf{J}(\Phi\circ A)\lvert_{U_k^+}\) contains a non-zero element in the \(k\)th row, and \(\mathbf{J}(\Phi\circ A)\lvert_{U_k^-}\) has a zero \(k\)th row, assumption (iv) guarantees a non-zero difference in one of the entries of \(\mathbf{J}F_{(A,\theta)}\lvert_{U_k^+} - \mathbf{J}F_{(A,\theta)}\lvert_{U_k^-}.\)</p> <p>Having determined that \(\mathbf{J}F_{(A,\theta)}\lvert_{U_k^+} \ne \mathbf{J}F_{(A,\theta)}\lvert_{U_k^-}\), they have a lemma (Lemma 8.3) that gives the conclusion (v). This lemma produces a set \(Z \subset U_k^+ \cup U_k^-\), which means that (vi) holds (by construction of \(U_k^{\pm}\)) and that (vii) must hold – making the neighborhood \(U_k\) smaller if needed. \(\blacksquare\)</p> </blockquote> <h5 class="env-title" id="lemma-83">Lemma 8.3.</h5> <p>Let \(M\) be a polyhedral complex embedded in \(\mathbb R^d\), \(d\ge 1\), and let \(F:\mathbb R^d \to \mathbb R^{n}\) be a continous map that is affine-linear on cells of \(M\). Let \(X, Y\) be two \(d\)-dimensional cells of \(M\) that share a \((d-1)\)-dimensional facet, and denote the hyperplane containing the shared facet by \(H\). Assume that \(\mathbf{J}F\lvert_X \ne \mathbf{J}F\lvert_Y\). Then, for any decisive sets, \(S_X\subset X\) for \(F\lvert_X\) and \(S_Y\subset Y\) for \(F\lvert_Y\), \(H\) is the solution set to an affine-linear equation \(\{x\ \lvert\ c + Ax = \mathbf{0}\}\) where every entry of \(A\) is an affine linear expression in the coordinates of \(E_{S_X\cup S_Y}(F)\). The matrix \(A\) is unique up to rescaling rows by constants.</p> <blockquote class="block-tip"> <h5 id="proof-sketch-1">Proof sketch</h5> <p>To prove Lemma 8.3, write the points \(S_X = \{z_0,z_1,\ldots,z_d\}\) (which are vertices of a \(d\)-dimensional simplex in \(X\), owing to the fact that \(F\lvert_X\) is affine-linear). Now, since the vectors \(u_i := z_i - z_0\), with \(1\le i\le d\), make a basis of \(\mathbb R^d\), each partial derivative \(\partial F/\partial x_i\) is a linear combination of the directional derivatives \(D_{u_i}F(z_0)\). Additionally, \(\lvert z_i-z_0\rvert D_{u_i}F(z_0)\) is the difference between two coordinates of \(E_{S_X}(F)\). And so, for every \(x\in X\), each entry of \(\mathbf{J}F\lvert_x\) is a linear combination of coordinates of \(E_{S_X}(F)\).  This is similarly true for \(\mathbf{J}F\lvert_y\), \(y\in Y\), and \(E_{S_Y}(F)\).</p> <p>Note that the extension to \(\mathbb R^d\) of the affine-linear map \(F\lvert_X\) can be expressed as \(\mathbf{x} \mapsto c_X + \mathbf{J}F\lvert_X\mathbf{x}\), for some constant vector \(c_X\) (and an analogous statement is true for \(Y\)). Since the hyperplane \(H\) that \(X\) and \(Y\) share consists of those \(\mathbf{x}\) where the extension of \(F\lvert_X\) and the extension of \(F\lvert_Y\) agree, we have</p> \[H = \{\mathbf{x} | c_X - c_Y + (\mathbf{J}F\lvert_X - \mathbf{J}F\lvert_Y)\mathbf{x} = \mathbf{0} \},\] <p>proving the statement. \(\blacksquare\)</p> </blockquote> <p>We now discuss Theorem 8.7, which provides sufficient conditions to have equality: \(\dim_{fun}(A, \theta) = n_0n_1 + \dim_{fun}(\theta)\).</p> <h5 class="env-title" id="theorem-87">Theorem 8.7.</h5> <p>Fix a parameter \(\theta\in\Omega\) which is “nice” and suppose that \(Z_1 \subset \mathbb R_{&gt;0}^{n_1}\) is a finite set whose ternary labels with respect to every neuron of \(\mathcal N(\theta)\) are nonzero, and so that \(\dim_{fun}(\theta) = \operatorname{rank} \mathbf{J}E_{Z_1}\lvert_\theta\). Suppose that \(A:\mathbb R^{n_0}\to\mathbb R^{n_1}\) is a surjective affine-linear map that satisfies all the assumptions of Lemma 8.5 (including that every \(y_k\) is in \(\mathbb R_{&gt;0}^{n_0}\)). Then there is a finite set \(Z \subset \mathbb R_{&gt;0}^{n_0}\) such that the ternary labeling, for all \(z\in Z\) and every neuron of \(\mathcal N(A,\theta)\), is nonzero, and</p> \[\dim_{fun}(A, \theta) = \operatorname{rank}\mathbf{J}E_Z\lvert_{(A,\theta)} = n_0n_1 + \dim_{fun}(\theta).\] <p>The proof of Theorem 8.7 is a bit more involved than the proofs of the lemmas above. However, let us remark on the assumptions being made to get the conclusion of this theorem.</p> <p>First, the existence of a set \(Z_1\), as in the theorem statement, requires that \(\dim_{fun+}(\theta) = \dim_{fun}(\theta)\). It would be valuable to understand what causes this to occur.</p> <p>Second, the surjectivity assumption on \(A\) requires that \(n_0 \ge n_1\). It also includes some restrictive assumptions (not full measure) in order to guarantee assumption (<em>iv</em>) of Lemma 8.5, as we discussed above, as well as guaranteeing that \(y_k\) can be chosen from \(\mathbb R_{&gt;0}^{n_0}\) for each \(k\). (For example, this is impossible if one of the hyperplanes/neurons of \(\Phi\circ A\) does not cut through the positive orthant.)</p> <h2 id="the-question-of-dim_funtheta-vs-dim_funtheta">The Question of \(\dim_{fun+}(\theta)\) vs. \(\dim_{fun}(\theta)\)</h2> <hr/> <p>As a start for understanding when \(\dim_{fun+}(\theta)\) and \(\dim_{fun}(\theta)\) are the same, let us make a simple observation. Let \(x_0, x_1, \ldots, x_{n_0}\) be affinely independent points in \(\mathbb R^{n_0}\) and let \(y_0,y_1,\ldots, y_{n_0} \in\mathbb R\). There is a unique affine linear function \(F:\mathbb R^{n_0} \to \mathbb R\) with the property that \(F(x_i) = y_i\) for every \(0\le i\le n_0\). This falls out of linear algebra.</p> <p>Indeed, since the points are affinely independent, \(\{x_i - x_0\ \lvert\ 1\le i\le n_0\}\) is a basis of \(\mathbb R^{n_0}\). The function \(F\) is determined by \(n_0+1\) scalars \(a_0,a_1,\ldots, a_{n_0}\), so that, writing \(\mathbf{a}\) for \((a_1,a_2,\ldots,a_{n_0})\), we have \(F(x) = a_0 + \mathbf{a}\cdot x\) for all \(x\in\mathbb R^{n_0}\). Note that \(y_i - y_0 = \mathbf{a}\cdot (x_i - x_0)\). Therefore, since for any \(x\in \mathbb R^{n_0}\), we have scalars \(c_1,\ldots, c_{n_0}\) so that \(x - x_0 = \sum_{i=1}^{n_0} c_i(x_i - x_0),\) we see that</p> \[F(x) - F(x_0) = \mathbf{a}\cdot (x - x_0) = \sum_{i=1}^{n_0} c_i\mathbf{a}\cdot(x_i - x_0) = \sum_{i=1}^{n_0} c_i(y_i - y_0).\] <p>Therefore, \(F(x) = y_0 + \sum_{i=1}^{n_0} c_i(y_i - y_0)\).</p> <p>Let’s consider a scenario.  Given an architecture of a ReLU network and a parameter \(\theta\in\Omega\) that is “nice,” say that we have a point \(p\) in the interior of a top-dimensional cell of the canonical polyhedral complex \(\mathcal C = \mathcal C(\theta)\). Write \(X\) for this cell containing \(p\). Further, suppose that \(Z\subset \mathbb R^{n_0}\) is a finite set so that:</p> <ol> <li>\(Z\) is the union of finite sets of points in the interior of top-dimensional cells of \(\mathcal C\);</li> <li>there is a vertex \(v\) of \(X\) such that for every top-dimensional cell \(A \ne X\) which has \(v\) as one of its vertices, \(int(A) \cap Z \ne\emptyset\); and</li> <li>if \(A\) is a top-dimensional cell and \(int(A)\cap Z\ne \emptyset\), then \(Z\) contains a decisive set (in \(A\)) for \(F_\theta\lvert_A\).<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">6</a></sup></li> </ol> <p>We <em>hope</em> that this will mean that \(\operatorname{rank}\mathbf{J}E_Z\lvert_\theta = \operatorname{rank}\mathbf{J}E_{Z^*}\lvert_\theta\), where \(Z^* = Z \cup \{p\}\).</p> <p>The intuition for this would be that, first, since \(Z\) has a decisive set on each cell \(A\) that it intersects, \(F_\theta\lvert_A\) is determined by values at points in \(Z\cap A\). This means that \(F_\theta\) is determined on the boundary of \(X\) on an affinely independent set of points.  Thus, it would seem that our set \(Z\) completely determines \(F_\theta\lvert_{X}\). That is, we should be able to “witness” the partial derivatives in \(E_{\{p\}}\lvert_\theta\) through rows of \(E_{Z}\lvert_\theta\).</p> <p>Let \(A\) be a top-dimensional cell of \(\mathcal C(\theta)\). The first thing to observe is that, related to the fact that an affine-linear function is determined by its values on \(n_0+1\) affinely independent points, if \(x_0,x_1,\ldots,x_{n_0}\) in \(int(A)\) is a set of affinely independent points then, for any \(x\in int(A)\), we can get \(\mathbf{J}E_x\lvert_{\theta}\) as a linear combination of \(\mathbf{J}E_{x_i}\lvert_{\theta}, 0\le i\le n_0\). Indeed, there is a unique set of scalars \(c_1,\ldots, c_{n_0}\) so that \(x - x_0 = \sum_{i=1}^{n_0} c_i (x_i - x_0)\).  Note that each element of \(\mathbf{J}E_{x}\lvert_\theta\) is either linear or constant in \(x\) (when restricting to \(int(A)\)). This means that</p> <p>\begin{equation} \label{eq:vertex-loop} \mathbf{J}E_{x}\lvert_\theta - \mathbf{J}E_{x_0}\lvert_\theta = \sum_{i=1}^{n_0}c_i (\mathbf{J}E_{x_i}\lvert_\theta - \mathbf{J}E_{x_0}\lvert_\theta), \end{equation}</p> <p>which expresses \(\mathbf{J}E_{x}\lvert_{\theta}\) in the desired way as a linear combination.</p> <p>Now, the vector \(\mathbf{J}E_{x}\lvert_{\theta}\) is not defined if \(x\) is contained in a facet of \(A\). <sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">7</a></sup> However, suppose that we use \eqref{eq:vertex-loop} to determine such a vector – this would be the limit of \(\mathbf{J}E_{p_i}\lvert_{\theta}\) for a sequence \(\{p_i\} \subset int(A)\), where \(p_i \to x\). Since it depends on “converging from \(int(A)\)”, say that we call this vector \(\mathbf{J}^AE_x\lvert_{\theta}\).  In fact, from here on we will drop the notation that indicates evaluation at \(\theta\), considering that as understood; hence, call this vector simply \(\mathbf{J}^AE_x\).</p> <p>Taking the above construction a step farther, since \(\{x_i - x_0\ \lvert\ 1\le i\le n_0\}\) is a basis of \(\mathbb R^{n_0}\) we could determine \(\mathbf{J}^AE_x\) from \eqref{eq:vertex-loop}, for any \(x \in \mathbb R^{n_0}\).  Another perspective on this would be to consider how the parameters in \(\theta\) express an affine linear function \(\mathbb R^{n_0} \to \mathbb R^{n_d}\) which has a restriction to \(A\) that agrees with the restriction of \(F_\theta\). Then \(\mathbf{J}^AE_x\) is the Jacobian of the evaluation map, at \(x\), corresponding to that affine linear function.</p> <p>Now, if \(A \ne X\) is one of the cells having non-empty intersection with \(Z\) in conditions 1, 2, and 3 above, then for any \(x\in\mathbb R^{n_0}\), \(\mathbf{J}^AE_x\) is a linear combination of rows of \(\mathbf{J}E_Z\). While we are still figuring out how it works in general, let’s consider a special case.</p> <p><strong>How it works when \(n_0 = 2\) and all bent hyperplanes at the vertex come from one layer.</strong> Under our “nice” assumptions when \(n_0=2\), for any vertex \(v\) of \(X\) there are 4 top-dimensional cells of the polyhedral complex that have \(v\) as a vertex, including \(X\). There are two (bent) hyperplanes intersecting at \(v\) – call them \(H_1\) and \(H_2\). The ternary labeling for each of \(H_1\) and \(H_2\) is positive in exactly two of the 4 aforementioned cells. We may associate in one-to-one manner these cells to elements of \(\{+, -\}^4.\) Let \(C\) and \(\bar C\) be the two cells which are associated to \((+,-)\) and \((-,+)\), which can be taken to mean that the ternary labeling for \(H_1\) is positive in \(C\), but negative in \(\bar C\). The opposite occurs for \(H_2\) (i.e., negative in \(C\) and positive in \(\bar C\)). Let \(D\) and \(\bar D\) be the cells which we associate to \((+,+)\) and \((-, -)\) respectively.</p> <h5 class="env-title" id="claim">Claim</h5> <p>\(\mathbf{J}^{C}E_v + \mathbf{J}^{\bar C}E_v - \mathbf{J}^{D}E_v - \mathbf{J}^{\bar D}E_v = \mathbf{0}\).</p> <blockquote class="block-tip"> <p>To prove this, note that for each of \(C, \bar C, D\), and \(\bar D\), the function  in each column of \(\mathbf{J}E_v\) restricts in the interiors to a polynomial, expressible so that every monomial in this polynomial is degree at most 1 in the parameter coordinates, and is degree 1 or less in the coordinates of \(v\), as well.  Suppose that such a monomial is degree 0 in every parameter coordinate corresponding to these two hyperplanes – that is, it is degree 0 in every one of the \(2(n_{\ell-1}+1)\) coordinates for the rows of \((W^{\ell} | b^{\ell})\) that correspond to these neurons, and it is also degree 0 in all \(2n_{\ell+1}\) coordinates appearing in the columns of \(W^{\ell+1}\) that correspond to these two neurons. Then this monomial appears in a column of \(\mathbf{J}^{C}E_v\) if and only if it appears in the same column of \(\mathbf{J}^{\bar C}E_v, \mathbf{J}^{D}E_v\), and \(\mathbf{J}^{\bar D}E_v\). Note, since these hyperplanes come from a “hidden layer”, any monomial of \(E_x\), \(x\) in the interior of one of these cells, that is positive degree in one of the coordinates corresponding to the two hyperplanes must be degree 1 in <em>two</em> such coordinates – one from \((W^{\ell} | b^{\ell})\) and one from \(W^{\ell+1}\). Thus, in the \(2(n_{\ell-1}+1) + 2n_{\ell+1}\) columns for partials with respect to such coordinates every non-zero monomial is degree 1 in one of those coordinates.  Hence, all monomials in every column that have degree 0 in those coordinates will vanish in the summation \(\mathbf{J}^{C}E_v + \mathbf{J}^{\bar C}E_v - \mathbf{J}^{D}E_v - \mathbf{J}^{\bar D}E_v\). Now, suppose that some monomial in \(\mathbf{J}^{A}E_v\), for \(A = C, \bar C, D,\) or \(\bar D\), has degree 1 in one of these coordinates. It cannot be that \(A = \bar D\) since both of the neurons in question are unactivated in \(\bar D\).  Moreover, in each column, such a monomial occuring in \(\mathbf{J}^DE_v\) must be precisely the sum of monomials that separately occur in \(\mathbf{J}^CE_v\) and \(\mathbf{J}^{\bar C}E_v\). Since no monomial in \(\mathbf{J}^{A}E_v\) can be larger than degree 1 in these coordinates, this shows the equation holds.</p> </blockquote> <p>While the discussion of the claim discusses the Jacobian of the evaluation map at \(v\), it would appear that it holds at <em>any</em> point. Using that \(X\) is one of \(C, \bar C, D\), or \(\bar D\), the claim tells us that \(\mathbf{J}^XE_{v}\) (resp. \(\mathbf{J}^XE_{p}\)) is a linear combination of vectors \(\mathbf{J}^AE_{v}\) (resp. \(\mathbf{J}^AE_{p}\)), where \(A\) takes on the other three cells (in each of which we have a subset of \(Z\) that is affinely independent. Since, for each \(A\in \{C, \bar C, D, \bar D\} \setminus \{X\}\), we may write \(\mathbf{J}^AE_v\) and \(\mathbf{J}^AE_p\) as a linear combination of rows of \(\mathbf{J}E_Z\), this tells us that \(\mathbf{J}^XE_p = \mathbf{J}E_p\) can be expressed as a linear combination of rows of \(\mathbf{J}E_Z\).</p> <hr/> <div class="footnotes" role="doc-endnotes"> <ol> <li id="fn:1"> <p>I could have written \(z^{n_d}_j\) instead for the \(j\)-th coordinate function. However, this choice will make for simpler notation below. <a href="#fnref:1" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:2"> <p>In other words, think of the function \(f_{j,z_i}:\Omega\to\mathbb R\) which is given by \(f_{j,z_i}(\theta) = F_{\theta,j}(z_i)\) and take partial derivatives of \(f_{j,z_i}\). <a href="#fnref:2" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:3"> <p>In terminology of [GLMW22], it is an <em>ordinary point</em>. <a href="#fnref:3" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:4"> <p>Shifting the index here, since the first hidden layer of the network of \(\theta\) has \(n_2\) neurons, and so on. <a href="#fnref:4" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:5"> <p>The partial derivatives in \(\mathbf{J}F_{\theta}\lvert_{\Phi\circ A(y_k)}\) does not involve partials with respect to parameters, but with respect to spatial coordinates in \(\mathbb R^{n_1}\); i.e., with respect to coordinates \((x_1,x_2,\ldots,x_{n_1})\) in \(\mathbb R^{n_1}\). <a href="#fnref:5" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:6"> <p>We also assume that the polyhedral complex is generic and transversal so, in particular, any subset of the supporting hyperplanes that determine the facets \(X\cap X_i\) (which has cardinality \(\le n_0\)), has a non-empty intersection that is some face of \(X\). <a href="#fnref:6" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> <li id="fn:7"> <p>The facet is associated to the bent hyperplane for one neuron, \(\{x\ \lvert\ \tau^{\ell}_j(x) = 0\}\); for any weight or bias “leading to” that neuron, in row \(j\) of \(W^{\ell}\) or \(b^{\ell}\), the partial of \(E_x\) with respect to that weight or bias will be undefined. <a href="#fnref:7" class="reversefootnote" role="doc-backlink">&#8617;</a></p> </li> </ol> </div>]]></content><author><name></name></author><category term="math"/><category term="neural_networks"/><category term="research"/><category term="thoughts"/><category term="ReLU_networks"/><summary type="html"><![CDATA[understanding functional dimension by precomposing]]></summary></entry><entry><title type="html">Hello world</title><link href="https://cornwell.github.io/blog/2024/hello-world/" rel="alternate" type="text/html" title="Hello world"/><published>2024-12-20T09:06:00+00:00</published><updated>2024-12-20T09:06:00+00:00</updated><id>https://cornwell.github.io/blog/2024/hello-world</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/hello-world/"><![CDATA[<p>It’s almost a new year and the time has come for a revamp of my website! For a while, I have wanted the versatility of a <a href="https://jekyllrb.com/">Jekyll</a>-based GitHub pages website, but I hadn’t taken the time to get it started. Well, I found this <a href="https://github.com/alshedivat/al-folio">al-folio theme</a> in a list of popular Jekyll themes. I like the look of it. Hopefully it will make it easy to integrate elements of research and teaching.</p> <p><mark>Posts from a date previous to this one are sample posts that were created by the al-folio theme contributors.</mark></p> <hr/> <h3 id="previous-webpage">Previous webpage</h3> <p>For a few years, I maintained a <a href="https://wp.towson.edu/ccornwell/">Wordpress webpage</a> and blog where I posted about both research and teaching. I haven’t posted to that in a while, though (1 post in the last 3 years?). I’m hoping to stay motivated in this new venue, especially since it should be easier to put together <em>thoughts and discussion</em> with <em>code and experiment</em>. Let’s say that keeping up the site is a New Year’s resolution! :oncoming_bus: :eyes:</p> <p><strong>Added bonus:</strong> I plan for the new machine learning courses I’ll be teaching (Math 371 and 471) to have some GitHub presence; this site should help with that.<br/></p> <ul> <li style="list-style-type:square"> There are some teaching-focused blog posts on the Wordpress site that I would rather not rewrite. We'll see how it goes, linking to them from here. </li> </ul> <hr/> <h3 id="research-and-teaching">Research and Teaching</h3> <p><strong>Research.</strong> I imagine that most of my updates to the site on the research end will be blogging (just thoughts, as well as research progress) and posting support scripts, or custom packages, that I made to help me with research. I’ll also post about publications and preprints, etc.</p> <p><strong>Teaching.</strong> Let’s start with small goals here, so it doesn’t soak up too much of my time. I plan to make a repo on GitHub for Math 371 – at some point that will appear under the repositories tab – as well as some info for students under the teaching tab. Other classes might follow, particularly Math 471.</p>]]></content><author><name></name></author><category term="personal"/><category term="personal"/><category term="remarks"/><summary type="html"><![CDATA[a new academic site and blog]]></summary></entry><entry><title type="html">Testing how posts work</title><link href="https://cornwell.github.io/blog/2024/test-blog-post/" rel="alternate" type="text/html" title="Testing how posts work"/><published>2024-12-18T19:36:00+00:00</published><updated>2024-12-18T19:36:00+00:00</updated><id>https://cornwell.github.io/blog/2024/test-blog-post</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/test-blog-post/"><![CDATA[<p>First, this theme uses MathJax 3. So we can write in math mode with <code class="language-plaintext highlighter-rouge">$$ $$</code> as expected, for example: \(e^{i\theta} = \cos(\theta) + i\sin(\theta)\), but not using single dollar-signs <code class="language-plaintext highlighter-rouge">$</code>.</p> <p>I am wondering if the MathJax used in Wordpress will cross-over. It would be nice if I could transfer some posts and not rewrite them. This uses <code class="language-plaintext highlighter-rouge">\( \)</code>. Tested <code class="language-plaintext highlighter-rouge">\(e^{i\theta} = \cos(\theta) + i\sin(\theta)\)</code>, but it does not work.</p> <p>Apparently you can also use tikz in this theme, as long as you set the <code class="language-plaintext highlighter-rouge">tikzjax</code> property to true:</p> <div align="center"> <script type="text/tikz">
    \begin{tikzpicture}[>=stealth]
        \draw[red,fill=black!60!red] (0,0) circle [radius=1.5];
        \draw[green,fill=black!60!green] (0,0) circle [x radius=1.5cm, y radius=10mm];
        \draw[blue,fill=black!60!blue] (0,0) circle [x radius=1cm, y radius=5mm, rotate=30];
        \draw[->, thick] (2.5,1.8) --node[at start, right]{$f(\alpha)$} (1.2,1);
        % a commented line? next: a Bezier curve
        \draw[->, thick] (3,0) ..controls (2,0) and (2.2,-1.75) ..(1.2, -1);
        \draw (3,0) node[right]{$\dag$};
    \end{tikzpicture}
    </script> </div> <hr/> <p>You can also embed lines of code. Like this?</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">get_primes_lessthan</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
    <span class="n">primes</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">n</span><span class="p">):</span>
        <span class="n">is_composite</span> <span class="o">=</span> <span class="nf">test_relcomposite</span><span class="p">(</span><span class="n">i</span><span class="p">,</span> <span class="n">primes</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">is_composite</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">primes</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">primes</span>
<span class="k">def</span> <span class="nf">test_relcomposite</span><span class="p">(</span><span class="n">num</span><span class="p">,</span> <span class="n">primelist</span><span class="p">):</span>
    <span class="c1"># catch the exceptional cases
</span>    <span class="k">try</span><span class="p">:</span>
        <span class="k">assert</span> <span class="n">num</span> <span class="o">&gt;</span> <span class="mi">1</span>
    <span class="k">except</span> <span class="nb">AssertionError</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">True</span>
    <span class="c1"># now test for divisors in primelist (assume doesn't contain 1)
</span>    <span class="n">found_divisor</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">if</span> <span class="nf">len</span><span class="p">(</span><span class="n">primelist</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">found_divsor</span> <span class="o">=</span> <span class="ow">not</span> <span class="p">(</span><span class="n">num</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">primelist</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">num</span><span class="o">%</span><span class="n">p</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">found_divisor</span> <span class="o">=</span> <span class="bp">True</span>
                <span class="k">break</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="k">continue</span>
    <span class="k">return</span> <span class="n">found_divisor</span>
</code></pre></div></div> <p>That works. I want to tweak the bg color though, at least in dark mode.</p> <p><mark>Reader beware! The default tab behavior for this theme is 4 spaces; some want only 2.</mark></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="testing"/><category term="formatting"/><summary type="html"><![CDATA[testing out functionality of blog posts]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://cornwell.github.io/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://cornwell.github.io/blog/2024/photo-gallery</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry><title type="html">a post with tabs</title><link href="https://cornwell.github.io/blog/2024/tabs/" rel="alternate" type="text/html" title="a post with tabs"/><published>2024-05-01T00:32:13+00:00</published><updated>2024-05-01T00:32:13+00:00</updated><id>https://cornwell.github.io/blog/2024/tabs</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/tabs/"><![CDATA[<p>This is how a post with <a href="https://github.com/Ovski4/jekyll-tabs">tabs</a> looks like. Note that the tabs could be used for different purposes, not only for code.</p> <h2 id="first-tabs">First tabs</h2> <p>To add tabs, use the following syntax:</p> <div class="language-liquid highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="cp">{%</span><span class="w"> </span><span class="nt">tabs</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-1</span><span class="w"> </span><span class="cp">%}</span>

Content 1

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">tab</span><span class="w"> </span><span class="nv">group-name</span><span class="w"> </span><span class="nv">tab-name-2</span><span class="w"> </span><span class="cp">%}</span>

Content 2

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtab</span><span class="w"> </span><span class="cp">%}</span>

<span class="cp">{%</span><span class="w"> </span><span class="nt">endtabs</span><span class="w"> </span><span class="cp">%}</span>
</code></pre></div></div> <p>With this you can generate visualizations like:</p> <ul id="log" class="tab" data-tab="f992633f-0040-4e4d-a153-5a29c8528515" data-name="log"> <li class="active" id="log-php"> <a href="#">php </a> </li> <li id="log-js"> <a href="#">js </a> </li> <li id="log-ruby"> <a href="#">ruby </a> </li> </ul> <ul class="tab-content" id="f992633f-0040-4e4d-a153-5a29c8528515" data-name="log"> <li class="active"> <div class="language-php highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">var_dump</span><span class="p">(</span><span class="s1">'hello'</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">console</span><span class="p">.</span><span class="nf">log</span><span class="p">(</span><span class="dl">"</span><span class="s2">hello</span><span class="dl">"</span><span class="p">);</span>
</code></pre></div></div> </li> <li> <div class="language-javascript highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nx">pputs</span> <span class="dl">'</span><span class="s1">hello</span><span class="dl">'</span>
</code></pre></div></div> </li> </ul> <h2 id="another-example">Another example</h2> <ul id="data-struct" class="tab" data-tab="1a980b51-9ace-46b3-bc94-dabe89a2e890" data-name="data-struct"> <li class="active" id="data-struct-yaml"> <a href="#">yaml </a> </li> <li id="data-struct-json"> <a href="#">json </a> </li> </ul> <ul class="tab-content" id="1a980b51-9ace-46b3-bc94-dabe89a2e890" data-name="data-struct"> <li class="active"> <div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">hello</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">whatsup"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">hi"</span>
</code></pre></div></div> </li> <li> <div class="language-json highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">{</span><span class="w">
  </span><span class="nl">"hello"</span><span class="p">:</span><span class="w"> </span><span class="p">[</span><span class="s2">"whatsup"</span><span class="p">,</span><span class="w"> </span><span class="s2">"hi"</span><span class="p">]</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div> </li> </ul> <h2 id="tabs-for-something-else">Tabs for something else</h2> <ul id="something-else" class="tab" data-tab="f7c0d6b3-1f42-4770-9bb6-a5fb425c8632" data-name="something-else"> <li class="active" id="something-else-text"> <a href="#">text </a> </li> <li id="something-else-quote"> <a href="#">quote </a> </li> <li id="something-else-list"> <a href="#">list </a> </li> </ul> <ul class="tab-content" id="f7c0d6b3-1f42-4770-9bb6-a5fb425c8632" data-name="something-else"> <li class="active"> <p>Regular text</p> </li> <li> <blockquote> <p>A quote</p> </blockquote> </li> <li> <p>Hipster list</p> <ul> <li>brunch</li> <li>fixie</li> <li>raybans</li> <li>messenger bag</li> </ul> </li> </ul>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="code"/><summary type="html"><![CDATA[this is what included tabs in a post could look like]]></summary></entry><entry><title type="html">a post with typograms</title><link href="https://cornwell.github.io/blog/2024/typograms/" rel="alternate" type="text/html" title="a post with typograms"/><published>2024-04-29T23:36:10+00:00</published><updated>2024-04-29T23:36:10+00:00</updated><id>https://cornwell.github.io/blog/2024/typograms</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/typograms/"><![CDATA[<p>This is an example post with some <a href="https://github.com/google/typograms/">typograms</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">+----+
|    |---&gt; My first diagram!
+----+</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-typograms">+----+
|    |---&gt; My first diagram!
+----+
</code></pre> <p>Another example:</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">typograms
</span><span class="sb">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.</span>
<span class="p">```</span>
</code></pre></div></div> <p>which generates:</p> <pre><code class="language-typograms">.------------------------.
|.----------------------.|
||"https://example.com" ||
|'----------------------'|
| ______________________ |
||                      ||
||   Welcome!           ||
||                      ||
||                      ||
||  .----------------.  ||
||  | username       |  ||
||  '----------------'  ||
||  .----------------.  ||
||  |"*******"       |  ||
||  '----------------'  ||
||                      ||
||  .----------------.  ||
||  |   "Sign-up"    |  ||
||  '----------------'  ||
||                      ||
|+----------------------+|
.------------------------.
</code></pre> <p>For more examples, check out the <a href="https://google.github.io/typograms/#examples">typograms documentation</a>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="diagrams"/><summary type="html"><![CDATA[this is what included typograms code could look like]]></summary></entry><entry><title type="html">a post that can be cited</title><link href="https://cornwell.github.io/blog/2024/post-citation/" rel="alternate" type="text/html" title="a post that can be cited"/><published>2024-04-28T15:06:00+00:00</published><updated>2024-04-28T15:06:00+00:00</updated><id>https://cornwell.github.io/blog/2024/post-citation</id><content type="html" xml:base="https://cornwell.github.io/blog/2024/post-citation/"><![CDATA[<p>This is an example post that can be cited. The content of the post ends here, while the citation information is automatically provided below. The only thing needed is for you to set the <code class="language-plaintext highlighter-rouge">citation</code> key in the front matter to <code class="language-plaintext highlighter-rouge">true</code>.</p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="citation"/><summary type="html"><![CDATA[this is what a post that can be cited looks like]]></summary></entry></feed>